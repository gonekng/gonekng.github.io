<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: google colab - Jiwon&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jiwon&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jiwon&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Jiwon&#039;s Blog"><meta property="og:url" content="http://gonekng.github.io/"><meta property="og:site_name" content="Jiwon&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://gonekng.github.io/img/og_image.png"><meta property="article:author" content="Jiwon Kang"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://gonekng.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://gonekng.github.io"},"headline":"Jiwon's Blog","image":["http://gonekng.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Jiwon Kang"},"publisher":{"@type":"Organization","name":"Jiwon's Blog","logo":{"@type":"ImageObject","url":"http://gonekng.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Jiwon&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">google colab</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-31T08:04:50.000Z" title="2022. 3. 31. 오후 5:04:50">2022-03-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:53.285Z" title="2022. 10. 5. 오후 2:39:53">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">4 minutes read (About 593 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/31/Python/ML/ML_ch_6_2/">ML Practice 6_2</a></h1><div class="content"><h1 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h1><ul>
<li>Find mean of pixel value : cluster center, centroid<ol>
<li>Determine the centers of k clusters at random.</li>
<li>Find the nearest cluster center from each sample and designate it as a sample of that cluster.</li>
<li>Change the center of the cluster to the average value of the samples belonging to the cluster.</li>
<li>Repeat 2~3 until there is no change in the center of the cluster.</li>
</ol>
</li>
</ul>
<hr>
<h1 id="Import-Data"><a href="#Import-Data" class="headerlink" title="Import Data"></a>Import Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!wget https://bit.ly/fruits_300_data -O fruits_300.npy</span><br></pre></td></tr></table></figure>

<pre><code>--2022-03-31 02:09:21--  https://bit.ly/fruits_300_data
Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11
Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following]
--2022-03-31 02:09:21--  https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy
Resolving github.com (github.com)... 140.82.114.3
Connecting to github.com (github.com)|140.82.114.3|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following]
--2022-03-31 02:09:22--  https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3000128 (2.9M) [application/octet-stream]
Saving to: ‘fruits_300.npy’

fruits_300.npy      100%[===================&gt;]   2.86M  --.-KB/s    in 0.01s   

2022-03-31 02:09:22 (223 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">fruits = np.load(<span class="string">&#x27;fruits_300.npy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fruits.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(300, 100, 100)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fruits_2d = fruits.reshape(-<span class="number">1</span>, <span class="number">100</span>*<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(fruits_2d.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(300, 10000)
</code></pre>
<hr>
<h1 id="KMeans-Class"><a href="#KMeans-Class" class="headerlink" title="KMeans Class"></a>KMeans Class</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">km = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">km.fit(fruits_2d) <span class="comment"># no target</span></span><br></pre></td></tr></table></figure>




<pre><code>KMeans(n_clusters=3, random_state=42)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(km.labels_) <span class="comment"># labels : [0, 1, 2]</span></span><br></pre></td></tr></table></figure>

<pre><code>[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.unique(km.labels_, return_counts=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># label 0: 111 samples / label 1: 98 samples / label 2: 91 samples</span></span><br></pre></td></tr></table></figure>

<pre><code>(array([0, 1, 2], dtype=int32), array([111,  98,  91]))
</code></pre>
<h2 id="Images-of-each-label"><a href="#Images-of-each-label" class="headerlink" title="Images of each label"></a>Images of each label</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_fruits</span>(<span class="params">arr, ratio=<span class="number">1</span></span>):</span><br><span class="line">    n = <span class="built_in">len</span>(arr)    <span class="comment"># the number of sample</span></span><br><span class="line">    rows = <span class="built_in">int</span>(np.ceil(n/<span class="number">10</span>))</span><br><span class="line">    cols = n <span class="keyword">if</span> rows &lt; <span class="number">2</span> <span class="keyword">else</span> <span class="number">10</span></span><br><span class="line">    fig, axs = plt.subplots(rows, cols, </span><br><span class="line">                            figsize=(cols*ratio, rows*ratio), squeeze=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(rows):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(cols):</span><br><span class="line">            <span class="keyword">if</span> i*<span class="number">10</span> + j &lt; n:    <span class="comment"># n 개까지만 그립니다.</span></span><br><span class="line">                axs[i, j].imshow(arr[i*<span class="number">10</span> + j], cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">            axs[i, j].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_fruits(fruits[km.labels_==<span class="number">0</span>])</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_2_1.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_fruits(fruits[km.labels_==<span class="number">1</span>])</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_2_2.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_fruits(fruits[km.labels_==<span class="number">2</span>])</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_2_3.png"></p>
<ul>
<li>label 0: mostly pineapples</li>
<li>label 1: mostly bananas</li>
<li>label 2: mostly apples</li>
</ul>
<h2 id="Centroid"><a href="#Centroid" class="headerlink" title="Centroid"></a>Centroid</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(km.cluster_centers_.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(6, 10000)
(6, 100, 100)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_fruits(km.cluster_centers_.reshape(-<span class="number">1</span>, <span class="number">100</span>, <span class="number">100</span>), ratio=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_2_4.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(km.transform(fruits_2d[<span class="number">100</span>:<span class="number">101</span>])) <span class="comment"># two-dimension array input required</span></span><br></pre></td></tr></table></figure>

<pre><code>[[3393.8136117  8837.37750892 5267.70439881]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(km.predict(fruits_2d[<span class="number">100</span>:<span class="number">101</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>[0]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_fruits(fruits[<span class="number">100</span>:<span class="number">101</span>])</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_2_5.png"></p>
<hr>
<h1 id="Finding-the-best-K-Elbow-method"><a href="#Finding-the-best-K-Elbow-method" class="headerlink" title="Finding the best K (Elbow method)"></a>Finding the best K (Elbow method)</h1><ul>
<li>inertia : sum of squares of the distance between centroid and each sample</li>
<li>As K increases, inertia decreases.</li>
<li>Set the optimal K at the point where the inertia graph is bent.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">inertia = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">7</span>):</span><br><span class="line">  km = KMeans(n_clusters=k, random_state=<span class="number">42</span>)</span><br><span class="line">  km.fit(fruits_2d)</span><br><span class="line">  inertia.append(km.inertia_)</span><br><span class="line"></span><br><span class="line">slope = []</span><br><span class="line">lst = []</span><br><span class="line"><span class="keyword">for</span> idx, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(inertia):</span><br><span class="line">  <span class="keyword">if</span> idx==<span class="number">0</span>:</span><br><span class="line">    slope.append(<span class="number">0</span>)</span><br><span class="line">    lst.append(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    slope.append(val - inertia[idx-<span class="number">1</span>])</span><br><span class="line">    lst.append(slope[idx-<span class="number">1</span>]-slope[idx])</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(<span class="built_in">range</span>(<span class="number">2</span>,<span class="number">7</span>), inertia)</span><br><span class="line">ax.scatter(<span class="number">2</span>+np.argmax(lst), inertia[np.argmax(lst)], marker=<span class="string">&quot;o&quot;</span>, color=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_2_6.png"></p>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-31T08:03:00.000Z" title="2022. 3. 31. 오후 5:03:00">2022-03-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:53.204Z" title="2022. 10. 5. 오후 2:39:53">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">4 minutes read (About 569 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/31/Python/ML/ML_ch_6_1/">ML Practice 6_1</a></h1><div class="content"><h1 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h1><ul>
<li>No dependent variables and targets. (↔ Supervised Learning)</li>
<li>Clustering (Multiple class)<ul>
<li>Must be many different types of data</li>
<li>Linked to deep learning</li>
</ul>
</li>
<li>Dimensionality reduction</li>
</ul>
<h1 id="Import-Numpy-Data"><a href="#Import-Numpy-Data" class="headerlink" title="Import Numpy Data"></a>Import Numpy Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!wget https://bit.ly/fruits_300_data -O fruits_300.npy</span><br></pre></td></tr></table></figure>

<pre><code>--2022-03-31 01:12:51--  https://bit.ly/fruits_300_data
Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10
Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following]
--2022-03-31 01:12:51--  https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy
Resolving github.com (github.com)... 140.82.113.4
Connecting to github.com (github.com)|140.82.113.4|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following]
--2022-03-31 01:12:51--  https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3000128 (2.9M) [application/octet-stream]
Saving to: ‘fruits_300.npy’

fruits_300.npy      100%[===================&gt;]   2.86M  --.-KB/s    in 0.02s   

2022-03-31 01:12:51 (157 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 100 apples, 100 pineapples, 100 bananas</span></span><br><span class="line">fruits = np.load(<span class="string">&#x27;fruits_300.npy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fruits.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(300, 100, 100)
</code></pre>
<ul>
<li>image samples of three dimensions<ul>
<li>dimension 1: the number of samples</li>
<li>dimension 2: the height of image</li>
<li>dimension 3: the width of image</li>
</ul>
</li>
<li>300 pieces of image sample of 100 x 100 size.</li>
</ul>
<h1 id="Visualize-Image-Data"><a href="#Visualize-Image-Data" class="headerlink" title="Visualize Image Data"></a>Visualize Image Data</h1><ul>
<li>black-and-white photographs</li>
<li>integer value from 0 to 255</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.imshow(fruits[<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>) <span class="comment"># 0: black, 255: white</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_1_.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(fruits[<span class="number">0</span>], cmap=<span class="string">&#x27;gray_r&#x27;</span>) <span class="comment"># 0: white, 255: black</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_1_2.png"></p>
<ul>
<li>multiple images</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">ax[<span class="number">0</span>].imshow(fruits[<span class="number">100</span>], cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].imshow(fruits[<span class="number">200</span>], cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_1_3.png"></p>
<h1 id="Pixel-value-analysis"><a href="#Pixel-value-analysis" class="headerlink" title="Pixel value analysis"></a>Pixel value analysis</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert 100*100 images to one-dimensional array with a length of 10000</span></span><br><span class="line">apple = fruits[<span class="number">0</span>:<span class="number">100</span>].reshape(-<span class="number">1</span>, <span class="number">100</span>*<span class="number">100</span>)</span><br><span class="line">pineapple = fruits[<span class="number">100</span>:<span class="number">200</span>].reshape(-<span class="number">1</span>, <span class="number">100</span>*<span class="number">100</span>)</span><br><span class="line">banana = fruits[<span class="number">200</span>:<span class="number">300</span>].reshape(-<span class="number">1</span>, <span class="number">100</span>*<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(apple.shape, pineapple.shape, banana.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(100, 10000) (100, 10000) (100, 10000)
</code></pre>
<ul>
<li>average comparison of pixel values for each image</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(np.mean(apple, axis=<span class="number">1</span>), alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.hist(np.mean(pineapple, axis=<span class="number">1</span>), alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.hist(np.mean(banana, axis=<span class="number">1</span>), alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;pineapple&#x27;</span>,<span class="string">&#x27;banana&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_1_4.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">3</span>,figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">ax[<span class="number">0</span>].bar(<span class="built_in">range</span>(<span class="number">10000</span>),np.mean(apple, axis=<span class="number">0</span>))</span><br><span class="line">ax[<span class="number">1</span>].bar(<span class="built_in">range</span>(<span class="number">10000</span>),np.mean(pineapple, axis=<span class="number">0</span>))</span><br><span class="line">ax[<span class="number">2</span>].bar(<span class="built_in">range</span>(<span class="number">10000</span>),np.mean(banana, axis=<span class="number">0</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_1_5.png"></p>
<ul>
<li>representative image using pixel mean</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apple_mean = np.mean(apple, axis=<span class="number">0</span>).reshape(<span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">pineapple_mean = np.mean(pineapple, axis=<span class="number">0</span>).reshape(<span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">banana_mean = np.mean(banana, axis=<span class="number">0</span>).reshape(<span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">3</span>,figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">ax[<span class="number">0</span>].imshow(apple_mean, cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].imshow(pineapple_mean, cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">ax[<span class="number">2</span>].imshow(banana_mean, cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_6_1_6.png"></p>
<ul>
<li>100 images close to the average value</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MAE(Mean Absolute Error)</span></span><br><span class="line">abs_diff = np.<span class="built_in">abs</span>(fruits - apple_mean)</span><br><span class="line">abs_mean = np.mean(abs_diff, axis=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(abs_mean.shape) <span class="comment"># one-dimensions array</span></span><br></pre></td></tr></table></figure>

<pre><code>(300,)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apple_index = np.argsort(abs_mean)[:<span class="number">100</span>] <span class="comment"># extract 100 indexes in the smallest order of MAE</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">10</span>,<span class="number">10</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    ax[i,j].imshow(fruits[apple_index[i*<span class="number">10</span>+j]], cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">    ax[i,j].axis(<span class="string">&#x27;off&#x27;</span>) <span class="comment"># remove axis</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>33 48 70 57 87 12 78 59 1 74 
86 38 50 92 69 27 68 30 66 24 
76 98 15 84 47 90 3 94 53 23 
14 71 32 7 73 36 55 77 21 10 
17 39 99 95 11 35 65 6 61 22 
56 89 2 13 80 0 97 4 58 34 
40 43 75 82 54 16 31 49 93 37 
63 64 41 28 67 25 96 8 83 46 
19 79 72 5 85 29 20 60 81 9 
45 51 88 62 91 26 52 18 44 42 
</code></pre>
<p><img src="/images/Python/ML/ML_ch_6_1_7.png"></p>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-31T03:40:40.000Z" title="2022. 3. 31. 오후 12:40:40">2022-03-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:54.111Z" title="2022. 10. 5. 오후 2:39:54">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">6 minutes read (About 885 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/31/Python/ML/plot_tree_ex/">ML Practice Tree plot Example</a></h1><div class="content"><h1 id="Goal-To-change-the-color-of-tree-plot"><a href="#Goal-To-change-the-color-of-tree-plot" class="headerlink" title="Goal : To change the color of tree plot"></a>Goal : To change the color of tree plot</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install -U matplotlib</span><br></pre></td></tr></table></figure>

<pre><code>Requirement already satisfied: matplotlib in c:\programdata\anaconda3\lib\site-packages (3.4.3)
Collecting matplotlib
  Downloading matplotlib-3.5.1-cp39-cp39-win_amd64.whl (7.2 MB)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (1.3.1)
Requirement already satisfied: numpy&gt;=1.17 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (1.20.3)
Requirement already satisfied: pillow&gt;=6.2.0 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (8.4.0)
Requirement already satisfied: cycler&gt;=0.10 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (0.10.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (4.25.0)
Requirement already satisfied: pyparsing&gt;=2.2.1 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (3.0.4)
Requirement already satisfied: python-dateutil&gt;=2.7 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: packaging&gt;=20.0 in c:\programdata\anaconda3\lib\site-packages (from matplotlib) (21.0)
Requirement already satisfied: six in c:\programdata\anaconda3\lib\site-packages (from cycler&gt;=0.10-&gt;matplotlib) (1.16.0)
Installing collected packages: matplotlib
  Attempting uninstall: matplotlib
    Found existing installation: matplotlib 3.4.3
    Uninstalling matplotlib-3.4.3:


ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: &#39;c:\\programdata\\anaconda3\\lib\\site-packages\\__pycache__\\pylab.cpython-39.pyc&#39;
Consider using the `--user` option or check the permissions.
</code></pre>
<h1 id="Stackflow-Ex"><a href="#Stackflow-Ex" class="headerlink" title="Stackflow Ex."></a>Stackflow Ex.</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap, to_rgb</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line">X = np.random.rand(<span class="number">50</span>, <span class="number">2</span>) * np.r_[<span class="number">100</span>, <span class="number">50</span>]</span><br><span class="line">y = X[:, <span class="number">0</span>] - X[:, <span class="number">1</span>] &gt; <span class="number">20</span></span><br><span class="line"></span><br><span class="line">clf = tree.DecisionTreeClassifier(random_state=<span class="number">2021</span>)</span><br><span class="line">clf = clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">colors = [<span class="string">&#x27;crimson&#x27;</span>, <span class="string">&#x27;dodgerblue&#x27;</span>]</span><br><span class="line"></span><br><span class="line">artists = tree.plot_tree(clf, feature_names=[<span class="string">&quot;X&quot;</span>, <span class="string">&quot;y&quot;</span>], class_names=colors,</span><br><span class="line">                         filled=<span class="literal">True</span>, rounded=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> artist, impurity, value <span class="keyword">in</span> <span class="built_in">zip</span>(artists, clf.tree_.impurity, clf.tree_.value):</span><br><span class="line">    <span class="comment"># let the max value decide the color; whiten the color depending on impurity (gini)</span></span><br><span class="line">    r, g, b = to_rgb(colors[np.argmax(value)])</span><br><span class="line">    f = impurity * <span class="number">2</span> <span class="comment"># for N colors: f = impurity * N/(N-1) if N&gt;1 else 0</span></span><br><span class="line">    artist.get_bbox_patch().set_facecolor((f + (<span class="number">1</span>-f)*r, f + (<span class="number">1</span>-f)*g, f + (<span class="number">1</span>-f)*b))</span><br><span class="line">    artist.get_bbox_patch().set_edgecolor(<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/plot_tree_ex_1.png"></p>
<h1 id="Iris-Ex"><a href="#Iris-Ex" class="headerlink" title="Iris Ex."></a>Iris Ex.</h1><h2 id="Tree-plot"><a href="#Tree-plot" class="headerlink" title="Tree plot"></a>Tree plot</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="built_in">print</span>(sklearn.__version__)</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="built_in">print</span>(matplotlib.__version__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="built_in">print</span>(iris.data.shape, iris.target.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;feature names&quot;</span>, iris.feature_names)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;class names&quot;</span>, iris.target_names)</span><br><span class="line"></span><br><span class="line">dt = tree.DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">dt.fit(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">18</span>, <span class="number">10</span>))</span><br><span class="line">ax = tree.plot_tree(dt, max_depth = <span class="number">2</span>, filled=<span class="literal">True</span>,</span><br><span class="line">                    feature_names = iris.feature_names, class_names = iris.target_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>0.24.2
3.4.3
(150, 4) (150,)
feature names [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]
class names [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;]
</code></pre>
<p><img src="/images/Python/ML/plot_tree_ex_2.png"></p>
<h2 id="matplotlib-text-Annotation"><a href="#matplotlib-text-Annotation" class="headerlink" title="matplotlib.text.Annotation"></a>matplotlib.text.Annotation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">ax = tree.plot_tree(dt, max_depth = <span class="number">2</span>, </span><br><span class="line">                    filled=<span class="literal">True</span>, </span><br><span class="line">                    feature_names = iris.feature_names, </span><br><span class="line">                    class_names = iris.target_names)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(ax)):</span><br><span class="line">  <span class="built_in">print</span>(<span class="built_in">type</span>(ax[i]))</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
&lt;class &#39;matplotlib.text.Annotation&#39;&gt;
</code></pre>
<p><img src="/images/Python/ML/plot_tree_ex_3.png"></p>
<ul>
<li>get_bbox_patch() method</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">ax = tree.plot_tree(dt, max_depth = <span class="number">2</span>, </span><br><span class="line">                    filled=<span class="literal">True</span>, </span><br><span class="line">                    feature_names = iris.feature_names, </span><br><span class="line">                    class_names = iris.target_names)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(ax)):</span><br><span class="line">  <span class="built_in">print</span>(ax[i].get_bbox_patch()) <span class="comment"># get patch properties (facecolor, edgewidth,,,)</span></span><br></pre></td></tr></table></figure>

<pre><code>FancyBboxPatch((0, 0), width=120.875, height=56.4)
FancyBboxPatch((0, 0), width=87.875, height=44.8)
FancyBboxPatch((0, 0), width=127.25, height=56.4)
FancyBboxPatch((0, 0), width=131.625, height=56.4)
FancyBboxPatch((0, 0), width=30, height=33.2)
FancyBboxPatch((0, 0), width=30, height=33.2)
FancyBboxPatch((0, 0), width=131.625, height=56.4)
FancyBboxPatch((0, 0), width=30, height=33.2)
FancyBboxPatch((0, 0), width=30, height=33.2)
</code></pre>
<p><img src="/images/Python/ML/plot_tree_ex_4.png"></p>
<ul>
<li>set_boxstyle()</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">ax = tree.plot_tree(dt, max_depth = <span class="number">2</span>, </span><br><span class="line">                    filled=<span class="literal">True</span>, </span><br><span class="line">                    feature_names = iris.feature_names, </span><br><span class="line">                    class_names = iris.target_names)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(ax)):</span><br><span class="line">  <span class="comment"># set patch properties</span></span><br><span class="line">  <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">    ax[i].get_bbox_patch().set_boxstyle(<span class="string">&quot;Rarrow&quot;</span>, pad=<span class="number">0.3</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    ax[i].get_bbox_patch().set_boxstyle(<span class="string">&quot;Round&quot;</span>, pad=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/plot_tree_ex_5.png"></p>
<h1 id="Final-ex"><a href="#Final-ex" class="headerlink" title="Final ex."></a>Final ex.</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">colors = [<span class="string">&quot;indigo&quot;</span>, <span class="string">&quot;violet&quot;</span>, <span class="string">&quot;crimson&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(colors[np.argmax([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">50.</span>]])])</span><br><span class="line"><span class="built_in">print</span>(colors[np.argmax([[<span class="number">50.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])])</span><br><span class="line"><span class="built_in">print</span>(colors[np.argmax([[<span class="number">0.</span>, <span class="number">50.</span>, <span class="number">0.</span>]])])</span><br><span class="line"><span class="built_in">print</span>(colors[np.argmax([[<span class="number">50.</span>, <span class="number">50.</span>, <span class="number">50.</span>]])])</span><br></pre></td></tr></table></figure>

<pre><code>crimson
indigo
violet
indigo
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> to_rgb</span><br><span class="line">%matplotlib inline</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">ax = tree.plot_tree(dt, max_depth = <span class="number">3</span>, </span><br><span class="line">                    filled=<span class="literal">True</span>, </span><br><span class="line">                    feature_names = iris.feature_names, </span><br><span class="line">                    class_names = iris.target_names)</span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">colors = [<span class="string">&quot;yellow&quot;</span>, <span class="string">&quot;violet&quot;</span>, <span class="string">&quot;lavenderblush&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> artist, impurity, value <span class="keyword">in</span> <span class="built_in">zip</span>(ax, dt.tree_.impurity, dt.tree_.value):</span><br><span class="line">  r, g, b = to_rgb(colors[np.argmax(value)])</span><br><span class="line">  <span class="comment"># 코드가 길어서 i로 재 저장</span></span><br><span class="line">  ip = impurity</span><br><span class="line">  <span class="comment"># print(ip + (1-ip)*r, ip + (1-ip)*g, ip + (1-ip)*b)</span></span><br><span class="line">  <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># set_boxtyle 적용</span></span><br><span class="line">    ax[i].get_bbox_patch().set_boxstyle(<span class="string">&quot;round&quot;</span>, pad=<span class="number">0.3</span>)</span><br><span class="line">    ax[i].get_bbox_patch().set_facecolor((ip + (<span class="number">1</span>-ip)*r, ip + (<span class="number">1</span>-ip)*g, ip + (<span class="number">1</span>-ip)*b))</span><br><span class="line">    ax[i].get_bbox_patch().set_edgecolor(<span class="string">&#x27;black&#x27;</span>)  </span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    ax[i].get_bbox_patch().set_boxstyle(<span class="string">&quot;circle&quot;</span>, pad=<span class="number">0.3</span>)</span><br><span class="line">    ax[i].get_bbox_patch().set_facecolor((ip + (<span class="number">1</span>-ip)*r, ip + (<span class="number">1</span>-ip)*g, ip + (<span class="number">1</span>-ip)*b))</span><br><span class="line">    ax[i].get_bbox_patch().set_edgecolor(<span class="string">&#x27;black&#x27;</span>)   </span><br><span class="line">  i = i+<span class="number">1</span></span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/plot_tree_ex_6.png"></p>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-30T07:48:30.000Z" title="2022. 3. 30. 오후 4:48:30">2022-03-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:53.131Z" title="2022. 10. 5. 오후 2:39:53">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">3 minutes read (About 411 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/30/Python/ML/ML_ch_5_3/">ML Practice 5_3</a></h1><div class="content"><h1 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h1><ul>
<li>algorithm that performs best in dealing with structured data</li>
<li>Bagging : A method of aggregating results by taking multiple bootstrap samples and training each model. (parallel learning)<ul>
<li>Random Forest</li>
</ul>
</li>
<li>Boosting : (sequential learning)<ul>
<li>GBM –&gt; XGBoost –&gt; LightGBM</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h1><ul>
<li>Create decision trees randomly and make final predictions based on each tree’s predictions.<ul>
<li>Classification : Average the probabilities for each class of each tree and uses the class with the highest probability as a prediction.</li>
<li>Regression : Average the predictions of each tree.</li>
</ul>
</li>
<li>Bootstrap : method of sampling data by permitting duplication in a dataset</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">wine = pd.read_csv(<span class="string">&#x27;https://bit.ly/wine_csv_data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data = wine[[<span class="string">&#x27;alcohol&#x27;</span>, <span class="string">&#x27;sugar&#x27;</span>, <span class="string">&#x27;pH&#x27;</span>]].to_numpy()</span><br><span class="line">target = wine[<span class="string">&#x27;class&#x27;</span>].to_numpy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    data, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_validate</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rf = RandomForestClassifier(n_jobs=-<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">scores = cross_validate(rf, train_input, train_target, return_train_score=<span class="literal">True</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&#x27;train_score&#x27;</span>]), np.mean(scores[<span class="string">&#x27;test_score&#x27;</span>])) <span class="comment"># overfitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.9973541965122431 0.8905151032797809
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rf.fit(train_input, train_target)</span><br><span class="line"><span class="built_in">print</span>(rf.feature_importances_)</span><br></pre></td></tr></table></figure>

<pre><code>[0.23167441 0.50039841 0.26792718]
</code></pre>
<ul>
<li>OOB(out of bag) Sample : remaining sample not included in bootstrap sample<ul>
<li>same effect as cross-validation using a verification set</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestClassifier(oob_score=<span class="literal">True</span>, n_jobs=-<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rf.fit(train_input, train_target)</span><br><span class="line"><span class="built_in">print</span>(rf.oob_score_)</span><br></pre></td></tr></table></figure>

<pre><code>0.8934000384837406
</code></pre>
<hr>
<h1 id="GBM-Gradient-Boosting-Machine"><a href="#GBM-Gradient-Boosting-Machine" class="headerlink" title="GBM(Gradient Boosting Machine)"></a>GBM(Gradient Boosting Machine)</h1><ul>
<li>Correct errors in previous trees by using shallow trees.</li>
<li>Adjust the speed (step width) through the learning rate parameter</li>
<li>less likely to overfit but speed is slow</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">gb = GradientBoostingClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">scores = cross_validate(gb, train_input, train_target, return_train_score=<span class="literal">True</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&#x27;train_score&#x27;</span>]), np.mean(scores[<span class="string">&#x27;test_score&#x27;</span>])) <span class="comment"># good fitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.8881086892152563 0.8720430147331015
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># n_estimators = 500 (default 100), learning rate = 0.2 (default 0.1)</span></span><br><span class="line">gb = GradientBoostingClassifier(n_estimators=<span class="number">500</span>, learning_rate=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">scores = cross_validate(gb, train_input, train_target, return_train_score=<span class="literal">True</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&#x27;train_score&#x27;</span>]), np.mean(scores[<span class="string">&#x27;test_score&#x27;</span>])) <span class="comment"># good fitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.9464595437171814 0.8780082549788999
</code></pre>
<hr>
<h1 id="Overall-Flow-of-ML"><a href="#Overall-Flow-of-ML" class="headerlink" title="Overall Flow of ML"></a>Overall Flow of ML</h1><ol start="0">
<li>Data preprocessing, EDA, Visualization</li>
<li>Design the entire flow as a basic model</li>
<li>compare multiple models with default hyperparameter</li>
<li>Cross-validation and Hyperparameter tuning</li>
<li>Repeat the above process until finding the best result</li>
</ol>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-30T06:40:40.000Z" title="2022. 3. 30. 오후 3:40:40">2022-03-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:53.059Z" title="2022. 10. 5. 오후 2:39:53">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">5 minutes read (About 688 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/30/Python/ML/ML_ch_5_2/">ML Practice 5_2</a></h1><div class="content"><h1 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h1><h4 id="Repeated-process-of-spliting-validation-set-and-evaluating-model"><a href="#Repeated-process-of-spliting-validation-set-and-evaluating-model" class="headerlink" title=": Repeated process of spliting validation set and evaluating model."></a>: Repeated process of spliting validation set and evaluating model.</h4><ul>
<li>Train set : Validation set : Test set &#x3D; 6 : 2 : 2 (generally)</li>
<li>Test sets are not used in the model learning process.</li>
<li>In Kagge competition, test sets are given separately.</li>
<li>purpose : To make a good model<ul>
<li>A good model doesn’t mean high-performance model.</li>
<li>A good model means low-error and stable model.</li>
</ul>
</li>
<li>Because it takes a long time, it is useful when there is not much data.</li>
</ul>
<h3 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data"></a>Prepare data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">wine = pd.read_csv(<span class="string">&#x27;https://bit.ly/wine_csv_data&#x27;</span>)</span><br><span class="line">data = wine[[<span class="string">&#x27;alcohol&#x27;</span>,<span class="string">&#x27;sugar&#x27;</span>,<span class="string">&#x27;pH&#x27;</span>]].to_numpy()</span><br><span class="line">target = wine[[<span class="string">&#x27;class&#x27;</span>]].to_numpy()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    data, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">sub_input, val_input, sub_target, val_target = train_test_split(</span><br><span class="line">    train_input, train_target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sub_input.shape, val_input.shape, test_input.shape</span><br></pre></td></tr></table></figure>




<pre><code>((4157, 3), (1040, 3), (1300, 3))
</code></pre>
<h3 id="Create-model"><a href="#Create-model" class="headerlink" title="Create model"></a>Create model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">dt.fit(sub_input, sub_target)</span><br><span class="line"><span class="built_in">print</span>(dt.score(sub_input, sub_target))</span><br><span class="line"><span class="built_in">print</span>(dt.score(val_input, val_target)) <span class="comment"># overfitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.9971133028626413
0.864423076923077
</code></pre>
<h3 id="Validate-model"><a href="#Validate-model" class="headerlink" title="Validate model"></a>Validate model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_validate</span><br><span class="line">scores = cross_validate(dt, train_input, train_target) <span class="comment"># dictionary type</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> scores.items():</span><br><span class="line">  <span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure>

<pre><code>(&#39;fit_time&#39;, array([0.01251197, 0.00755358, 0.0074594 , 0.00742102, 0.00734329]))
(&#39;score_time&#39;, array([0.00133634, 0.00079608, 0.0007925 , 0.00083232, 0.00076413]))
(&#39;test_score&#39;, array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&#x27;test_score&#x27;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>0.855300214703487
</code></pre>
<ul>
<li>In cross-validation, a splitter must be specified to mix training sets.<ul>
<li>Regression model &gt; KFold</li>
<li>Classification model &gt; StratifiedKFold</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">splitter = StratifiedKFold(shuffle=<span class="literal">True</span>, random_state=<span class="number">42</span>) <span class="comment"># default : 5 fold</span></span><br><span class="line">scores = cross_validate(dt, train_input, train_target, cv=splitter)</span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&#x27;test_score&#x27;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>0.8539548012141852
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">splitter = StratifiedKFold(n_splits=<span class="number">10</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">42</span>) <span class="comment"># 10 fold</span></span><br><span class="line">scores = cross_validate(dt, train_input, train_target, cv=splitter)</span><br><span class="line"><span class="built_in">print</span>(np.mean(scores[<span class="string">&#x27;test_score&#x27;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>0.8574181117533719
</code></pre>
<hr>
<h1 id="Hyperparameter-Tuning"><a href="#Hyperparameter-Tuning" class="headerlink" title="Hyperparameter Tuning"></a>Hyperparameter Tuning</h1><ul>
<li>ex) max_depth&#x3D;3, accuracy&#x3D;0.84</li>
<li>Finding the best value by adjusting multiple parameters simultaneously.</li>
<li>AutoML : technology that automatically performs hyperparameter tuning without intervention of person.<ul>
<li>Grid Search, Random Search</li>
</ul>
</li>
</ul>
<h2 id="Grid-Search"><a href="#Grid-Search" class="headerlink" title="Grid Search"></a>Grid Search</h2><ul>
<li>Perform hyperparameter tuning and cross-validation simultaneously</li>
<li>Find the optimal hyperparameters based on all combinations of predetermined values.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;min_impurity_decrease&#x27;</span> : [<span class="number">0.0001</span>, <span class="number">0.0002</span>, <span class="number">0.0003</span>, <span class="number">0.0004</span>, <span class="number">0.0005</span>]</span><br><span class="line">&#125;</span><br><span class="line">gs = GridSearchCV(DecisionTreeClassifier(random_state=<span class="number">42</span>), params, n_jobs=-<span class="number">1</span>)</span><br><span class="line">gs.fit(train_input, train_target)</span><br></pre></td></tr></table></figure>

<pre><code>CPU times: user 70.1 ms, sys: 6.06 ms, total: 76.1 ms
Wall time: 183 ms
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dt = gs.best_estimator_</span><br><span class="line"><span class="built_in">print</span>(dt)</span><br><span class="line"><span class="built_in">print</span>(dt.score(train_input, train_target))</span><br></pre></td></tr></table></figure>

<pre><code>DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42)
0.9615162593804117
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(gs.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(gs.best_params_)</span><br></pre></td></tr></table></figure>

<pre><code>[0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]
&#123;&#39;min_impurity_decrease&#39;: 0.0001&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;min_impurity_decrease&#x27;</span> : [<span class="number">0.0001</span>, <span class="number">0.0002</span>, <span class="number">0.0003</span>, <span class="number">0.0004</span>, <span class="number">0.0005</span>],</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span> : [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change the values in params and create a total of 5 models with each value.</span></span><br><span class="line"><span class="comment"># n_jobs=-1 : to enable all cores in the system</span></span><br><span class="line">gs = GridSearchCV(DecisionTreeClassifier(random_state=<span class="number">42</span>), params, n_jobs=-<span class="number">1</span>)</span><br><span class="line">gs.fit(train_input, train_target)</span><br></pre></td></tr></table></figure>

<pre><code>CPU times: user 167 ms, sys: 4.85 ms, total: 172 ms
Wall time: 585 ms
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dt = gs.best_estimator_</span><br><span class="line"><span class="built_in">print</span>(dt)</span><br><span class="line"><span class="built_in">print</span>(dt.score(train_input, train_target))</span><br></pre></td></tr></table></figure>

<pre><code>DecisionTreeClassifier(max_depth=7, min_impurity_decrease=0.0005,
                       random_state=42)
0.8830094285164518
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(gs.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>]) <span class="comment"># 5*5=25</span></span><br><span class="line"><span class="built_in">print</span>(gs.best_params_)</span><br></pre></td></tr></table></figure>

<pre><code>[0.84125583 0.84125583 0.84125583 0.84125583 0.84125583 0.85337806
 0.85337806 0.85337806 0.85337806 0.85318557 0.85780355 0.85799604
 0.85857352 0.85857352 0.85838102 0.85645721 0.85799678 0.85876675
 0.85972866 0.86088306 0.85607093 0.85761031 0.85799511 0.85991893
 0.86280466]
&#123;&#39;max_depth&#39;: 7, &#39;min_impurity_decrease&#39;: 0.0005&#125;
</code></pre>
<ul>
<li>The optimal value of ‘min_impurity_decrease’ varies when the value of ‘max_depth’ changes.</li>
</ul>
<h2 id="Random-Search"><a href="#Random-Search" class="headerlink" title="Random Search"></a>Random Search</h2><ul>
<li>Find the optimal hyperparameters based on possible combinations within a predetermined range of values.</li>
<li>Delivers probability distribution objects that can sample parameters.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># randint : sampling int</span></span><br><span class="line"><span class="comment"># uniform : sampling float</span></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> uniform, randint</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;min_impurity_decrease&#x27;</span> : uniform(<span class="number">0.0001</span>, <span class="number">0.001</span>),</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span> : randint(<span class="number">20</span>, <span class="number">50</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line">gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=<span class="number">42</span>), params,</span><br><span class="line">                        n_iter=<span class="number">100</span>, n_jobs=-<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">gs.fit(train_input, train_target)</span><br></pre></td></tr></table></figure>

<pre><code>CPU times: user 629 ms, sys: 15.8 ms, total: 645 ms
Wall time: 2.54 s
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dt = gs.best_estimator_</span><br><span class="line"><span class="built_in">print</span>(dt)</span><br><span class="line"><span class="built_in">print</span>(dt.score(train_input, train_target))</span><br></pre></td></tr></table></figure>

<pre><code>DecisionTreeClassifier(max_depth=29, min_impurity_decrease=0.000437615171403628,
                       random_state=42)
0.8903213392341736
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(gs.best_params_)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;max_depth&#39;: 29, &#39;min_impurity_decrease&#39;: 0.000437615171403628&#125;
</code></pre>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-30T06:10:20.000Z" title="2022. 3. 30. 오후 3:10:20">2022-03-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:52.983Z" title="2022. 10. 5. 오후 2:39:52">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">5 minutes read (About 818 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/30/Python/ML/ML_ch_5_1/">ML Practice 5_1</a></h1><div class="content"><h1 id="Prepare-Data"><a href="#Prepare-Data" class="headerlink" title="Prepare Data"></a>Prepare Data</h1><ul>
<li>Import wine data set<ul>
<li>class 0: red wine</li>
<li>class 1: white wine</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">wine = pd.read_csv(<span class="string">&quot;https://bit.ly/wine_csv_data&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(wine.head())</span><br></pre></td></tr></table></figure>

<pre><code>   alcohol  sugar    pH  class
0      9.4    1.9  3.51    0.0
1      9.8    2.6  3.20    0.0
2      9.8    2.3  3.26    0.0
3      9.8    1.9  3.16    0.0
4      9.4    1.9  3.51    0.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># checking missing value and types of variable</span></span><br><span class="line">wine.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 6497 entries, 0 to 6496
Data columns (total 4 columns):
 #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   alcohol  6497 non-null   float64
 1   sugar    6497 non-null   float64
 2   pH       6497 non-null   float64
 3   class    6497 non-null   float64
dtypes: float64(4)
memory usage: 203.2 KB
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(wine.describe()) <span class="comment"># vary in scale of variables, in need of standardization</span></span><br></pre></td></tr></table></figure>

<pre><code>           alcohol        sugar           pH        class
count  6497.000000  6497.000000  6497.000000  6497.000000
mean     10.491801     5.443235     3.218501     0.753886
std       1.192712     4.757804     0.160787     0.430779
min       8.000000     0.600000     2.720000     0.000000
25%       9.500000     1.800000     3.110000     1.000000
50%      10.300000     3.000000     3.210000     1.000000
75%      11.300000     8.100000     3.320000     1.000000
max      14.900000    65.800000     4.010000     1.000000
</code></pre>
<ul>
<li>Split data into training sets and test sets</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = wine[[<span class="string">&#x27;alcohol&#x27;</span>, <span class="string">&#x27;sugar&#x27;</span>, <span class="string">&#x27;pH&#x27;</span>]].to_numpy()</span><br><span class="line">target = wine[<span class="string">&#x27;class&#x27;</span>].to_numpy()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    data, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(train_input.shape, test_input.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(5197, 3) (1300, 3)
</code></pre>
<ul>
<li>Standardize Data</li>
<li>Decision tree does not require standardized preprocessing,<br/> but it is recommended to perform standardization basically.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(train_input)</span><br><span class="line">train_scaled = ss.transform(train_input)</span><br><span class="line">test_scaled = ss.transform(test_input)</span><br></pre></td></tr></table></figure>

<h1 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(lr.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(lr.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.7808350971714451
0.7776923076923077
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.predict_proba(train_scaled[:<span class="number">5</span>]))</span><br><span class="line"><span class="built_in">print</span>(lr.predict(train_scaled[:<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>[[0.06189333 0.93810667]
 [0.21742616 0.78257384]
 [0.40703571 0.59296429]
 [0.45226659 0.54773341]
 [0.00530794 0.99469206]]
[1. 1. 1. 1. 1.]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_, lr.intercept_)</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]
</code></pre>
<h1 id="Decision-Tree-Model"><a href="#Decision-Tree-Model" class="headerlink" title="Decision Tree Model"></a>Decision Tree Model</h1><h4 id="a-non-parametric-supervised-learning-method-used-for-classification-and-regression"><a href="#a-non-parametric-supervised-learning-method-used-for-classification-and-regression" class="headerlink" title=": a non-parametric supervised learning method used for classification and regression"></a>: a non-parametric supervised learning method used for classification and regression</h4><ul>
<li>to predict the value of a target variable by learning simple decision rules inferred from the data features</li>
<li>Simple to understand and to interpret</li>
<li>More likely to be overfitting the training set.</li>
<li>New Algorithm Using Decision Tree Algorithms<ul>
<li>XGBoost, LightGBM, CatBoost, etc</li>
<li>In particular, LightGBM is now widely used in practice.</li>
</ul>
</li>
</ul>
<h3 id="DecisionTreeClassifier"><a href="#DecisionTreeClassifier" class="headerlink" title="DecisionTreeClassifier"></a>DecisionTreeClassifier</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">dt.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(dt.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(dt.score(test_scaled, test_target)) <span class="comment"># appear to be overfitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.996921300750433
0.8592307692307692
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">18</span>,<span class="number">10</span>))</span><br><span class="line">plot_tree(dt, filled=<span class="literal">True</span>, feature_names=[<span class="string">&#x27;alcohol&#x27;</span>,<span class="string">&#x27;sugar&#x27;</span>,<span class="string">&#x27;pH&#x27;</span>])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># - conditions for testing : sugar</span></span><br><span class="line"><span class="comment"># - impurity : gini</span></span><br><span class="line"><span class="comment"># - samples : total number of samples</span></span><br><span class="line"><span class="comment"># - value : number of samples by class</span></span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_5_1_1.png"></p>
<h2 id="Impurity-불순도"><a href="#Impurity-불순도" class="headerlink" title="Impurity(불순도)"></a>Impurity(불순도)</h2><ul>
<li>parameter criterion; default ‘gini’</li>
<li>gini &#x3D; 1 - (negative_prop^2 + positive_prop^2)<ul>
<li>best : 0 (pure node)</li>
<li>worst : 0.5 (exactly half and half)</li>
</ul>
</li>
<li>entropy &#x3D; - negative_prop * log_2(negative_prop) - positive_prop * log_2(positive_prop)</li>
<li>Information gain(정보 이득) : impurity differences between parent node and child node<ul>
<li>Decision tree splits nodes to maximize information gain using impurity criteria.</li>
</ul>
</li>
</ul>
<h2 id="Pruning-가지치기"><a href="#Pruning-가지치기" class="headerlink" title="Pruning(가지치기)"></a>Pruning(가지치기)</h2><ul>
<li>In order to prevent overfitting the training set</li>
<li>By specifying the maximum depth of a tree that can grow</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dt = DecisionTreeClassifier(max_depth=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">dt.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(dt.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(dt.score(test_scaled, test_target)) <span class="comment"># successful in reducing overfitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.8454877814123533
0.8415384615384616
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">18</span>,<span class="number">10</span>))</span><br><span class="line">plot_tree(dt, filled=<span class="literal">True</span>, feature_names=[<span class="string">&#x27;alcohol&#x27;</span>,<span class="string">&#x27;sugar&#x27;</span>,<span class="string">&#x27;pH&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_5_1_2.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tree Plot Image Download</span></span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line">dot_data = tree.export_graphviz(</span><br><span class="line">    dt, out_file=<span class="literal">None</span>, feature_names = [<span class="string">&#x27;alcohol&#x27;</span>,<span class="string">&#x27;sugar&#x27;</span>,<span class="string">&#x27;pH&#x27;</span>], filled=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">graph = graphviz.Source(dot_data, <span class="built_in">format</span>=<span class="string">&quot;png&quot;</span>) </span><br><span class="line">graph</span><br></pre></td></tr></table></figure>




<p><img src="/images/Python/ML/ML_ch_5_1.svg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph.render(<span class="string">&quot;decision_tree_graphivz&quot;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&#39;decision_tree_graphivz.png&#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Customize color of nodes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap, to_rgb</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">15</span>))</span><br><span class="line">artists = plot_tree(dt, filled = <span class="literal">True</span>, feature_names = [<span class="string">&#x27;alcohol&#x27;</span>,<span class="string">&#x27;sugar&#x27;</span>,<span class="string">&#x27;pH&#x27;</span>])</span><br><span class="line"></span><br><span class="line">colors = [<span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;red&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> artist, impurity, value <span class="keyword">in</span> <span class="built_in">zip</span>(artists, dt.tree_.impurity, dt.tree_.value):</span><br><span class="line">    r, g, b = to_rgb(colors[np.argmax(value)])</span><br><span class="line">    f = impurity * <span class="number">2</span></span><br><span class="line">    artist.get_bbox_patch().set_facecolor((f + (<span class="number">1</span>-f)*r, f + (<span class="number">1</span>-f)*g, f + (<span class="number">1</span>-f)*b))</span><br><span class="line">    artist.get_bbox_patch().set_edgecolor(<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_5_1_3.png"></p>
<ul>
<li>parameter min_impurity_decrease; default 0.0<ul>
<li>Split nodes if this split induces a decrease of the impurity greater than or equal to this value.</li>
</ul>
<ul>
<li>More likely to be asymmetric tree</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dt = DecisionTreeClassifier(min_impurity_decrease=<span class="number">0.0005</span>, random_state=<span class="number">42</span>)</span><br><span class="line">dt.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(dt.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(dt.score(test_scaled, test_target))</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">18</span>, <span class="number">10</span>))</span><br><span class="line">plot_tree(dt, filled=<span class="literal">True</span>, feature_names=[<span class="string">&#x27;alcohol&#x27;</span>,<span class="string">&#x27;sugar&#x27;</span>,<span class="string">&#x27;pH&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>0.8874350586877044
0.8615384615384616
</code></pre>
<p><img src="/images/Python/ML/ML_ch_5_1_4.png"></p>
<h2 id="Feature-importance"><a href="#Feature-importance" class="headerlink" title="Feature importance"></a>Feature importance</h2><p>: an indicator of the degree to which each feature contributed to reducing impurities</p>
<ul>
<li>Multiply the information gain and the ratio of the total sample by each node, and add it up by feature.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(dt.feature_importances_) <span class="comment"># Sugar is the most important feature.</span></span><br></pre></td></tr></table></figure>

<pre><code>[0.12345626 0.86862934 0.0079144 ]
</code></pre>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-29T07:10:25.000Z" title="2022. 3. 29. 오후 4:10:25">2022-03-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:52.900Z" title="2022. 10. 5. 오후 2:39:52">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">5 minutes read (About 696 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/29/Python/ML/ML_ch_4_2/">ML Practice 4_2</a></h1><div class="content"><h1 id="Gradient-Descent-경사-하강법"><a href="#Gradient-Descent-경사-하강법" class="headerlink" title="Gradient Descent(경사 하강법)"></a>Gradient Descent(경사 하강법)</h1><h4 id="Algorithm-for-finding-the-minimum-value-of-a-loss-function-using-a-sample-of-a-training-set"><a href="#Algorithm-for-finding-the-minimum-value-of-a-loss-function-using-a-sample-of-a-training-set" class="headerlink" title=": Algorithm for finding the minimum value of a loss function using a sample of a training set"></a>: Algorithm for finding the minimum value of a loss function using a sample of a training set</h4><ol>
<li>stochastic gradient descent(확률적 경사 하강법; SGD)<ul>
<li>method of randomly selecting <strong>one</strong> sample from a training set</li>
</ul>
</li>
<li>minibatch gradient descent(미니배치 경사 하강법)<ul>
<li>method of randomly selecting <strong>several</strong> samples from a training set</li>
</ul>
</li>
<li>batch gradient descent(배치 경사 하강법)<ul>
<li>method of selecting <strong>all</strong> the samples from a training set at once</li>
</ul>
</li>
</ol>
<ul>
<li>Sampling method is different from the existing model. (more detailed approach)</li>
<li>It aims to correct errors by reducing the slope of the loss function</li>
<li>SGDClassifier : Create a classification model using SGD.</li>
<li>SGDRegressor : Create a regression model using SGD.</li>
<li>Epoch : process of using the entire training set once</li>
</ul>
<h4 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h4><ul>
<li>Deep learning algorithm (especially, image and text)</li>
<li>Tree algorithm + Gradient Descent &#x3D; Boosting<ul>
<li>ex) LightGBM, Xgboost, Catboost</li>
</ul>
</li>
</ul>
<h2 id="Loss-function-손실-함수"><a href="#Loss-function-손실-함수" class="headerlink" title="Loss function(손실 함수)"></a>Loss function(손실 함수)</h2><ul>
<li>Cost function 비용 함수)</li>
<li>Loss is the difference between the predicted value and the actual value of the model (equivalent to error)</li>
<li>Loss function is a function that expresses loss of the model<ul>
<li>an indicator of how poorly a model processes data</li>
</ul>
</li>
<li>Loss function must be differentiable.</li>
</ul>
<hr>
<h1 id="Prepare-Data"><a href="#Prepare-Data" class="headerlink" title="Prepare Data"></a>Prepare Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">fish = pd.read_csv(<span class="string">&quot;https://bit.ly/fish_csv_data&quot;</span>)</span><br><span class="line">fish_input = fish[[<span class="string">&#x27;Weight&#x27;</span>, <span class="string">&#x27;Length&#x27;</span>, <span class="string">&#x27;Diagonal&#x27;</span>, <span class="string">&#x27;Height&#x27;</span>, <span class="string">&#x27;Width&#x27;</span>]].to_numpy()</span><br><span class="line">fish_target = fish[<span class="string">&#x27;Species&#x27;</span>].to_numpy()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    fish_input, fish_target, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">train_input.shape, test_input.shape, train_target.shape, test_target.shape</span><br></pre></td></tr></table></figure>




<pre><code>((119, 5), (40, 5), (119,), (40,))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalize</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(train_input)</span><br><span class="line"></span><br><span class="line">train_scaled = ss.transform(train_input)</span><br><span class="line">test_scaled = ss.transform(test_input)</span><br></pre></td></tr></table></figure>

<p>※ To prevent data leakage, make sure to convert the test set to the statistics learned from the training set.<br>※ Data leakage : containing the information you want to predict in the data used for model training </p>
<hr>
<h1 id="SGD-Classifier"><a href="#SGD-Classifier" class="headerlink" title="SGD Classifier"></a>SGD Classifier</h1><h2 id="Fitting-model"><a href="#Fitting-model" class="headerlink" title="Fitting model"></a>Fitting model</h2><ul>
<li>set 2 parameter in SGD Classifier<ul>
<li>loss : specifying the type of loss function</li>
<li>max_iter : specifying the number of epochs to be executed</li>
</ul>
</li>
<li>In the case of a multi-classification model,<br/> if loss is set as ‘log’, a binary classification model is created for each class.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"></span><br><span class="line">sc = SGDClassifier(loss=<span class="string">&#x27;log&#x27;</span>, max_iter=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line">sc.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(sc.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(sc.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.773109243697479
0.775


/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning,
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># partial_fit() : continue training one epoch per call</span></span><br><span class="line">sc.partial_fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(sc.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(sc.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.8151260504201681
0.85
</code></pre>
<h2 id="Finding-appropriate-epoch"><a href="#Finding-appropriate-epoch" class="headerlink" title="Finding appropriate epoch"></a>Finding appropriate epoch</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">sc = SGDClassifier(loss=<span class="string">&#x27;log&#x27;</span>, random_state=<span class="number">42</span>)</span><br><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br><span class="line">classes = np.unique(train_target)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">300</span>): <span class="comment"># _ : temporal variable</span></span><br><span class="line">  sc.partial_fit(train_scaled, train_target, classes=classes)</span><br><span class="line">  train_score.append(sc.score(train_scaled, train_target))</span><br><span class="line">  test_score.append(sc.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(train_score)</span><br><span class="line">ax.plot(test_score)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_4_2.png"></p>
<ul>
<li>In the early stages of epoch, the scores of training sets and test sets are low because they are underfitting.</li>
<li>After epoch 100, the score difference between the training set and the test set gradually increases.</li>
<li>Epoch 100 appears to be the most appropriate number of iterations.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD classifier stops by itself, if performance does not improve during a certain epoch.</span></span><br><span class="line"><span class="comment"># tol = None : to repeat unconditionally untill max_iter</span></span><br><span class="line">sc = SGDClassifier(loss=<span class="string">&#x27;log&#x27;</span>, max_iter=<span class="number">100</span>, tol=<span class="literal">None</span>, random_state=<span class="number">42</span>)</span><br><span class="line">sc.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(sc.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(sc.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.957983193277311
0.925
</code></pre>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-29T03:50:45.000Z" title="2022. 3. 29. 오후 12:50:45">2022-03-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:52.815Z" title="2022. 10. 5. 오후 2:39:52">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">6 minutes read (About 856 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/29/Python/ML/ML_ch_4_1/">ML Practice 4_1</a></h1><div class="content"><h1 id="Prepare-Data"><a href="#Prepare-Data" class="headerlink" title="Prepare Data"></a>Prepare Data</h1><h2 id="Import-data-set"><a href="#Import-data-set" class="headerlink" title="Import data set"></a>Import data set</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">fish = pd.read_csv(<span class="string">&#x27;https://bit.ly/fish_csv_data&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fish.head())</span><br></pre></td></tr></table></figure>

<pre><code>  Species  Weight  Length  Diagonal   Height   Width
0   Bream   242.0    25.4      30.0  11.5200  4.0200
1   Bream   290.0    26.3      31.2  12.4800  4.3056
2   Bream   340.0    26.5      31.1  12.3778  4.6961
3   Bream   363.0    29.0      33.5  12.7300  4.4555
4   Bream   430.0    29.0      34.0  12.4440  5.1340
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(pd.unique(fish[<span class="string">&#x27;Species&#x27;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;]
</code></pre>
<h2 id="Convert-to-Numpy-array"><a href="#Convert-to-Numpy-array" class="headerlink" title="Convert to Numpy array"></a>Convert to Numpy array</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fish_input = fish[[<span class="string">&#x27;Weight&#x27;</span>,<span class="string">&#x27;Length&#x27;</span>,<span class="string">&#x27;Diagonal&#x27;</span>,<span class="string">&#x27;Height&#x27;</span>,<span class="string">&#x27;Width&#x27;</span>]].to_numpy()</span><br><span class="line"><span class="built_in">print</span>(fish_input.shape)</span><br><span class="line"><span class="built_in">print</span>(fish_input[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<pre><code>(159, 5)
[[242.      25.4     30.      11.52     4.02  ]
 [290.      26.3     31.2     12.48     4.3056]
 [340.      26.5     31.1     12.3778   4.6961]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fish_target = fish[<span class="string">&#x27;Species&#x27;</span>].to_numpy()</span><br><span class="line"><span class="built_in">print</span>(fish_target.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(159,)
</code></pre>
<h2 id="Split-and-Standardize"><a href="#Split-and-Standardize" class="headerlink" title="Split and Standardize"></a>Split and Standardize</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    fish_input, fish_target, random_state=<span class="number">42</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(train_input)</span><br><span class="line">train_scaled = ss.transform(train_input)</span><br><span class="line">test_scaled = ss.transform(test_input)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_input[:<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(train_scaled[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[[720.      35.      40.6     16.3618   6.09  ]
 [500.      45.      48.       6.96     4.896 ]
 [  7.5     10.5     11.6      1.972    1.16  ]]
[[ 0.91965782  0.60943175  0.81041221  1.85194896  1.00075672]
 [ 0.30041219  1.54653445  1.45316551 -0.46981663  0.27291745]
 [-1.0858536  -1.68646987 -1.70848587 -1.70159849 -2.0044758 ]
 [-0.79734143 -0.60880176 -0.67486907 -0.82480589 -0.27631471]
 [-0.71289885 -0.73062511 -0.70092664 -0.0802298  -0.7033869 ]]
[[-0.88741352 -0.91804565 -1.03098914 -0.90464451 -0.80762518]
 [-1.06924656 -1.50842035 -1.54345461 -1.58849582 -1.93803151]
 [-0.54401367  0.35641402  0.30663259 -0.8135697  -0.65388895]
 [-0.34698097 -0.23396068 -0.22320459 -0.11905019 -0.12233464]
 [-0.68475132 -0.51509149 -0.58801052 -0.8998784  -0.50124996]]
</code></pre>
<hr>
<h1 id="KNN-Classifier"><a href="#KNN-Classifier" class="headerlink" title="KNN Classifier"></a>KNN Classifier</h1><h2 id="Model-fitting"><a href="#Model-fitting" class="headerlink" title="Model fitting"></a>Model fitting</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">kn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">kn.fit(train_scaled, train_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(kn.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(kn.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.8907563025210085
0.85
</code></pre>
<h2 id="Multi-class-Classfication"><a href="#Multi-class-Classfication" class="headerlink" title="Multi-class Classfication"></a>Multi-class Classfication</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">proba = kn.predict_proba(test_scaled[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(kn.classes_)</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(proba, decimals=<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;]
[[0.     0.     1.     0.     0.     0.     0.    ]
 [0.     0.     0.     0.     0.     1.     0.    ]
 [0.     0.     0.     1.     0.     0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">distances, indexes = kn.kneighbors(test_scaled[<span class="number">3</span>:<span class="number">4</span>]) <span class="comment"># Two-dimensional array must be input</span></span><br><span class="line"><span class="built_in">print</span>(train_target[indexes])</span><br></pre></td></tr></table></figure>

<pre><code>[[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]]
</code></pre>
<ul>
<li>The probability calculated by the model is the ratio of the nearest neighbor.<ul>
<li>In this model(k&#x3D;3), the probability values are 0, 1&#x2F;3, 2&#x2F;3, and 1.</li>
<li>If k is set as 5, the probability values may be 0, 0.2, 0.4, 0.6, 0.8 and 1.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">kn.fit(train_scaled, train_target)</span><br><span class="line">proba = kn.predict_proba(test_scaled[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(kn.classes_)</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(proba, decimals=<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(kn.predict(test_scaled[:<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;]
[[0.  0.  0.6 0.  0.4 0.  0. ]
 [0.  0.  0.  0.  0.  1.  0. ]
 [0.  0.  0.2 0.8 0.  0.  0. ]
 [0.  0.  0.8 0.  0.2 0.  0. ]
 [0.  0.  0.8 0.  0.2 0.  0. ]]
[&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;]
</code></pre>
<hr>
<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h3 id="Estimating-a-model-with-a-regression-equation-for-categorical-dependent-variables"><a href="#Estimating-a-model-with-a-regression-equation-for-categorical-dependent-variables" class="headerlink" title=": Estimating a model with a regression equation for categorical dependent variables."></a>: Estimating a model with a regression equation for categorical dependent variables.</h3><ul>
<li>Despite its name, a classification model rather than regression model</li>
<li>Highly important model<ul>
<li>used as basic statistics (especially medical statistics)</li>
<li>the basis of the machine learning classification model.</li>
<li>early model of deep learning</li>
</ul>
</li>
<li>To overcome the linearity assumption problem of general regression equation<ul>
<li>Logit transformation : the log of the odds ratio</li>
<li>Using the logit of Y as the dependent variable of the regression</li>
</ul>
</li>
</ul>
<h2 id="Sigmoid-function"><a href="#Sigmoid-function" class="headerlink" title="Sigmoid function"></a>Sigmoid function</h2><ul>
<li>also called a logistic function</li>
<li>Convert the value z calculated by linear regression to a probability value between 0 and 1<ul>
<li>z &lt; 0: the function approaches 0</li>
<li>z &gt; 0: the function approaches 1</li>
<li>z &#x3D; 0: the function value is 0.5</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">z = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">phi = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z)) <span class="comment"># sigmoid function</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(z, phi)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;z&#x27;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;phi&#x27;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">ax.set_title(<span class="string">&quot;Sigmoid Function for Logistic Regression&quot;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_4_1.png"></p>
<h2 id="Binary-classification"><a href="#Binary-classification" class="headerlink" title="Binary classification"></a>Binary classification</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Boolean Indexing: using a boolean vector to filter the data. </span></span><br><span class="line"><span class="comment"># Choose only Bream and Smelt from the training set.</span></span><br><span class="line">bream_smelt_indexes = (train_target == <span class="string">&#x27;Bream&#x27;</span>) | (train_target == <span class="string">&#x27;Smelt&#x27;</span>)</span><br><span class="line">train_bream_smelt = train_scaled[bream_smelt_indexes]</span><br><span class="line">target_bream_smelt = train_target[bream_smelt_indexes]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(train_bream_smelt, target_bream_smelt)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lr.classes_) <span class="comment"># 0: Bream / 1: Smelt</span></span><br><span class="line">proba = lr.predict_proba(train_bream_smelt[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(proba, decimals=<span class="number">3</span>)) <span class="comment"># 5 rows, 2 columns</span></span><br><span class="line"><span class="built_in">print</span>(lr.predict(train_bream_smelt[:<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;Bream&#39; &#39;Smelt&#39;]
[[0.998 0.002]
 [0.027 0.973]
 [0.995 0.005]
 [0.986 0.014]
 [0.998 0.002]]
[&#39;Bream&#39; &#39;Smelt&#39; &#39;Bream&#39; &#39;Bream&#39; &#39;Bream&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_, lr.intercept_)</span><br></pre></td></tr></table></figure>

<pre><code>[[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
</code></pre>
<blockquote>
<p> <em>z &#x3D; - 0.404 * Weight - 0. 576 * Length - 0.663 * Diagonal - 1.013 * Height - 0.732 * Width - 2.162</em></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">decisions = lr.decision_function(train_bream_smelt[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(decisions) <span class="comment"># original z-value of positive class(Smelt)</span></span><br></pre></td></tr></table></figure>

<pre><code>[-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> expit</span><br><span class="line"><span class="built_in">print</span>(expit(decisions)) <span class="comment"># probability value through sigmoid function</span></span><br></pre></td></tr></table></figure>

<pre><code>[0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
</code></pre>
<h2 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h2><ul>
<li>basically use iterative algorithms (max_iter, default 100)<ul>
<li>in this model, set max_iter as 1000 (for sufficient training)</li>
</ul>
</li>
</ul>
<h4 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h4><ul>
<li>based on the square value of the coefficient such as ridge regression</li>
<li>hyperparameter; C ( default 1)<ul>
<li>the smaller the value, the greater the regulation.</li>
<li>in this model, set C as 20 (in order to ease regulations a little)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(C=<span class="number">20</span>, max_iter=<span class="number">1000</span>)</span><br><span class="line">lr.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(lr.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(lr.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.9327731092436975
0.925
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.classes_)</span><br><span class="line">proba = lr.predict_proba(test_scaled[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(proba, decimals=<span class="number">3</span>)) <span class="comment"># 5 rows, 7 columns</span></span><br><span class="line"><span class="built_in">print</span>(lr.predict(test_scaled[:<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;]
[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
[&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_.shape, lr.intercept_.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(7, 5) (7,)
[[-1.49002087 -1.02912886  2.59345551  7.70357682 -1.2007011 ]
 [ 0.19618235 -2.01068181 -3.77976834  6.50491489 -1.99482722]
 [ 3.56279745  6.34357182 -8.48971143 -5.75757348  3.79307308]
 [-0.10458098  3.60319431  3.93067812 -3.61736674 -1.75069691]
 [-1.40061442 -6.07503434  5.25969314 -0.87220069  1.86043659]
 [-1.38526214  1.49214574  1.39226167 -5.67734118 -4.40097523]
 [ 0.62149861 -2.32406685 -0.90660867  1.71599038  3.6936908 ]]
[-0.09205179 -0.26290885  3.25101327 -0.14742956  2.65498283 -6.78782948
  1.38422358]
</code></pre>
<ul>
<li>The z value is calculated one by one for each class and classified into the class that outputs the highest value.</li>
</ul>
<h2 id="Softmax-function"><a href="#Softmax-function" class="headerlink" title="Softmax function"></a>Softmax function</h2><ul>
<li>also called a normalized exponential function (because of using exponential functions)</li>
<li>The outputs of several linear equations are compressed from 0 to 1, and the total sum is 1.<blockquote>
<p><em>e_sum &#x3D; e^z1 + e^z2 + … + e^z7</em></p>
</blockquote>
<blockquote>
<p><em>s1 &#x3D; e^z1&#x2F;e_sum, s2 &#x3D; e^z2&#x2F;e_sum, … , s7 &#x3D; e^z7&#x2F;e_sum</em></p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">decision = lr.decision_function(test_scaled[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(decision, decimals=<span class="number">3</span>)) <span class="comment"># original z value</span></span><br></pre></td></tr></table></figure>

<pre><code>[[ -6.498   1.032   5.164  -2.729   3.339   0.327  -0.634]
 [-10.859   1.927   4.771  -2.398   2.978   7.841  -4.26 ]
 [ -4.335  -6.233   3.174   6.487   2.358   2.421  -3.872]
 [ -0.683   0.453   2.647  -1.187   3.265  -5.753   1.259]
 [ -6.397  -1.993   5.816  -0.11    3.503  -0.112  -0.707]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> softmax</span><br><span class="line">proba = softmax(decision, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">round</span>(proba, decimals=<span class="number">3</span>)) <span class="comment"># probability value through softmax function</span></span><br></pre></td></tr></table></figure>

<pre><code>[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
</code></pre>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-28T15:34:50.000Z" title="2022. 3. 29. 오전 12:34:50">2022-03-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:52.733Z" title="2022. 10. 5. 오후 2:39:52">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">7 minutes read (About 1020 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/29/Python/ML/ML_ch_3_3/">ML Practice 3_3</a></h1><div class="content"><h1 id="Prepare-Data"><a href="#Prepare-Data" class="headerlink" title="Prepare Data"></a>Prepare Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;https://bit.ly/perch_csv_data&#x27;</span>)</span><br><span class="line">perch_full = df.to_numpy() <span class="comment"># Convert Pandas DataFrame to Numpy Array</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">perch_weight = np.array([<span class="number">5.9</span>, <span class="number">32.0</span>, <span class="number">40.0</span>, <span class="number">51.5</span>, <span class="number">70.0</span>, <span class="number">100.0</span>, <span class="number">78.0</span>, <span class="number">80.0</span>, <span class="number">85.0</span>, <span class="number">85.0</span>, <span class="number">110.0</span>,</span><br><span class="line">       <span class="number">115.0</span>, <span class="number">125.0</span>, <span class="number">130.0</span>, <span class="number">120.0</span>, <span class="number">120.0</span>, <span class="number">130.0</span>, <span class="number">135.0</span>, <span class="number">110.0</span>, <span class="number">130.0</span>,</span><br><span class="line">       <span class="number">150.0</span>, <span class="number">145.0</span>, <span class="number">150.0</span>, <span class="number">170.0</span>, <span class="number">225.0</span>, <span class="number">145.0</span>, <span class="number">188.0</span>, <span class="number">180.0</span>, <span class="number">197.0</span>,</span><br><span class="line">       <span class="number">218.0</span>, <span class="number">300.0</span>, <span class="number">260.0</span>, <span class="number">265.0</span>, <span class="number">250.0</span>, <span class="number">250.0</span>, <span class="number">300.0</span>, <span class="number">320.0</span>, <span class="number">514.0</span>,</span><br><span class="line">       <span class="number">556.0</span>, <span class="number">840.0</span>, <span class="number">685.0</span>, <span class="number">700.0</span>, <span class="number">700.0</span>, <span class="number">690.0</span>, <span class="number">900.0</span>, <span class="number">650.0</span>, <span class="number">820.0</span>,</span><br><span class="line">       <span class="number">850.0</span>, <span class="number">900.0</span>, <span class="number">1015.0</span>, <span class="number">820.0</span>, <span class="number">1100.0</span>, <span class="number">1000.0</span>, <span class="number">1100.0</span>, <span class="number">1000.0</span>,</span><br><span class="line">       <span class="number">1000.0</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    perch_full, perch_weight, random_state=<span class="number">42</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<hr>
<h1 id="Transform-Data"><a href="#Transform-Data" class="headerlink" title="Transform Data"></a>Transform Data</h1><h3 id="※-Scikit-Learn-Class"><a href="#※-Scikit-Learn-Class" class="headerlink" title="※ Scikit-Learn Class"></a>※ Scikit-Learn Class</h3><ul>
<li>Estimator(추정기; model class) : Fitting and predicting<ul>
<li>KNeighborsClassifier, LinearRegression, etc.</li>
<li>common method : fit(), score(), predict()</li>
</ul>
</li>
<li>Transformer(변환기) and Pre-processors : transforming or imputing data<ul>
<li>PolynomialFeatures, StandardScaler, etc</li>
<li>common method : fit(), transform()</li>
</ul>
</li>
</ul>
<h3 id="※-Feature-engineering-특성-공학"><a href="#※-Feature-engineering-특성-공학" class="headerlink" title="※ Feature engineering(특성 공학)"></a>※ Feature engineering(특성 공학)</h3><ul>
<li>extracting new features using existing features</li>
<li>existing features, square features of each, and features multiplied by each other.</li>
</ul>
<h2 id="Import-transformer"><a href="#Import-transformer" class="headerlink" title="Import transformer"></a>Import transformer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br></pre></td></tr></table></figure>

<h2 id="Transform-sample-data"><a href="#Transform-sample-data" class="headerlink" title="Transform sample data"></a>Transform sample data</h2><ul>
<li>case 1: Including a bias</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poly = PolynomialFeatures()</span><br><span class="line">poly.fit([[<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(poly.transform([[<span class="number">2</span>,<span class="number">3</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[[1. 2. 3. 4. 6. 9.]]


      &gt; existing features : 2, 3
      &gt; new features : 1(for intercept), 4(2^2), 6(2*3), 9(3^2)
</code></pre>
<ul>
<li>case 2: Not including a bias (recommended)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poly = PolynomialFeatures(include_bias=<span class="literal">False</span>)</span><br><span class="line">poly.fit([[<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(poly.transform([[<span class="number">2</span>,<span class="number">3</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[[2. 3. 4. 6. 9.]]


      &gt; existing features : 2, 3
      &gt; new features : 4(2^2), 6(2*3), 9(3^2)
</code></pre>
<h2 id="Transform-perch-data"><a href="#Transform-perch-data" class="headerlink" title="Transform perch data"></a>Transform perch data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">poly = PolynomialFeatures(include_bias=<span class="literal">False</span>)</span><br><span class="line">poly.fit(train_input)</span><br><span class="line">train_poly = poly.transform(train_input)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_input.shape) <span class="comment"># have 3 features</span></span><br><span class="line"><span class="built_in">print</span>(train_poly.shape) <span class="comment"># have 9 features</span></span><br><span class="line"><span class="built_in">print</span>(poly.get_feature_names_out())</span><br></pre></td></tr></table></figure>

<pre><code>(42, 3)
(42, 9)
[&#39;x0&#39; &#39;x1&#39; &#39;x2&#39; &#39;x0^2&#39; &#39;x0 x1&#39; &#39;x0 x2&#39; &#39;x1^2&#39; &#39;x1 x2&#39; &#39;x2^2&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_poly = poly.transform(test_input)</span><br><span class="line"><span class="built_in">print</span>(test_input.shape) <span class="comment"># have 3 features</span></span><br><span class="line"><span class="built_in">print</span>(test_poly.shape) <span class="comment"># have 9 features</span></span><br></pre></td></tr></table></figure>

<pre><code>(14, 3)
(14, 9)
</code></pre>
<hr>
<h1 id="Mutiple-Regression"><a href="#Mutiple-Regression" class="headerlink" title="Mutiple Regression"></a>Mutiple Regression</h1><ul>
<li>same process as training a linear regression model</li>
<li>linear regression using multiple features</li>
</ul>
<h4 id="degree-2"><a href="#degree-2" class="headerlink" title="degree 2"></a>degree 2</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(train_poly, train_target)</span><br><span class="line"><span class="built_in">print</span>(lr.score(train_poly, train_target))</span><br><span class="line"><span class="built_in">print</span>(lr.score(test_poly, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.9903183436982124
0.9714559911594134
</code></pre>
<ul>
<li>Multiple regression solves the linear model’s underfitting problem.</li>
<li>The score for the training set is very high.</li>
</ul>
<h4 id="degree-5"><a href="#degree-5" class="headerlink" title="degree 5"></a>degree 5</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">poly = PolynomialFeatures(degree=<span class="number">5</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">poly.fit(train_input)</span><br><span class="line">train_poly = poly.transform(train_input)</span><br><span class="line">test_poly = poly.transform(test_input)</span><br><span class="line"><span class="built_in">print</span>(train_poly.shape)</span><br><span class="line"><span class="built_in">print</span>(test_poly.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(42, 55)
(14, 55)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr.fit(train_poly, train_target)</span><br><span class="line"><span class="built_in">print</span>(lr.score(train_poly, train_target))</span><br><span class="line"><span class="built_in">print</span>(lr.score(test_poly, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.9999999999991097
-144.40579242684848
</code></pre>
<ul>
<li>The score for the training set is almost perfect.</li>
<li>But the score for the testing set is extremely negative.<ul>
<li>The model appears to be too overfitting to the training set.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Regularization-규제"><a href="#Regularization-규제" class="headerlink" title="Regularization(규제)"></a>Regularization(규제)</h1><ul>
<li>preventing the model from overfitting the training set</li>
<li>linear regression model : reducing the size of the coefficient multiplied by the feature.</li>
</ul>
<h2 id="hyperparameter-alpha"><a href="#hyperparameter-alpha" class="headerlink" title="hyperparameter: alpha"></a>hyperparameter: alpha</h2><ul>
<li>parameter which has to be set in advance<ul>
<li>increase&#x2F;decrease in regulatory intensity</li>
<li>adjusted to increase the performance of the model</li>
</ul>
</li>
<li>Conceptual understanding is important, but it doesn’t mean much in practice.<ul>
<li>No guarantee of performance compared to working hours.</li>
<li>More than 100 libraries in scikit-learn, and the types and numbers of hyperparameters vary.</li>
</ul>
</li>
<li>Better to use the existing hyperparameters, for unfamiliar models.</li>
</ul>
<h2 id="Normalize-feature-scales"><a href="#Normalize-feature-scales" class="headerlink" title="Normalize feature scales"></a>Normalize feature scales</h2><ul>
<li>using StandardScaler class in scikit-learn</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(train_poly)</span><br><span class="line">train_scaled = ss.transform(train_poly)</span><br><span class="line">test_scaled = ss.transform(test_poly)</span><br><span class="line"><span class="built_in">print</span>(train_poly.shape)</span><br><span class="line"><span class="built_in">print</span>(test_poly.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(42, 55)
(14, 55)
</code></pre>
<h2 id="Ridge-regression"><a href="#Ridge-regression" class="headerlink" title="Ridge regression"></a>Ridge regression</h2><ul>
<li>based on the square value of the coefficient</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">ridge = Ridge()</span><br><span class="line">ridge.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(ridge.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(ridge.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.9896101671037343
0.9790693977615397
</code></pre>
<ul>
<li>Many features are used, but they’re not overfitting the training set and perform well on the test set.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alpha_list = [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpha_list:</span><br><span class="line">  ridge = Ridge(alpha=alpha)</span><br><span class="line">  ridge.fit(train_scaled, train_target)</span><br><span class="line">  train_score.append(ridge.score(train_scaled, train_target))</span><br><span class="line">  test_score.append(ridge.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(np.log10(alpha_list), train_score)</span><br><span class="line">ax.plot(np.log10(alpha_list), test_score)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;log10(alpha)&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;R^2&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_3_3_1.png"></p>
<ul>
<li>left side : overfitting</li>
<li>right side : underfitting</li>
<li>appropriate alpha : 0.1</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ridge = Ridge(alpha=<span class="number">0.1</span>)</span><br><span class="line">ridge.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(ridge.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(ridge.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.9903815817570366
0.9827976465386926
</code></pre>
<h2 id="Lasso-regression"><a href="#Lasso-regression" class="headerlink" title="Lasso regression"></a>Lasso regression</h2><ul>
<li>based on the absolute value of the coefficient</li>
<li>The coefficient can be completely zero.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">lasso = Lasso()</span><br><span class="line">lasso.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(lasso.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(lasso.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.989789897208096
0.9800593698421883
</code></pre>
<ul>
<li>Many features are used, but they’re not overfitting the training set and perform well on the test set.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br><span class="line"></span><br><span class="line">alpha_list = [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpha_list:</span><br><span class="line">  lasso = Lasso(alpha=alpha, max_iter=<span class="number">10000</span>)</span><br><span class="line">  lasso.fit(train_scaled, train_target)</span><br><span class="line">  train_score.append(lasso.score(train_scaled, train_target))</span><br><span class="line">  test_score.append(lasso.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+04, tolerance: 5.183e+02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive
/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e+04, tolerance: 5.183e+02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(np.log10(alpha_list), train_score)</span><br><span class="line">ax.plot(np.log10(alpha_list), test_score)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;log10(alpha)&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;R^2&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_3_3_2.png"></p>
<ul>
<li>left side : overfitting</li>
<li>right side : underfitting</li>
<li>appropriate alpha : 10</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lasso = Lasso(alpha=<span class="number">10</span>)</span><br><span class="line">lasso.fit(train_scaled, train_target)</span><br><span class="line"><span class="built_in">print</span>(lasso.score(train_scaled, train_target))</span><br><span class="line"><span class="built_in">print</span>(lasso.score(test_scaled, test_target))</span><br></pre></td></tr></table></figure>

<pre><code>0.9888067471131867
0.9824470598706695
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(lasso.coef_==<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<pre><code>40
</code></pre>
<ul>
<li>40 coefficients became zero</li>
<li>Of the 55 features, only 15 were finally used.</li>
</ul>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-28T08:34:00.000Z" title="2022. 3. 28. 오후 5:34:00">2022-03-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-10-05T05:39:52.607Z" title="2022. 10. 5. 오후 2:39:52">2022-10-05</time></span><span class="level-item"> Jiwon Kang </span><span class="level-item"><a class="link-muted" href="/categories/python/">python</a><span> / </span><a class="link-muted" href="/categories/python/ML/">ML</a></span><span class="level-item">4 minutes read (About 653 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/28/Python/ML/ML_ch_3_2/">ML Practice 3_2</a></h1><div class="content"><h1 id="Data-Set"><a href="#Data-Set" class="headerlink" title="Data Set"></a>Data Set</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">perch_length = np.array(</span><br><span class="line">    [<span class="number">8.4</span>, <span class="number">13.7</span>, <span class="number">15.0</span>, <span class="number">16.2</span>, <span class="number">17.4</span>, <span class="number">18.0</span>, <span class="number">18.7</span>, <span class="number">19.0</span>, <span class="number">19.6</span>, <span class="number">20.0</span>, </span><br><span class="line">     <span class="number">21.0</span>, <span class="number">21.0</span>, <span class="number">21.0</span>, <span class="number">21.3</span>, <span class="number">22.0</span>, <span class="number">22.0</span>, <span class="number">22.0</span>, <span class="number">22.0</span>, <span class="number">22.0</span>, <span class="number">22.5</span>, </span><br><span class="line">     <span class="number">22.5</span>, <span class="number">22.7</span>, <span class="number">23.0</span>, <span class="number">23.5</span>, <span class="number">24.0</span>, <span class="number">24.0</span>, <span class="number">24.6</span>, <span class="number">25.0</span>, <span class="number">25.6</span>, <span class="number">26.5</span>, </span><br><span class="line">     <span class="number">27.3</span>, <span class="number">27.5</span>, <span class="number">27.5</span>, <span class="number">27.5</span>, <span class="number">28.0</span>, <span class="number">28.7</span>, <span class="number">30.0</span>, <span class="number">32.8</span>, <span class="number">34.5</span>, <span class="number">35.0</span>, </span><br><span class="line">     <span class="number">36.5</span>, <span class="number">36.0</span>, <span class="number">37.0</span>, <span class="number">37.0</span>, <span class="number">39.0</span>, <span class="number">39.0</span>, <span class="number">39.0</span>, <span class="number">40.0</span>, <span class="number">40.0</span>, <span class="number">40.0</span>, </span><br><span class="line">     <span class="number">40.0</span>, <span class="number">42.0</span>, <span class="number">43.0</span>, <span class="number">43.0</span>, <span class="number">43.5</span>, <span class="number">44.0</span>]</span><br><span class="line">     )</span><br><span class="line">perch_weight = np.array(</span><br><span class="line">    [<span class="number">5.9</span>, <span class="number">32.0</span>, <span class="number">40.0</span>, <span class="number">51.5</span>, <span class="number">70.0</span>, <span class="number">100.0</span>, <span class="number">78.0</span>, <span class="number">80.0</span>, <span class="number">85.0</span>, <span class="number">85.0</span>, </span><br><span class="line">     <span class="number">110.0</span>, <span class="number">115.0</span>, <span class="number">125.0</span>, <span class="number">130.0</span>, <span class="number">120.0</span>, <span class="number">120.0</span>, <span class="number">130.0</span>, <span class="number">135.0</span>, <span class="number">110.0</span>, </span><br><span class="line">     <span class="number">130.0</span>, <span class="number">150.0</span>, <span class="number">145.0</span>, <span class="number">150.0</span>, <span class="number">170.0</span>, <span class="number">225.0</span>, <span class="number">145.0</span>, <span class="number">188.0</span>, <span class="number">180.0</span>, </span><br><span class="line">     <span class="number">197.0</span>, <span class="number">218.0</span>, <span class="number">300.0</span>, <span class="number">260.0</span>, <span class="number">265.0</span>, <span class="number">250.0</span>, <span class="number">250.0</span>, <span class="number">300.0</span>, <span class="number">320.0</span>, </span><br><span class="line">     <span class="number">514.0</span>, <span class="number">556.0</span>, <span class="number">840.0</span>, <span class="number">685.0</span>, <span class="number">700.0</span>, <span class="number">700.0</span>, <span class="number">690.0</span>, <span class="number">900.0</span>, <span class="number">650.0</span>, </span><br><span class="line">     <span class="number">820.0</span>, <span class="number">850.0</span>, <span class="number">900.0</span>, <span class="number">1015.0</span>, <span class="number">820.0</span>, <span class="number">1100.0</span>, <span class="number">1000.0</span>, <span class="number">1100.0</span>, </span><br><span class="line">     <span class="number">1000.0</span>, <span class="number">1000.0</span>]</span><br><span class="line">     )</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, test_input, train_target, test_target = train_test_split(</span><br><span class="line">    perch_length, perch_weight, random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">train_input.shape, test_input.shape, train_target.shape, test_target.shape</span><br></pre></td></tr></table></figure>




<pre><code>((42,), (14,), (42,), (14,))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_input = train_input.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">test_input = test_input.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_input.shape, test_input.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(42, 1) (14, 1)
</code></pre>
<h1 id="KNN-Regression"><a href="#KNN-Regression" class="headerlink" title="KNN Regression"></a>KNN Regression</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"></span><br><span class="line">knr = KNeighborsRegressor(n_neighbors=<span class="number">3</span>)</span><br><span class="line">knr.fit(train_input, train_target)</span><br><span class="line">knr.score(test_input, test_target)</span><br></pre></td></tr></table></figure>




<pre><code>0.9746459963987609
</code></pre>
<h2 id="Predict-a-data-1"><a href="#Predict-a-data-1" class="headerlink" title="Predict a data 1"></a>Predict a data 1</h2><ul>
<li>the weight of a 50-centimeter-long perch</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(knr.predict([[<span class="number">50</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[1033.33333333]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">distances, indexes = knr.kneighbors([[<span class="number">50</span>]])</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(train_input, train_target)</span><br><span class="line">ax.scatter(train_input[indexes], train_target[indexes], marker=<span class="string">&quot;D&quot;</span>) <span class="comment"># 3 neighbors</span></span><br><span class="line">ax.scatter(<span class="number">50</span>, knr.predict([[<span class="number">50</span>]]), marker=<span class="string">&#x27;^&#x27;</span>) <span class="comment"># new data</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_3_2_1.png"></p>
<h2 id="Predict-a-data-2"><a href="#Predict-a-data-2" class="headerlink" title="Predict a data 2"></a>Predict a data 2</h2><ul>
<li>the weight of a 100-centimeter-long perch</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(knr.predict([[<span class="number">100</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[1033.33333333]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">distances, indexes = knr.kneighbors([[<span class="number">100</span>]])</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(train_input, train_target)</span><br><span class="line">ax.scatter(train_input[indexes], train_target[indexes], marker=<span class="string">&quot;D&quot;</span>) <span class="comment"># 3 neighbors</span></span><br><span class="line">ax.scatter(<span class="number">100</span>, knr.predict([[<span class="number">100</span>]]), marker=<span class="string">&#x27;^&#x27;</span>) <span class="comment"># new data</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_3_2_2.png"></p>
<ul>
<li><p>Beyond the scope of the new training set, incorrect values can be predicted.</p>
</li>
<li><p>No matter how big the length is, the weight doesn’t increase anymore.</p>
</li>
</ul>
<p>※ Machine learning models must be trained periodically.</p>
<blockquote>
<p>MLOps (Machine Learning &amp; Opearations)</p>
</blockquote>
<ul>
<li>the essential skill for data scientist, ML engineer.</li>
</ul>
<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><ul>
<li>in statistics:<ul>
<li>The process of finding causal relationships is more important.</li>
<li>4 assumptions (linearity, normality, independence, equal variance)</li>
</ul>
</li>
<li>in ML:<ul>
<li>Predicting results is more important.</li>
<li>R-squared, MAE, RMSE, etc</li>
</ul>
</li>
</ul>
<h2 id="Predict-a-data"><a href="#Predict-a-data" class="headerlink" title="Predict a data"></a>Predict a data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lr = LinearRegression()</span><br><span class="line"></span><br><span class="line">lr.fit(train_input, train_target)</span><br><span class="line"><span class="built_in">print</span>(lr.predict([[<span class="number">50</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[1241.83860323]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(train_input, train_target)</span><br><span class="line">ax.scatter(train_input[indexes], train_target[indexes], marker=<span class="string">&quot;D&quot;</span>) <span class="comment"># 3 neighbors</span></span><br><span class="line">ax.scatter(<span class="number">50</span>, lr.predict([[<span class="number">50</span>]]), marker=<span class="string">&#x27;^&#x27;</span>) <span class="comment"># new data</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_3_2_3.png"></p>
<h2 id="Regression-equation"><a href="#Regression-equation" class="headerlink" title="Regression equation"></a>Regression equation</h2><ul>
<li>coef_ : regression coefficient(weight)</li>
<li>intercept_ : regression intercept<blockquote>
<p>$y &#x3D; a + bx$</p>
</blockquote>
</li>
<li>coefficient &amp; intercept : model parameter<ul>
<li>Linear Regression is a model-based learning.</li>
<li>KNN Regression is a case-based learning.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.coef_, lr.intercept_)</span><br></pre></td></tr></table></figure>

<pre><code>[39.01714496] -709.0186449535477
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line"><span class="comment"># scatter plot of training set</span></span><br><span class="line">ax.scatter(train_input, train_target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># linear equation from 0 to 50</span></span><br><span class="line">ax.plot([<span class="number">0</span>,<span class="number">50</span>], [<span class="number">0</span>*lr.coef_+lr.intercept_, <span class="number">50</span>*lr.coef_+lr.intercept_])</span><br><span class="line"></span><br><span class="line">ax.scatter(<span class="number">50</span>, lr.predict([[<span class="number">50</span>]]), marker=<span class="string">&quot;^&quot;</span>)</span><br><span class="line">ax.set_label(<span class="string">&quot;length&quot;</span>)</span><br><span class="line">ax.set_label(<span class="string">&quot;weight&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_3_2_4.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.score(train_input, train_target))</span><br><span class="line"><span class="built_in">print</span>(lr.score(test_input, test_target)) <span class="comment"># Underfitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.939846333997604
0.8247503123313558
</code></pre>
<ul>
<li>The model is so simple that it is underfit overall.<ul>
<li>It seems that polynomial regression is needed.</li>
</ul>
</li>
</ul>
<h1 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h1><ul>
<li>coef_ : regression coefficients(weights)</li>
<li>intercept_ : regression intercept<blockquote>
<p>$y &#x3D; a + b_1x_1 + b_2x_2 + … + b_nx_n$</p>
</blockquote>
</li>
</ul>
<h2 id="Predict-a-data-1"><a href="#Predict-a-data-1" class="headerlink" title="Predict a data"></a>Predict a data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Broadcasting in Numpy</span></span><br><span class="line">train_poly = np.column_stack((train_input ** <span class="number">2</span>, train_input))</span><br><span class="line">test_poly = np.column_stack((test_input ** <span class="number">2</span>, test_input))</span><br><span class="line"><span class="built_in">print</span>(train_poly.shape, test_poly.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(42, 2) (14, 2)
</code></pre>
<p>※ Broadcasting in Numpy</p>
<ul>
<li>tutorial : <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">https://numpy.org/doc/stable/user/basics.broadcasting.html</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr2 = LinearRegression()</span><br><span class="line">lr2.fit(train_poly, train_target)</span><br><span class="line"><span class="built_in">print</span>(lr2.predict([[<span class="number">50</span>**<span class="number">2</span>, <span class="number">50</span>]]))</span><br></pre></td></tr></table></figure>

<pre><code>[1573.98423528]
</code></pre>
<h2 id="Regression-equation-1"><a href="#Regression-equation-1" class="headerlink" title="Regression equation"></a>Regression equation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr2.coef_, lr2.intercept_)</span><br></pre></td></tr></table></figure>

<pre><code>[  1.01433211 -21.55792498] 116.0502107827827
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">point = np.arange(<span class="number">15</span>,<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(train_input, train_target)</span><br><span class="line">ax.plot(point,  lr2.coef_[<span class="number">0</span>]*point**<span class="number">2</span> + lr2.coef_[<span class="number">1</span>]*point + lr2.intercept_)</span><br><span class="line">ax.scatter(<span class="number">50</span>, lr2.predict([[<span class="number">50</span>**<span class="number">2</span>, <span class="number">50</span>]]), marker=<span class="string">&quot;^&quot;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;length&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;weight&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/images/Python/ML/ML_ch_3_2_5.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr2.score(train_poly, train_target))</span><br><span class="line"><span class="built_in">print</span>(lr2.score(test_poly, test_target)) <span class="comment"># Underfitting</span></span><br></pre></td></tr></table></figure>

<pre><code>0.9706807451768623
0.9775935108325122
</code></pre>
<ul>
<li>The model has improved a lot, but it is still underfit.<ul>
<li>It seems that a more complex model is needed.</li>
</ul>
</li>
</ul>
<p><em>Ref.) <u> 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어) <u/></em></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/tags/google-colab/">Previous</a></div><div class="pagination-next"><a href="/tags/google-colab/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/tags/google-colab/">1</a></li><li><a class="pagination-link is-current" href="/tags/google-colab/page/2/">2</a></li><li><a class="pagination-link" href="/tags/google-colab/page/3/">3</a></li><li><a class="pagination-link" href="/tags/google-colab/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/images/profile.png" alt="Jiwon Kang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jiwon Kang</p><p class="is-size-6 is-block">Data Scientist</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul, South Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">52</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">29</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/gonekng" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/gonekng"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://instagram.com/gone_kng"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Naver Blog" href="https://blog.naver.com/donumm"><i class="fas fa-blog"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/hexo/"><span class="level-start"><span class="level-item">hexo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/python/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile" href="/categories/python/crawling/"><span class="level-start"><span class="level-item">crawling</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/python/tutorial/"><span class="level-start"><span class="level-item">tutorial</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/r/"><span class="level-start"><span class="level-item">r</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/setting/"><span class="level-start"><span class="level-item">setting</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/setting/data-engineering/"><span class="level-start"><span class="level-item">data engineering</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/setting/development/"><span class="level-start"><span class="level-item">development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/sql/"><span class="level-start"><span class="level-item">sql</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/sql/Oracle/"><span class="level-start"><span class="level-item">Oracle</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-17T07:14:08.000Z">2022-10-17</time></p><p class="title"><a href="/2022/10/17/Setting/Git%20Installation%20in%20Windows11/">Git Installation in Windows11</a></p><p class="categories"><a href="/categories/setting/">setting</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-03T06:53:01.000Z">2022-05-03</time></p><p class="title"><a href="/2022/05/03/Setting/Setting%20VS%20Code%20for%20Web%20Development/">Setting VS Code for Web Development</a></p><p class="categories"><a href="/categories/setting/">setting</a> / <a href="/categories/setting/development/">development</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-02T03:08:45.000Z">2022-05-02</time></p><p class="title"><a href="/2022/05/02/SQL/SQL%20TEST%206-7/">SQL TEST 6-7</a></p><p class="categories"><a href="/categories/sql/">sql</a> / <a href="/categories/sql/Oracle/">Oracle</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-02T00:32:11.000Z">2022-05-02</time></p><p class="title"><a href="/2022/05/02/SQL/SQL%20EXERCISE%206-7/">SQL EXERCISE 6-7</a></p><p class="categories"><a href="/categories/sql/">sql</a> / <a href="/categories/sql/Oracle/">Oracle</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-26T07:02:41.000Z">2022-04-26</time></p><p class="title"><a href="/2022/04/26/SQL/Conneting%20SQL%20Developer%20with%20Github/">Conneting SQL Developer with Github</a></p><p class="categories"><a href="/categories/sql/">sql</a> / <a href="/categories/sql/Oracle/">Oracle</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">27</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/BeautifulSoup/"><span class="tag">BeautifulSoup</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Oracle/"><span class="tag">Oracle</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/airflow/"><span class="tag">airflow</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/apache/"><span class="tag">apache</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/crawling/"><span class="tag">crawling</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-engineering/"><span class="tag">data engineering</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/development/"><span class="tag">development</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/elasticsearch/"><span class="tag">elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/github/"><span class="tag">github</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/google-colab/"><span class="tag">google colab</span><span class="tag">33</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kibana/"><span class="tag">kibana</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">24</span></a></div><div class="control"><a class="tags has-addons" href="/tags/matplotlib/"><span class="tag">matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/numpy/"><span class="tag">numpy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pandas/"><span class="tag">pandas</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pipeline/"><span class="tag">pipeline</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">36</span></a></div><div class="control"><a class="tags has-addons" href="/tags/r/"><span class="tag">r</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/seaborn/"><span class="tag">seaborn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/setting/"><span class="tag">setting</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/spark/"><span class="tag">spark</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sql/"><span class="tag">sql</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/statistic/"><span class="tag">statistic</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/visualization/"><span class="tag">visualization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vscode/"><span class="tag">vscode</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/windows11/"><span class="tag">windows11</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/wsl2/"><span class="tag">wsl2</span><span class="tag">6</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Jiwon&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Jiwon Kang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>