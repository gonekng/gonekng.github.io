{"posts":[{"title":"Creating github blog with hexo","text":"Repesitory 생성 node.js 설치 옵션 - Chocolatey도 같이 설치해야 함 바탕화면 git bash에 입력 node -v : 버전 확인 npm install -g hexo-cli : hexo 뭐시기 설치 hexo init myblog : 바탕화면에 myblog 폴더 생성 myblog 폴더 오른쪽 클릭해서 open foler as pycharm pycharm terminal에 Git Bash 추가한 후 hexo server 입력 github에서 profile - new repesitories - new 클릭 Repository 이름은 myblog로 설정 Creating repository 하면 생성 pycharm terminal Git Bast에 아래 소스 한줄씩 입력 1234567echo &quot;# myblog&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M maingit remote add origin https://github.com/gonekng/myblog.gitgit push -u origin main 4번째 줄에서 나오는 에러를 해결해야 함 git config –global user.email “donumm64@gmail.com” git config –global user.name “gonekng” 그러고 나면 1 file changed, 1 insertion(+) 어쩌고 나옴 5번째 줄 입력하면 master에서 main으로 바뀜 끝까지 입력하고 Sign in 창에서 비밀번호 입력까지 완료 github에서 새로고침하고 ReadMe 파일 확인 소스 코드 설명1234567echo &quot;# myblog&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M maingit remote add origin https://github.com/gonekng/myblog.gitgit push -u origin main echo “# myblog” : README.md 생성 git add 파일명 : 해당 파일명 업로드 git add . : 모든 파일 업로드 git commit : 커밋 메시지 ex) “UPDATE”, “First Commit” git remote : 로컬 폴더와 github 주소를 연동 git push : 최종 업로드 단계 한번 폴더 세팅이 끝나고 나면 3,4,7번째 줄만 입력하면 됨 파일명, 메시지명 바꾸기 마지막 줄도 git push만 입력 hello-world 수정 hello-world.md 파일에서 내용 수정 후 브라우저 열어서 확인 git add . / git commit -m “updated” / git push 차례로 입력 블로그 생성12345$ hexo init myblog$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save 위의 두 줄은 경로를 지정했으면 스킵 아래 세 줄 그대로 pycharm Git Bash 창에 입력 _config.yml 에서 내용 수정 title, subtitle, author 등 수정 후 브라우저에서 확인 url : https://gonekng.github.io 맨 아래 Deployment 에 아래 소스 입력 1234deploy: type: git repo: https://github.com/gonekng/gonekng.github.io.git branch: main 새로운 Repository 이름으로 gonekng.github.io 지정하여 생성 pycharm Git Bash에서 hexo generate 입력 css뭐시기 쫘르륵 나와야함 pycharm Git Bash에서 hexo deploy 입력 Deploy done: git 나와야 함 github에서 해당 Repository 확인 브라우저 URL창에 gonekng.github.io 입력하면 창 열림 추가/수정 할때마다 hexo generate –deploy 입력 새로운 포스트 발행 pycharm Git Bash에서 hexo new “[파일명]” 입력 pycharm에 파일 생성된거 확인 title 및 내용 추가/수정 후 hexo server 에서 확인 hexo generate –hexo deploy 로 github 블로그에 배포 블로그 테마 변경 ICARUS 테마 npm install -S hexo-theme-icarus 입력 hexo config theme icarus 입력 hexo server 에서 에러 나는 경우 npm install --save bulma-stylus@0.8.0 hexo-renderer-inferno@^0.1.3 입력 hexo clean 을 통해 정리한 후 hexo generate –hexo deploy 로 github 블로그에 배포 https://hexo.io/themes/ 에서 테마 정해서 해당 테마의 Github으로 이동 (중국어는 거르자) 최근에도 지속적으로 업데이트 되고 있는지 확인 npm install hexo-theme-[테마명] 입력 hexo config theme [테마명] 입력 R 마크다운 업로드 R 마크다운 소스에서 개요 부분 수정 123output: html_document: keep_md: true R에서 Knit 하면 해당 디렉토리에 md파일 생성됨 myblog &gt; source &gt; _posts 에 복사 pycharm에서 md 파일 열어서 내용 수정 및 배포 R 디렉토리에 있는 blog_files 폴더를 myblog &gt; source &gt; images 에 복사 pycharm에서 md 파일 열어서 이미지 링크 수정 및 배포","link":"/2022/03/17/notion_github/"},{"title":"Establishing an Airflow Data Pipeline","text":"Step 01. Create a Virtual Data Create dags foler below (venv) airflow-test folder. 1234$ mkdir dags$ lsairflow-webserver.pid airflow.cfg airflow.db dags logs venv webserver_config.py Install the necessary libraries. 1$ pip3 install faker pandas Create data folder and write python file in the folder to create a virtual data. filename : step01_writecsv.py 123$ mkdir data$ cd data$ vi step01_writecsv.py 123456789101112131415161718192021***# step01_writecsv.py***from faker import Fakerimport csvoutput = open('data.csv','w')fake = Faker()header = ['name','age','street','city','state','zip','lng','lat']mywriter = csv.writer(output)mywriter.writerow(header)for r in range(1000): mywriter.writerow([[fake.name](http://fake.name/)(), fake.random_int(min=18, max=80, step=1), fake.street_address(), fake.city(), fake.state(), fake.zipcode(), fake.longitude(), fake.latitude()])output.close() Run the file above and make sure that the data is well generated. 1234$ python3 step01_writecsv.py$ lsdata.csv step01_writecsv.py Step 2. Establish csv2join file Write code to build CSV and JSON transform files in dags folder. filename : csv2join.py 1$ vi csv2json.py 123456789101112131415161718192021222324252627282930313233343536***# csv2join.py***import datetime as dtfrom datetime import timedeltafrom airflow import DAGfrom airflow.operators.bash import BashOperatorfrom airflow.operators.python import PythonOperatorimport pandas as pddef csvToJson(): df=pd.read_csv('data/data.csv') for i,r in df.iterrows(): print(r['name']) df.to_json('fromAirflow.json',orient='records')default_args = { 'owner': 'evan', 'start_date': dt.datetime(2020, 3, 18), 'retries': 1, 'retry_delay': dt.timedelta(minutes=5),}with DAG('MyCSVDAG', default_args=default_args, schedule_interval=timedelta(minutes=5), # '0 * * * *', ) as dag: print_starting = BashOperator(task_id='starting', bash_command='echo &quot;I am reading the CSV now.....&quot;') csvJson = PythonOperator(task_id='convertCSVtoJson', python_callable=csvToJson)print_starting &gt;&gt; csvJson Run the csv2json.py above. 1$ python3 csv2json.py Step 04. Run Webserver and Scheduler Simultaneously Open a separate terminal and run the webserver and scheduler. 12$ airflow webserver -p 8080$ airflow scheduler Check if it works normally in the Web UI. Reference https://dschloe.github.io/python/data_engineering/ch03_reading_writing_file/airflow_csv2json_sample/","link":"/2022/04/15/Setting/Establishing%20an%20Airflow%20Data%20Pipeline/"},{"title":"Link VSCode with Remote WSL","text":"Step 1. Install VSCode URL : https://code.visualstudio.com/download Download the System Installer for each OS. Check ‘Add to PATH’ and reboot after installation. Step 2. Link Remote WSL Install Remote WSL in Extension tab of VSCode. (File tab → Open Folder) Select the airflow-test folder that WSL installed. (Terminal → New Terminal) Open a new terminal and add a WSL terminal. Activate the virtual environment in WSL terminal. Run a python code and check if it is printed well. ex) main.py Reference https://dschloe.github.io/settings/vscode_wsl2/","link":"/2022/04/15/Setting/Link%20VSCode%20with%20Remote%20WSL/"},{"title":"ElasticSearch and Kibana Setting in WSL 2","text":"Step 1. Install Package Update the system package and install a package related to HTTPS. 12$ sudo apt update$ sudo apt install apt-transport-https Install Java and check the version of Java. 12345$ sudo apt install openjdk-11-jdk$ java -versionopenjdk 11.0.14.1 2022-02-08OpenJDK Runtime Environment (build 11.0.14.1+1-Ubuntu-0ubuntu1.20.04)OpenJDK 64-Bit Server VM (build 11.0.14.1+1-Ubuntu-0ubuntu1.20.04, mixed mode, sharing) Open the vi editor to set the java environment variable. 1$ sudo vi /etc/environment Insert the following sentence in vi editor. JAVA_HOME=&quot;/usr/lib/jvm/java-11-openjdk-amd64&quot; Update the environment variables and check the contents. 1234$ source /etc/environment$ echo $JAVA_HOME/usr/lib/jvm/java-11-openjdk-amd64 Step 2. Install ElasticSearch Check the GPG keys. 123$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key addOK Add a library and install ElasticSearch. 1234$ sudo sh -c 'echo &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; &gt; /etc/apt/sources.list.d/elastic-7.x.list'$ sudo apt-get update$ sudo apt-get install elasticsearch Step 3. Start ElasticSearch Start EleasticSearch 1234$ sudo systemctl start elasticsearchSystem has not been booted with systemd as init system (PID 1). Can't operate.Failed to connect to bus: Host is down If the above error is printed, add the following command. 12$ sudo -b unshare --pid --fork --mount-proc /lib/systemd/systemd --system-unit=basic.target$ sudo -E nsenter --all -t $(pgrep -xo systemd) runuser -P -l $USER -c &quot;exec $SHELL&quot; Enable the ElasticSearch and start the service. 123456$ sudo systemctl enable elasticsearchSynchronizing state of elasticsearch.service with SysV service script with /lib/systemd/systemd-sysv-install.Executing: /lib/systemd/systemd-sysv-install enable elasticsearch$ sudo systemctl start elasticsearch Ensure that the service is actually operational. 12345678910111213141516171819$ curl -X GET &quot;localhost:9200/&quot;{ &quot;name&quot; : &quot;DESKTOP-JM1I3QF&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;ma7ulQQ_RL-Y3ZNsjz0ZVw&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;7.17.2&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;deb&quot;, &quot;build_hash&quot; : &quot;de7261de50d90919ae53b0eff9413fd7e5307301&quot;, &quot;build_date&quot; : &quot;2022-03-28T15:12:21.446567561Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.11.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} Check whether it is printed well on the window screen. Step 4. Install and Start Kibana Install and enable Kibana service 12345$ sudo apt-get install kibana$ sudo systemctl enable kibanaSynchronizing state of kibana.service with SysV service script with /lib/systemd/systemd-sysv-install.Executing: /lib/systemd/systemd-sysv-install enable kibana Start Kibana service and check the status 1234567891011121314$ sudo systemctl start kibana$ sudo systemctl status kibana● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2022-04-14 11:53:07 KST; 21min ago Docs: https://www.elastic.co Main PID: 303 (node) Tasks: 11 (limit: 4646) Memory: 599.0M CGroup: /system.slice/kibana.service └─303 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli/dis&gt;Apr 14 11:53:07 DESKTOP-JM1I3QF systemd[1]: Started Kibana. Step 5. Check Kibana WebUI Make sure it connects to ElasticSearch well URL : http://localhost:5601/ Reference https://dschloe.github.io/settings/elasticsearch_kibana_wsl2/ https://www.how2shout.com/how-to/install-uninstall-elasticsearch-ubuntu-19-04-18-04-16-04.html","link":"/2022/04/15/Setting/ElasticSearch%20and%20Kibana%20Setting%20in%20WSL%202/"},{"title":"Apache-Airflow Setting in Windows11 (WSL 2)","text":"Step 1. Create a virtual environment Install pip and virtualenv package 12$ sudo apt install python3-pip$ sudo pip3 install virtualenv Create a virtual environment in c:\\airflow-test folder 1234567$ virtualenv venvcreated virtual environment CPython3.8.10.final.0-64 in 29086mscreator CPython3Posix(dest=/mnt/c/airflow-test/venv, clear=False, no_vcs_ignore=False, global=False)seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/donumm/.local/share/virtualenv)added seed packages: pip==22.0.4, setuptools==62.1.0, wheel==0.37.1activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator Open .bashrc file and add the following code. 123$ vi ~/.bashrc*export AIRFLOW_HOME=/mnt/c/airflow-test* Update the code and make sure it’s actually reflected. 1234$ source ~/.bashrc$ echo $AIRFLOW_HOME/mnt/c/airflow-test Connect to virtual environment. 1$ source venv/bin/activate ※ Airflow must be installed in the virtual environment and executed in the virtual environment. Step 4. Install Apache Airflow Install PostgreSQL, Slack, and Celery packages 1pip3 install 'apache-airflow[postgres, slack, celery]' Initialize the DB to run the airflow. 1$ airflow db init Register an username and password of the airflow 12***# Create a new user***$ airflow users create --username airflow --password airflow --firstname Jiwon --lastname Kang --role Admin --email donumm64@gmail.co 123456***# Check the user list***$ airflow users listid | username | email | first_name | last_name | roles===+==========+====================+============+===========+======1 | donumm | donumm64@gmail.com | Jiwon | Kang | Admin Open airflow.cfg file, and change the value of load_examples from True to False. Reset the db in terminal. 123$ airflow db reset...Proceed? (y/n) Y Run the airflow webserver and scheduler. 12$ airflow webserver -p 8080$ airflow scheduler Connect the airflow webserver. URL : http://localhost:8080/login/ Reference https://dschloe.github.io/settings/apache_airflow_using_wsl2/","link":"/2022/04/15/Setting/Apache-Airflow%20Setting%20in%20Windows11%20(WSL%202)/"},{"title":"Spark Installation in WSL2","text":"Step 1. Install required files Install java and spark file. (Skip if already installed.) 123$ sudo apt-get install openjdk-8-jdk$ sudo wget https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz$ sudo tar -xvzf spark-3.2.0-bin-hadoop3.2.tgz Step 2. Set environment variables Open .bashrc file and add the code below. 12345export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export SPARK_HOME=/mnt/c/hadoop/spark-3.2.0-bin-hadoop3.2export PATH=$JAVA_HOME/bin:$PATHexport PATH=$SPARK_HOME/bin:$PATHexport PYSPARK_PYTHON=/usr/bin/python3 Update the code and make sure it’s actually reflected. 1234source ~/.bashrcecho SPARK_HOME/mnt/c/hadoop/spark-3.2.0-bin-hadoop3.2 Step 3. Run Pyspark Run pyspark in the path. Run the code below in the CMD and check the result printed. 1234&gt;&gt;&gt; rd = sc.textFile(&quot;README.md&quot;)&gt;&gt;&gt; rd.count()109 Step 4. Deploy in Web browser. Create a new directory temp, and virtual environment. 12$ mkdir temp &amp;&amp; cd temp$ virtualenv venv Connect to virtual environment and install pyspark. 12$ source venv/bin/activate$ pip install pyspark Create a new directory and README.md file. 12$ mkdir data &amp;&amp; cd data$ vi README.md *This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a text file. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is installed. As with the Scala and Java examples, we use a SparkSession to create Datasets. For applications that use custom classes or third-party libraries, we can also add code dependencies to spark-submit through its –py-files argument by packaging them into a .zip file (see spark-submit –help for details). SimpleApp is simple enough that we do not need to specify any code dependencies. We can run this application using the bin/spark-submit script:* Back to temp and create SampleApp.py. 12$ cd ..$ vi SampleApp.py 123456789101112131415***# SampleApp.py***from pyspark.sql import SparkSessionlogFile = &quot;data/README.md&quot; # Should be some file on your systemspark = SparkSession.builder.appName(&quot;SimpleApp&quot;).getOrCreate()logData = spark.read.text(logFile).cache()numAs = logData.filter(logData.value.contains('a')).count()numBs = logData.filter(logData.value.contains('b')).count()print(&quot;Lines with a: %i, lines with b: %i&quot; % (numAs, numBs))input(&quot;Typing....&quot;)spark.stop() Run the SimpleApp.py 1$SPARK_HOME/bin/spark-submit --master local[4] SimpleApp.py Check the address below and copy it. Enter the corresponding address in the web browser and check the web UI.","link":"/2022/04/20/Setting/Spark%20Installation%20in%20WSL2/"},{"title":"Setting VS Code for Web Development","text":"Install Beautify in Extension tap. Functional contribution → Command → Copy HookyQR.beautify Type Ctrl + Shift + P and paste HookyQR.beautify in the search window. Set the key binding of Beauty Selection to Ctrl + Alt + L. Install Live Server in Extension tap. Install Auto Rename Tag in Extension tap.","link":"/2022/05/03/Setting/Setting%20VS%20Code%20for%20Web%20Development/"},{"title":"R_markdown Sample","text":"개요 R에서 만든 sample 파일 github에 업로드 R MarkdownThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 1summary(cars) 1234567## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Including PlotsYou can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.","link":"/2022/03/18/R/R_sample/"},{"title":"R_basic_statistics","text":"통계 분석 개요 기술통계(discriptive Statistics) : 평균, 최솟값, 최댓값, 중앙값 등 데이터의 특징을 서술하는 것 추론통계(inferential Statistics) : 변수 간의 관계를 파악하여 변수 간 인과관계나 새로운 사실을 밝혀내는 것 평균 차이 검정 : 집단별 평균의 차이가 실제로 있는가를 검정하는 것 교차분석 : 범주형 변수로 구성된 집단들의 관련성을 검정하는 것 상관관계분석 : 변수 간의 상관관계(correlation)를 알아보는 것 상관관계는 한 변수가 변화하면 다른 변수도 변화하는 관계를 의미 상관계수(r) : 변화의 강도와 방향을 나타내는 계수 (-1 &lt;= r &lt;= 1) 수치가 클수록 영향을 주는 강도가 크며, ‘+’는 정의 관계, ‘-‘는 역의 관계 회귀분석 : 독립변수와 종속변수 간의 인과관계를 분석하는 것 독립변수 : 영향을 주는 변수 / 종속변수 : 영향을 받는 변수 단순회귀분석 : 종속변수 1개, 독립변수 1개 (y = a + b*x) 다중회귀분석 : 종속변수 1개, 독립변수 2개 이상 (y = a + b1x1 + b2x2 +…) 통계 검정 가설(hypothesis) 어떤 현상을 설명하기 위해서 가정하는 명제 귀무가설(H0) : 처음부터 기각될 것으로 예상되는 가설 (영가설) 대립가설(H1) : 귀무가설이 기각될 경우 받아들여지는 가설 유의수준(significance level, p값) 귀무가설이 맞는데도 대립가설을 채택할 확률 (제1종 오류의 최대 허용 범위) 가설 검정에서 인정하는 유의수준 : 5%, 1%, 0.1% 신뢰수준(confidence level) : 신뢰할 수 있는 범위 (1-유의수준) 척도(scale) 명목척도 : 측정대상의 특성이나 범주를 구분하는 척도 등번호, 성별, 인종, 지역 등 산술 연산을 할 수 없음 서열척도 : 측정대상의 등급순위를 나타내는 척도 계급, 사회계층, 자격등급 등 산술 연산을 할 수 없음 척도 간의 거리나 간격을 나타내지는 않음 등간척도 : 측정대상을 일정한 간격으로 구분한 척도 온도, 학력, 시험점수 등 서열 뿐만 아니라 거리와 간격도 표현 가능 덧셈, 뺄셈을 할 수 있음 비율척도 : 측정대상을 비율로 나타낼 수 있는 척도 연령, 키, 무게 등 사칙연산을 모두 할 수 있음 통계 분석 사례1. 두 집단의 평균 차이 검정 - 독립표본 t검정(t.test()) 독립변수는 명목척도, 종속변수는 등간척도 또는 비율척도이어야 함 귀무가설 : auto와 manual의 cty평균은 차이가 없다. 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer : chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1t.test(data=mpg1, cty~trans) 1234567891011## ## Welch Two Sample t-test## ## data : cty by trans## t = -4.5375, df = 132.32, p-value = 1.263e-05## alternative hypothesis : true difference in means between group auto and group manual is not equal to 0## 95 percent confidence interval:## -3.887311 -1.527033## sample estimates:## mean in group auto mean in group manual ## 15.96815 18.67532 &gt;&gt; p-value = 1.263e-05, 귀무가설 기각(유의수준 .05에서 유의미한 차이가 있음)2. 교차분석 - 카이제곱 검정(chisq.test()) 귀무가설 : trans에 따라 drv의 차이가 없다. 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer : chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1table(mpg1$trans, mpg1$drv) 1234## ## 4 f r## auto 75 65 17## manual 28 41 8 1prop.table(table(mpg1$trans, mpg1$drv),1) 1234## ## 4 f r## auto 0.4777070 0.4140127 0.1082803## manual 0.3636364 0.5324675 0.1038961 1chisq.test(mpg1$trans, mpg1$drv) 12345## ## Pearson's Chi-squared test## ## data : mpg1$trans and mpg1$drv## X-squared = 3.1368, df = 2, p-value = 0.2084 &gt;&gt; p-value = 0.2084, 귀무가설 채택(유의수준 .05에서 유의미한 차이가 없음)3) 상관관계분석 - cor.test()- 귀무가설 : cty와 hwy는 상관관계가 없다. 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer : chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1cor.test(mpg1$cty, mpg1$hwy) 1234567891011## ## Pearson's product-moment correlation## ## data : mpg1$cty and mpg1$hwy## t = 49.585, df = 232, p-value &lt; 2.2e-16## alternative hypothesis : true correlation is not equal to 0## 95 percent confidence interval:## 0.9433129 0.9657663## sample estimates:## cor ## 0.9559159 &gt;&gt; p-value &lt; 2.2e-16, 귀무가설 기각(유의수준 .05에서 상관관계가 있음)&gt;&gt; 상관계수 r = 0.9559159 (매우 높은 상관관계)4. 단순회귀분석 - lm() 독립변수와 종속변수가 모두 등간척도 또는 비율척도이어야 함 귀무가설 : disp는 mpg에 영향을 주지 않는다. 12RA &lt;- lm(data=mtcars, mpg~disp)summary(RA) 123456789101112131415161718## ## Call:## lm(formula = mpg ~ disp, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 ***## disp -0.041215 0.004712 -8.747 9.38e-10 ***## ---## Signif. codes : 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error : 3.251 on 30 degrees of freedom## Multiple R-squared : 0.7183, Adjusted R-squared : 0.709 ## F-statistic : 76.51 on 1 and 30 DF, p-value : 9.38e-10 12plot(data=mtcars, mpg~disp)abline(RA, col=&quot;red&quot;) &gt;&gt; p-value = 9.38e-10, 귀무가설 기각(유의수준 .05에서 회귀모형이 적합함)&gt;&gt; 절편(Intercept) = 29.599855 (유의수준 .05에서 유의함)&gt;&gt; 회귀계수(Estimate) = -0.041215 (유의수준 .05에서 유의함)&gt;&gt; 회귀식 : mpg = 29.599855 - 0.041215 * disp&gt;&gt; 수정된 결정계수(Adjusted R-Squared) = .7095. 다중회귀분석 - lm()12RA &lt;- lm(data=mtcars, mpg~disp+hp+wt)summary(RA) 1234567891011121314151617181920## ## Call:## lm(formula = mpg ~ disp + hp + wt, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -3.891 -1.640 -0.172 1.061 5.861 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.105505 2.110815 17.579 &lt; 2e-16 ***## disp -0.000937 0.010350 -0.091 0.92851 ## hp -0.031157 0.011436 -2.724 0.01097 * ## wt -3.800891 1.066191 -3.565 0.00133 ** ## ---## Signif. codes : 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error : 2.639 on 28 degrees of freedom## Multiple R-squared : 0.8268, Adjusted R-squared : 0.8083 ## F-statistic : 44.57 on 3 and 28 DF, p-value : 8.65e-11 &gt;&gt; p-value = 8.65e-11, 귀무가설 기각(유의수준 .05에서 회귀모형이 적합함)&gt;&gt; 절편(Intercept) = 29.599855 (유의수준 .05에서 유의함)&gt;&gt; dist의 계수 = -0.000937 (유의수준 .05에서 통계적으로 유의하지 않음)&gt;&gt; hp의 계수 = -0.031157 (유의수준 .05에서 유의함)&gt;&gt; wt의 계수 = -3.800891 (유의수준 .05에서 유의함)&gt;&gt; 회귀식 : mpg = 29.599855 - 0.000937 * disp - 0.031157 * hp - 3.800891 * wt&gt;&gt; 수정된 결정계수(Adjusted R-Squared) = .8083","link":"/2022/03/15/R/R_basic_stat/"},{"title":"Spark Installation on Windows11","text":"Step 1. Install Java DK Download Windows Installer. URL : https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html Run the download file as an administrator. Modify the path as shown in the picture below. (Be careful not to include spaces in the path name.) Step 2. Install Spark Download Spark .tgz file. (Click the link in the images below.) URL : https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz Download WinRAR to unzip the .tgz compressed file, and run as an administrator. URL : https://www.rarlab.com/download.htm Open the Spark file with WinRAR and extract to the folder. Rename the folder to spark, and copy and paste under the C drive. Open spark\\conf\\log4j.properties file with memo pad, and change the log4j.rootCategory value from INFO to ERROR. Step 3. Install Winutils Download winutils.exe. (Check the version of Spark.) URL : ‣ Create a foler winutils\\bin, and copy and paste winutils.exe. Run a CMD as an administrator, and write the code below. 1234&gt; cd c:\\winutils\\bin&gt; winutils.exe chmod 777 \\tmp\\hive****ChangeFileModeByMask error (2): ??? ??? ?? ? ????. If the above error occurs, create the tmp\\hive folder under the C drive and run it again. Step 4. Setting environment variables Create a new user variable SPARK_HOME, and set the value as the path of spark folder. Create a new user variable JAVA_HOME, and set the value as the path of jdk folder. Create a new user variable HADOOP_HOME, and set the value as the path of winutils folder. Edit the Path variable Insert %SPARK_HOME%\\bin and %JAVA_HOME%\\bin. Create a new user variable PYSPARK_PYTHON, and set the value as PYTHON. Run a CMD as an administrator, and run pyspark in the c:\\spark path. Run the code below in the CMD and check the result printed. 1234&gt; rd = sc.textFile(&quot;README.md&quot;)&gt; rd.count()109 Create new user variables and set the value. PYSPARK_DRIVER_PYTHON ; jupyter PYSPARK_DRIVER_PYTHON_OPTS ; notebook Reference https://dschloe.github.io/python/python_edu/00_settings/spark_installation_windows_10/","link":"/2022/04/19/Setting/Spark%20Installation%20on%20Windows11/"},{"title":"WSL 2 Installation in Windows11","text":"Step 1. Enable WSL-related features by DISM Run Windows Terminal as administrator Enable Microsoft-Windows-Subsystem-Linux Features 1$ dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart Enable the VirtualMachinePlatform feature Reboot if the operation is completed successfully. 1$ dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Step 2. WSL2 Kernel Update Install the update file from the link below. URL : https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi Open Windows terminal and change WSL version to 2. 1$ wsl --set-default-version 2 Install Ubuntu, the most popular Linux distribution, and run as administrator. ![](/images/Setting/wsl2/Untitled 1.png) In Ubuntu, Set the username and password. Check the currently installed version with wsl -l -v 123$ wsl -l -v NAME STATE VERSION* Ubuntu Running 2 If it says version 1, execute the following command. 12345$ wsl --set-version Ubuntu 2변환이 진행 중입니다. 몇 분 정도 걸릴 수 있습니다...WSL 2와의 주요 차이점에 대한 자세한 내용은 [https://aka.ms/wsl2를](https://aka.ms/wsl2%EB%A5%BC) 참조하세요변환이 완료되었습니다. Make sure that it says version 2. 123$ wsl -l -v NAME STATE VERSION* Ubuntu Running 2 Reference https://www.lainyzine.com/ko/article/how-to-install-wsl2-and-use-linux-on-windows-10/#google_vignette","link":"/2022/04/15/Setting/WSL%202%20Installation%20in%20Windows11/"},{"title":"Conneting SQL Developer with Github","text":"Step 1. Preliminary work Register in github and install git. Create a new public repository. (my repository name : Sql_edu) Step 2. SQL Developer Setting SQL Developer &gt; 팀 &gt; Git &gt; 복제 Click the Next button. Enter the URL of the repository and Username, Password. Repository must be Public so that there is no authentication error. Select main and move on. Check the information for the git and move on. The process of replication is done. Step 3. Git Push Test SQL Developer &gt; 파일 &gt; 새로 만들기 &gt; 모든 항목 Create a new SQL file(temp.sql) and set a directory linked to github. Create and run a simple query, save it, and click ‘commit all’ in Git. Enter your name in ‘Author’ and ‘Commiters’ and click OK. Click ‘push’ in Git. Proceed with the push operation in order. If the error below occurs, an authentication token must be issued from Github. Github &gt; Profile &gt; Settings &gt; Developer Settings &gt; Personal Access Tokens &gt; Generate new token Enter the repository name and select the repo in Select scopes. The generated token number will not be shown again, so you must copy it immediately and remember it separately. Perform the push operation again and check that it is uploaded normally.","link":"/2022/04/26/SQL/Conneting%20SQL%20Developer%20with%20Github/"},{"title":"Oracle 19c Installation in Windows11","text":"Step 1. Install Oracle Database Run the setup file as administrator and follow the procedure below. If the following error occurs, go back to the beginning and change to ‘Software Only Settings’. Creating and configuring a single instance database : Installing myoracle and database Software Only Settings : Installing myoracle only Run a cmd as administrator and enter the code below. (if you changed to ‘Software Only Settings’.) C:\\sql_lecture\\WINDOWS.X64_193000_db_home&gt;**dbca** If the error above occurs again, proceed as follows. Step 2. Create Tablespace by SQL Plus Run SQL Plus as administrator and enter the username and password. Enter the SQL code below. 12# Create a new tablespaceCREATE TABLESPACE myts DATAFILE 'C:\\sql_lecture\\oradata\\MYORACLE\\myts.dbf' SIZE 100M AUTOEXTEND ON NEXT 5M; 12# Create a new userCREATE USER ora_user IDENTIFIED BY jiwon DEFAULT TABLESPACE MYTS TEMPORARY TABLESPACE TEMP; 1234# Grant a DBA role to the userGRANT DBA TO ora_user;권한이 부여되었습니다. 1234# Connect to the database as the userconnect ora_user/jiwon;연결되었습니다. 123456# Print out the currently logged-in usernameselect user from dual;USER--------------------------------------------------------------------------------ORA_USER Step 3. Install SQL Developer Run the setup file. Click No if the warning below occurs. Run a CMD as administrator and enter the code below. C:\\WINDOWS\\system32&gt;lsnrctl status If an Unknown error occurs as described above, run the Net Configuration Assistant. If you input the code at the CMD again, the listener information is printed normally. Create a new Database Access. Tool &gt; Setting &gt; Database &gt; NLS Enter YYYY/MM/DD HH24:MI:SS in ‘Time Record Format’. Write the query below and check the result. 1SELECT user from DUAL; Create a backup folder under the C drive and download expall.dmp and expcust.dmp URL : https://github.com/gilbutITbook/006696/tree/master/01장 환경설정 Run a CMD as administrator and enter the code below at the backup folder. 1imp ora_user/evan file=expall.dmp log=empall.log ignore=y grants=y rows=y indexes=y full=y 1imp ora_user/evan file=expcust.dmp log=expcust.log ignore=y grants=y rows=y indexes=y full=y Write the query below and check the result. 1SELECT table_name FROM user_tables; In SQL Plus, make sure that the user is created correctly.","link":"/2022/04/25/SQL/Oracle%2019c%20Installation%20in%20Windows11/"},{"title":"SQL EXERCISE 6-7","text":"CHAPTER 06 ‘오라클 SQL과 PL/SQL을 다루는 기술 (길벗)’ Q1.101번 사원에 대해 아래의 결과를 산출하는 쿼리를 작성해 보자. 123---------------------------------------------------------------------------------------사번 사원명 job명칭 job시작일자 job종료일자 job수행부서명--------------------------------------------------------------------------------------- A1.1234567891011121314SELECT a.employee_id 사번 , a.emp_name 사원명 , b.job_title job 명칭 , c.start_date job 시작일자 , c.end_date job 종료일자 , d.department_name FROM employees a , jobs b , job_history c , departments d WHERE a.employee_id = c.employee_id AND b.job_id = c.job_id AND c.department_id = d.department_id AND a.employee_id = 101; 필요한 컬럼 및 테이블 사번(employee_id), 사원명(emp_name) → employees job명칭(job_title) → jobs job시작일자(start_date), job종료일자(end_date) → job_history job수행부서명(department_name) → departments 테이블 조인 조건 employees &amp; job_history → employee_id jobs &amp; job_history → job_id job_history &amp; departments → department_id 기타 조건 101번 사원에 대한 정보 : a.employee_id = 101 Q2.아래의 쿼리를 수행하면 오류가 발생한다. 오류의 원인은 무엇인가? 12345select a.employee_id, a.emp_name, b.job_id, b.department_idfrom employees a,job_history bwhere a.employee_id = b.employee_id(+)and a.department_id(+) = b.department_id; A2.(+) 연산자를 활용한 외부 조인의 경우 한쪽 방향으로만 가능하고, 이때 (+) 연산자는 데이터가 없는 테이블의 컬럼에만 붙여야 한다. 따라서, 위의 쿼리에서는 마지막 줄을 and a.department_id = b.department_id(+)로 수정해야 한다. Q3.외부조인시 (+)연산자를 같이 사용할 수 없는데, IN절에 사용하는 값이 1개인 경우는 사용 가능하다. 그 이유는 무엇일까? A3.IN절에 사용하는 값이 1개인 경우는 등호를 사용하는 것과 같은 의미이므로 사용 가능하다. Q4.다음의 쿼리를 ANSI 문법으로 변경해 보자. 1234567SELECT a.department_id , a.department_name FROM departments a , employees b WHERE a.department_id = b.department_id AND b.salary &gt; 3000 ORDER BY a.department_name; A4.123456SELECT a.department_id, a.department_name FROM departments a INNER JOIN employees b On (a.department_id = b.department_id AND b.salary &gt; 3000) ORDER BY a.department_name; 위의 쿼리는 departments 테이블과 employees 테이블의 내부 조인이다. ANSI 문법에서 내부 조인은 FROM절에서 INNER JOIN 으로 수행하며,조인 조건은 ON 절에 명시한다. Q5.다음은 연관성 있는 서브쿼리이다. 이를 연관성 없는 서브쿼리로 변환해 보자. 123456SELECT a.department_id , a.department_name FROM departments a WHERE EXISTS ( SELECT 1 FROM job_history b WHERE a.department_id = b.department_id ); A5.123456SELECT a.department_id , a.department_name FROM departments a WHERE a.department_id IN (SELECT b.department_id FROM job_history b); 위의 쿼리는 job_history 테이블에 존재하는 department_id에 대해departments 테이블의 department_id와 department_name을 출력한다. 이를 연관성 없는 서브쿼리로 변환하기 위해조인 조건 대신 IN 연산자를 통해 메인 쿼리의 조건으로 활용했다. Q6.연도별 이태리 최대매출액과 사원을 작성하는 쿼리를 학습했다. 이를 기준으로 최대 매출액, 최소매출액, 해당 사원을 조회하는 쿼리를 작성해 보자. A6.1234567891011121314151617181920212223242526272829303132333435363738SELECT emp.sales_year , emp.employee_id , emp2.emp_name , emp.amount_sold FROM (SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = 'Italy' GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id) emp , (SELECT sales_year , MAX(amount_sold) AS max_sold , MIN(amount_sold) AS min_sold FROM (SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = 'Italy' GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id) k GROUP BY sales_year) sale , employees emp2 WHERE emp.sales_year = sale.sales_year AND (emp.amount_sold = sale.max_sold OR emp.amount_sold = sale.min_sold) AND emp.employee_id = emp2.employee_id ORDER BY sales_year; 서브쿼리 1 : 연도, 사원별 이탈리아 매출액 (emp) sales, customers, countries를 조인하여 매출액 합계 계산 123456789101112SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = 'Italy' GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id 서브쿼리 2: 연도별 최대, 최소 매출액 (sale) emp 서브쿼리에서 연도별 최대, 최소값 계산 12345678910111213141516SELECT sales_year , MAX(amount_sold) AS max_sold , MIN(amount_sold) AS min_sold FROM (SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = 'Italy' GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id) k GROUP BY sales_year CHAPTER 07Q1.계층형 쿼리 응용편에서 LISTAGG 함수를 사용해 다음과 같이 로우를 컬럼으로 분리했었다. 12345SELECT department_id,LISTAGG(emp_name, ',') WITHIN GROUP (ORDER BY emp_name) as empnamesFROM employeesWHERE department_id IS NOT NULLGROUP BY department_id; LISTAGG 함수 대신 계층형 쿼리, 분석함수를 사용해서 위 쿼리와 동일한 결과를 산출하는 쿼리를 작성해 보자. A1.123456789101112131415SELECT department_id , SUBSTR(SYS_CONNECT_BY_PATH(emp_name, ','),2) empnames FROM ( SELECT emp_name , department_id , COUNT(*) OVER (PARTITION BY department_id) cnt , ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY emp_name) rowseq FROM employees WHERE department_id IS NOT NULL ) WHERE rowseq = cnt START WITH rowseq = 1 CONNECT BY PRIOR rowseq + 1 = rowseq AND PRIOR department_id = department_id; 서브쿼리 : 부서별 사원명, 사원 수, 행 번호 구하기 부서별 파티션 : PARTITION BY department_id ORDER BY emp_name 1234567SELECT emp_name , department_id , COUNT(*) OVER (PARTITION BY department_id) cnt , ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY emp_name) rowseq FROM employees WHERE department_id IS NOT NULL 각 파티션의 마지막 행에 대하여(WHERE rowseq = cnt)파티션의 첫 행부터(START WITH rowseq = 1)부서번호가 같은 직전 행까지(CONNECT BY PRIOR rowseq + 1 = rowseq AND PRIOR department_id = department_id)의 emp_name을연결하여 나타낸다.(SUBSTR(SYS_CONNECT_BY_PATH(emp_name, ','),2)) Q2.아래의 쿼리는 사원테이블에서 JOB_ID가 ‘SH_CLERK‘인 사원을 조회하는 쿼리이다. 12345678910111213141516SELECT employee_id, emp_name, hire_dateFROM employeesWHERE job_id = 'SH_CLERK'ORDER By hire_date;EMPLOYEE_ID EMP_NAME HIRE_DATE ----------- -------------------- ------------------- 184 Nandita Sarchand 2004/01/27 00:00:00 192 Sarah Bell 2004/02/04 00:00:00 185 Alexis Bull 2005/02/20 00:00:00 193 Britney Everett 2005/03/03 00:00:00 188 Kelly Chung 2005/06/14 00:00:00.... .... 199 Douglas Grant 2008/01/13 00:00:00 183 Girard Geoni 2008/02/03 00:00:00 사원테이블에서 퇴사일자(retire_date)는 모두 비어있는데,위 결과에서 사원번호가 184인 사원의 퇴사일자는 다음으로 입사일자가 빠른 192번 사원의 입사일자라고 가정해서다음과 같은 형태로 결과를 추출해낼 수 있도록 쿼리를 작성해 보자. (입사일자가 가장 최근인 183번 사원의 퇴사일자는 NULL이다) 1234567891011EMPLOYEE_ID EMP_NAME HIRE_DATE RETIRE_DATE----------- -------------------- ------------------- --------------------------- 184 Nandita Sarchand 2004/01/27 00:00:00 2004/02/04 00:00:00 192 Sarah Bell 2004/02/04 00:00:00 2005/02/20 00:00:00 185 Alexis Bull 2005/02/20 00:00:00 2005/03/03 00:00:00 193 Britney Everett 2005/03/03 00:00:00 2005/06/14 00:00:00 188 Kelly Chung 2005/06/14 00:00:00 2005/08/13 00:00:00.... .... 199 Douglas Grant 2008/01/13 00:00:00 2008/02/03 00:00:00 183 Girard Geoni 2008/02/03 00:00:00 A2.123456789SELECT employee_id , emp_name , hire_date , LEAD(hire_date) OVER (PARTITION BY job_id ORDER BY hire_date) AS retire_date FROM employees WHERE job_id = 'SH_CLERK' ORDER BY hire_date; 문제에서 요구하는 퇴사일자(retire_date)는입사일자로 정렬했을 때 다음 사원의 입사일자(hire_date)와 같다. 따라서, 다음 행의 데이터를 가져오는 LEAD(hire_date) 함수를 통해각 사원의 퇴사일자(retire_date)를 산출할 수 있다. Q3.sales 테이블에는 판매데이터, customers 테이블에는 고객정보가 있다.2001년 12월 판매데이터 중 현재일자를 기준으로 고객의 나이를 계산해서 다음과 같이 연령대별 매출금액을 보여주는 쿼리를 작성해 보자. 12345678------------------------- 연령대 매출금액-------------------------10대 xxxxxx20대 ....30대 .... 40대 ....------------------------- A3.12345678910111213141516WITH age_amt AS ( SELECT TRUNC((TO_CHAR(SYSDATE, 'yyyy') - b.cust_year_of_birth), -1) AS age_seg , SUM(a.amount_sold) AS amount FROM sales a , customers b WHERE a.sales_month = '200112' AND a.cust_id = b.cust_id GROUP BY TRUNC((TO_CHAR(SYSDATE, 'yyyy') - b.cust_year_of_birth), -1) )SELECT * FROM age_amt ORDER BY age_seg; 서브쿼리 : 현재일자 기준 고객 연령대별 매출액 구하기 (age_amt) 현재일자를 기준으로 고객의 나이를 계산한 다음(TO_CHAR(SYSDATE, 'yyyy') - b.cust_year_of_birth)각 연령대별 amount_sold의 합계를 계산하였음 1234567891011SELECT TRUNC((TO_CHAR(SYSDATE, 'yyyy') - b.cust_year_of_birth), -1) AS age_seg , SUM(a.amount_sold) AS amount FROM sales a , customers b WHERE a.sales_month = '200112' AND a.cust_id = b.cust_id GROUP BY TRUNC((TO_CHAR(SYSDATE, 'yyyy') - b.cust_year_of_birth), -1) Q4.월별로 판매금액이 가장 하위에 속하는 대륙 목록을 뽑아보자.(대륙목록은 countries 테이블의 country_region에 있으며, country_id 컬럼으로 customers 테이블과 조인을 해서 구한다.) 1234567--------------------------------- 매출월 지역(대륙) 매출금액 ---------------------------------199801 Oceania xxxxxx199803 Oceania xxxxxx...--------------------------------- A4.12345678910111213141516171819202122232425WITH basis AS ( SELECT a.sales_month , c.country_region , SUM(a.amount_sold) as amt FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id GROUP BY a.sales_month, c.country_region ) , month_amt AS ( SELECT sales_month AS &quot;매출월&quot; , country_region AS &quot;지역(대륙)&quot; , amt AS &quot;매출금액&quot; , RANK() OVER (PARTITION BY sales_month ORDER BY amt) AS ranks FROM basis )SELECT &quot;매출월&quot;, &quot;지역(대륙)&quot;, &quot;매출금액&quot; FROM month_amt WHERE ranks = 1; 서브쿼리 1 : 월별, 지역별 판매금액 합계 구하기 (basis) sales, customers, countries 조인 월별, 지역별 합계 : SUM(a.amount_sold) as amt 123456789SELECT a.sales_month , c.country_region , SUM(a.amount_sold) as amt FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id GROUP BY a.sales_month, c.country_region 서브쿼리 2 : 월별로 각 대륙의 판매금액 합계 순위 구하기 (month_amt) basis 서브쿼리에서 sales_month 파티션별 amt 순위값 계산 1234567SELECT sales_month AS &quot;매출월&quot; , country_region AS &quot;지역(대륙)&quot; , amt AS &quot;매출금액&quot; , RANK() OVER (PARTITION BY sales_month ORDER BY amt) AS ranks FROM basis Q5.5장 연습문제 5번의 정답 결과를 이용해 다음과 같이 지역별, 대출종류별, 월별 대출잔액과 지역별 파티션을 만들어대출종류별 대출잔액의 %를 구하는 쿼리를 작성해보자. 123456789------------------------------------------------------------------------------------------------지역 대출종류 201111 201112 201210 201211 201212 203110 201311------------------------------------------------------------------------------------------------서울 기타대출 73996.9( 36% )서울 주택담보대출 130105.9( 64% ) 부산......------------------------------------------------------------------------------------------------- A5.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354WITH basis AS ( SELECT region, gubun , CASE WHEN period = '201111' THEN loan_jan_amt ELSE 0 END amt1 , CASE WHEN period = '201112' THEN loan_jan_amt ELSE 0 END amt2 , CASE WHEN period = '201210' THEN loan_jan_amt ELSE 0 END amt3 , CASE WHEN period = '201211' THEN loan_jan_amt ELSE 0 END amt4 , CASE WHEN period = '201212' THEN loan_jan_amt ELSE 0 END amt5 , CASE WHEN period = '201310' THEN loan_jan_amt ELSE 0 END amt6 , CASE WHEN period = '201311' THEN loan_jan_amt ELSE 0 END amt7 FROM kor_loan_status ) , sum_amt AS ( SELECT region, gubun , SUM(amt1) AS amt1 , SUM(amt2) AS amt2 , SUM(amt3) AS amt3 , SUM(amt4) AS amt4 , SUM(amt5) AS amt5 , SUM(amt6) AS amt6 , SUM(amt7) AS amt7 FROM basis GROUP BY region, gubun )SELECT region AS &quot;지역&quot;, gubun AS &quot;대출종류&quot; , amt1 || '(' || ROUND(RATIO_TO_REPORT(amt1) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201111&quot; , amt2 || '(' || ROUND(RATIO_TO_REPORT(amt2) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201112&quot; , amt3 || '(' || ROUND(RATIO_TO_REPORT(amt3) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201210&quot; , amt4 || '(' || ROUND(RATIO_TO_REPORT(amt4) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201211&quot; , amt5 || '(' || ROUND(RATIO_TO_REPORT(amt5) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201212&quot; , amt6 || '(' || ROUND(RATIO_TO_REPORT(amt6) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201311&quot; , amt7 || '(' || ROUND(RATIO_TO_REPORT(amt7) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201311&quot; FROM sum_amt ORDER BY region; 서브쿼리 1 : 월별 대출잔액 변수 만들기 (basis) CASE WHEN ~ THEN ~ ELSE 구문으로 월별 대출잔액 변수 생성 12345678910111213141516SELECT region, gubun , CASE WHEN period = '201111' THEN loan_jan_amt ELSE 0 END amt1 , CASE WHEN period = '201112' THEN loan_jan_amt ELSE 0 END amt2 , CASE WHEN period = '201210' THEN loan_jan_amt ELSE 0 END amt3 , CASE WHEN period = '201211' THEN loan_jan_amt ELSE 0 END amt4 , CASE WHEN period = '201212' THEN loan_jan_amt ELSE 0 END amt5 , CASE WHEN period = '201310' THEN loan_jan_amt ELSE 0 END amt6 , CASE WHEN period = '201311' THEN loan_jan_amt ELSE 0 END amt7 FROM kor_loan_status 서브쿼리 2 : 지역, 구분으로 그룹화하여 월별 합계 산출 (sum_amt) 1234567SELECT region, gubun , SUM(amt1) AS amt1, SUM(amt2) AS amt2 , SUM(amt3) AS amt3, SUM(amt4) AS amt4 , SUM(amt5) AS amt5, SUM(amt6) AS amt6 , SUM(amt7) AS amt7 FROM basis GROUP BY region, gubun 메인 쿼리 : 지역 내 대출종류별 대출잔액의 비율 산출 1234567891011121314151617SELECT region AS &quot;지역&quot;, gubun AS &quot;대출종류&quot; , amt1 || '(' || ROUND(RATIO_TO_REPORT(amt1) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201111&quot; , amt2 || '(' || ROUND(RATIO_TO_REPORT(amt2) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201112&quot; , amt3 || '(' || ROUND(RATIO_TO_REPORT(amt3) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201210&quot; , amt4 || '(' || ROUND(RATIO_TO_REPORT(amt4) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201211&quot; , amt5 || '(' || ROUND(RATIO_TO_REPORT(amt5) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201212&quot; , amt6 || '(' || ROUND(RATIO_TO_REPORT(amt6) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201311&quot; , amt7 || '(' || ROUND(RATIO_TO_REPORT(amt7) OVER (PARTITION BY region), 3) *100 || '%)' AS &quot;201311&quot; FROM sum_amt ORDER BY region","link":"/2022/05/02/SQL/SQL%20EXERCISE%206-7/"},{"title":"SQL TEST 6-7","text":"SQL Subquery ‘오라클 SQL과 PL/SQL을 다루는 기술 (길벗)’ Q1.populations 테이블에서 2015년 평균 기대수명보다 높은 모든 정보를 조회한다. A1.12345678SELECT * FROM populations WHERE year = 2015 AND life_expectancy &gt; (SELECT AVG(life_expectancy) as AVG FROM populations WHERE year = 2015 GROUP BY year); Q2.subquery_countries 테이블에 있는 capital과 매칭되는 cities 테이블의 정보를 조회한다. A2.123456SELECT a.name, a.country_code, a.urbanarea_pop FROM cities a WHERE a.name IN (SELECT b.capital FROM subquery_countries b) ORDER BY a.urbanarea_pop desc; Q3.economies 테이블에서 code, inflation_rate, unemployment_rate를 조회한다. inflation_rate 오름차순으로 정렬한다. subquery_countries 테이블내 gov_form 컬럼에서 Constitutional Monarchy 또는 Republic이 들어간 국가는 제외한다. A3.123456789SELECT a.code, a.inflation_rate, a.unemployment_rate FROM economies a WHERE a.year = 2015 AND a.code NOT IN (SELECT b.code FROM subquery_countries b WHERE b.gov_form = 'Constitutional Monarchy' OR b.gov_form LIKE '%Republic%') ORDER BY a.inflation_rate ASC; Q4.2015년 각 대륙별 inflation_rate가 가장 심한 국가와 inflation_rate를 구한다. 힌트) 아래 쿼리 실행 123456SELECT country_name, continent, inflation_rate FROM subquery_countries INNER JOIN economies USING (code) WHERE year = 2015; A4.123456789101112131415With basis AS ( SELECT country_name, continent, inflation_rate FROM subquery_countries INNER JOIN economies USING (code) WHERE year = 2015 ) , max_inf AS ( SELECT continent, MAX(inflation_rate) as inflation_rate FROM basis GROUP BY continent )SELECT a.country_name, b.continent, b.inflation_rate FROM basis a, max_inf b WHERE a.inflation_rate = b.inflation_rate; SQL Window FunctionQ1.각 행에 숫자를 1, 2, 3, … 형태로 추가한다. (row_n으로 표시) row_n 기준으로 오름차순으로 출력 테이블명에 alias를 적용한다. A1.12345678With sub_table AS ( SELECT ROWNUM-95 AS ROW_N , YEAR, CITY, SPORT, DISCIPLINE, ATHLETE FROM summer_medals )SELECT * FROM sub_table WHERE ROW_N &gt; 0; Q2.올림픽 년도를 오름차순 순번대로 작성한다. 힌트) 서브쿼리와 윈도우 함수를 이용한다. A2.12345SELECT year , ROW_NUMBER() OVER (ORDER BY year) as ROW_N FROM (SELECT year FROM summer_medals GROUP BY year); Q3.(1) WITH 절 사용하여 각 운동선수들이 획득한 메달 갯수를 내림차순으로 정렬하도록 한다.(2) (1) 쿼리를 활용하여 그리고 선수들의 랭킹을 추가한다. 상위 5개만 추출 : OFFSET 0 ROWS FETCH NEXT 5 ROWS ONLY A3.123456789With basis AS ( SELECT ATHLETE, COUNT() AS MEDALS FROM summer_medals GROUP BY ATHLETE ORDER BY COUNT() desc )SELECT MEDALS, ATHLETE, ROWNUM FROM basis OFFSET 0 ROWS FETCH NEXT 5 ROWS ONLY; Q4.다음 쿼리는 남자 69KG 역도 경기에서 매년 금메달리스트 조회하는 쿼리이다. 이때 매년 전년도 챔피언도 같이 조회하도록 한다. (LAG &amp; WITH절 사용) 123456SELECT Year, Country AS champion FROM summer_medals WHERE Discipline = 'Weightlifting' AND Event = '69KG' AND Gender = 'Men' AND Medal = 'Gold'; A4.1234567891011WITH basis AS ( SELECT Year, Country AS champion FROM summer_medals WHERE Discipline = 'Weightlifting' AND Event = '69KG' AND Gender = 'Men' AND Medal = 'Gold' )SELECT year, champion , LAG(champion, 1) OVER(order by champion) AS LAST_CHAMPION FROM basis;","link":"/2022/05/02/SQL/SQL%20TEST%206-7/"},{"title":"Python Basic 1","text":"Hello World1print(&quot;Hello, World!&quot;) Hello, World! 주석 처리 코드 작업 시, 특정 코드에 대해 설명 사용자 정의 함수 작성 시, 클래스 작성 시 중요 (도움말 작성) 123456# 한 줄 주석 처리&quot;&quot;&quot;여러 줄 주석 처리&quot;&quot;&quot;print(&quot;Hello, World!&quot;) Hello, World! 변수 (Scalar) 객체(OBject)로 구현이 됨 하나의 자료형(Type)을 가진다. 클래스(Class)로 정의된다. 다양한 함수들 존재 int int 정수를 표현하는 데 사용 12345num_int = 1num_int2 = 3print(num_int)print(num_int2)print(type(num_int)) 1 3 &lt;class 'int'&gt; float 실수를 표현하는 데 사용 123num_float = 0.2print(num_float)print(type(num_float)) 0.2 &lt;class 'float'&gt; bool True와 False로 나타나는 Boolean 값을 표현하는 데 사용 123bool_true = Trueprint(bool_true)print(type(bool_true)) True &lt;class 'bool'&gt; None Null을 나타내는 자료형으로 None이라는 한 가지 값만 가진다. 123none_x = Noneprint(none_x)print(type(none_x)) None &lt;class 'NoneType'&gt; 사칙연산정수형 사칙연산123456789a = 15 # intb = 2 # intprint('a + b = ', a+b) # intprint('a - b = ', a-b) # intprint('a * b = ', a*b) # intprint('a / b = ', a/b) # floatprint('a // b = ', a//b) # intprint('a % b = ', a%b) # intprint('a ** b = ', a**b) # int a + b = 17 a - b = 13 a * b = 30 a / b = 7.5 a // b = 7 a % b = 1 a ** b = 225 실수형 사칙연산123456789a = 15.0 # floatb = 2.0 # floatprint('a + b =', a+b) # floatprint('a - b =', a-b) # floatprint('a * b =', a*b) # floatprint('a / b =', a/b) # floatprint('a // b =', a//b) # floatprint('a % b =', a%b) # floatprint('a ** b =', a**b) # float a + b = 17.0 a - b = 13.0 a * b = 30.0 a / b = 7.5 a // b = 7.0 a % b = 1.0 a ** b = 225.0 논리형 연산자 Bool형은 True와 False 값으로 정의 AND, OR, NOT 123456789101112x = 5 &gt; 4print('x =', x)y = 3 &gt; 9print('y =', y)print('x and x =', x and x)print('x and y =', x and y)print('y and x =', y and x)print('y and y =', y and y)print('x or x =', x or x)print('x or y =', x or y)print('y or x =', y or x)print('y or y =', y or y) x = True y = False x and x = True x and y = False y and x = False y and y = False x or x = True x or y = True y or x = True y or y = False 비교 연산자 부등호를 의미 비교 연산자를 True와 False 값을 도출 논리 &amp; 비교 연산자 응용123var = input(&quot;숫자를 입력하시오. :&quot;)print(var)print(type(var)) 숫자를 입력하시오. :24 24 &lt;class 'str'&gt; 123var = int(input(&quot;숫자를 입력하시오. :&quot;))print(var)print(type(var)) 숫자를 입력하시오. :92 92 &lt;class 'int'&gt; 123456789num1 = int(input(&quot;숫자를 입력하시오. :&quot;))num2 = int(input(&quot;숫자를 입력하시오. :&quot;))num3 = int(input(&quot;숫자를 입력하시오. :&quot;))num4 = int(input(&quot;숫자를 입력하시오. :&quot;))var1 = num1 &gt;= num2 var2 = num3 &lt; num4print(var1 and var2)print(var1 or var2) 숫자를 입력하시오. :29 숫자를 입력하시오. :15 숫자를 입력하시오. :8 숫자를 입력하시오. :10 True True 문자열문자열 입력 방법 문자열을 입력하는 4가지 방법 1234print(&quot;Hello, World&quot;)print('Hello, World')print(&quot;'Hello, World'&quot;)print('&quot;Hello, World&quot;') Hello, World Hello, World 'Hello, World &quot;Hello, World&quot; 문자열에 작은따옴표, 큰따옴표 포함하는 방법 123456789food = &quot;Python's favorite food is perl&quot;print(food)say = '&quot;Python is very easy.&quot; he says.'print(say)food2 = 'Python\\'s favorite food is perl'print(food2)say2 = &quot;\\&quot;Python is very easy.\\&quot; he says.&quot;print(say2) Python's favorite food is perl &quot;Python is very easy.&quot; he says. Python's favorite food is perl &quot;Python is very easy.&quot; he says. 변수에 여러 줄의 문자열 대입 12multiline = &quot;Life is too short.\\nYou need python.&quot;print(multiline) Life is too short. You need python. 12345multiline ='''Life is too short.You need python'''print(multiline) Life is too short. You need python String 연산자 덧셈 연산자 1234str1 = &quot;Hello &quot;str2 = &quot;World! &quot;print(str1 + str2) Hello World! 곱셈 연산자 12greeting = str1 + str2print(greeting * 3) Hello World! Hello World! Hello World! Indexing 문자열 인덱싱은 문자열 안에서 범위를 지정하여 특정 단일문자 추출 12345greeting = &quot;Hello Kaggle!&quot;print(greeting[0])print(greeting[6])print(greeting[len(greeting)-1])print(greeting[-1]) H K ! ! Slicing 문자열 슬라이싱은 문자열 안에서 범위를 지정하고 특정 문자열 추출 1234567print(greeting[:])print(greeting[:5])print(greeting[6:])print(greeting[3:9])print(greeting[0:9:2])print(greeting[6:-1])print(greeting[::-1]) Hello Kaggle! Hello Kaggle! lo Kag HloKg Kaggle !elggaK olleH Formattingformat 코드1234567print(&quot;I eat %d apples.&quot; % 3) # 숫자 대입print(&quot;I eat %s apples.&quot; % &quot;five&quot;) # 문자열 대입num = 10day = &quot;three&quot;say = &quot;I ate %d apples, so I was sick for %s days.&quot; % (num, day)print(say) I eat 3 apples. I eat five apples. I ate 10 apples, so I was sick for three days. 123print(&quot;I have %s apples&quot; % 3)print(&quot;rate is %s&quot; % 3.234)print(&quot;Error is %d%%.&quot; % 98) # fomatting 연산자와 %를 함께 쓸 때는 %% I have 3 apples rate is 3.234 Error is 98%. 123456print(&quot;%10s,Jane!&quot; % &quot;hi&quot;)print(&quot;%-10s,Jane!&quot; % &quot;hi&quot;)print(&quot;'%0.4f'&quot; % 3.42134234)print(&quot;'%10.4f'&quot; % 3.42134234)print(&quot;'%-10.4f'&quot; % 3.42134234) hi,Jane! hi ,Jane! '3.4213' ' 3.4213' '3.4213 ' format 함수12345678910print(&quot;I eat {0} apples.&quot;.format(7))print(&quot;I eat {0} apples.&quot;.format(&quot;five&quot;))num = 8day = 3print(&quot;I ate {0} apples.&quot;.format(num))print(&quot;I ate {0} apples, so I was sick for {1} days.&quot;.format(num, day))print(&quot;I ate {num} apples, so I was sick for {day} days.&quot;.format(num=6,day=2))print(&quot;I ate {0} apples, so I was sick for {day} days.&quot;.format(4,day=1)) I eat 7 apples. I eat five apples. I ate 8 apples. I ate 8 apples, so I was sick for 3 days. I ate 6 apples, so I was sick for 2 days. I ate 4 apples, so I was sick for 1 days. 123456print(&quot;'{0:&lt;10}'&quot;.format(&quot;hi&quot;))print(&quot;'{0:^10}'&quot;.format(&quot;hi&quot;))print(&quot;'{0:&gt;10}'&quot;.format(&quot;hi&quot;))print(&quot;'{0:=^10}'&quot;.format(&quot;hi&quot;))print(&quot;'{0:!&lt;10}'&quot;.format(&quot;hi&quot;)) 'hi ' ' hi ' ' hi' '====hi====' 'hi!!!!!!!!' 12345y = 3.42134234print(&quot;'{0:0.4f}'&quot;.format(y))print(&quot;'{0:10.4f}'&quot;.format(y))print(&quot;'{0:^10.4f}'&quot;.format(y))print(&quot;'{0:&lt;10.4f}'&quot;.format(y)) '3.4213' ' 3.4213' ' 3.4213 ' '3.4213 ' 123name1 = &quot;John&quot;name2 = &quot;Marry&quot;print(&quot;{0} {{and}} {1}&quot;.format(name1, name2)) John {and} Marry f 문자열1234567name = 'Sally'age = 29print(f&quot;My name is {name}, and I'm {age} years old.&quot;)print(f&quot;Next year, I'm going to be {age+1} years old.&quot;)d = {'name':'Sally', 'age':29}print(f&quot;My name is {d['name']}, and I'm {d['age']} years old.&quot;) # 딕셔너리 자료형 활용 My name is Sally, and I'm 29 years old. Next year, I'm going to be 30 years old. My name is Sally, and I'm 29 years old. 123456print(f'{&quot;hi&quot;:&lt;10}')print(f'{&quot;hi&quot;:^10}')print(f'{&quot;hi&quot;:&gt;10}')print(f'{&quot;hi&quot;:=^10}')print(f'{&quot;hi&quot;:!&lt;10}') hi hi hi ====hi==== hi!!!!!!!! 12345y = 3.42134234print(f'{y:0.4f}')print(f'{y:10.4f}')print(f'{y:^10.4f}')print(f'{y:&lt;10.4f}') 3.4213 3.4213 3.4213 3.4213 123name1 = &quot;John&quot;name2 = &quot;Marry&quot;print(f&quot;{name1} {{and}} {name2}&quot;) John {and} Marry 문자열 함수12345678910# counta = 'hobby'print(a.count('b'))# find, indexa = &quot;Python is the best choice&quot;print(a.find(&quot;b&quot;))print(a.find(&quot;k&quot;)) # 없으면 -1 반환print(a.index(&quot;t&quot;))# print(a.index(&quot;k&quot;)) # 없으면 에러 2 14 -1 2 12345678910111213# joinprint(&quot;,&quot;.join('abcdefg'))# upper, lowera = &quot;Hello&quot;print(a.upper())print(a.lower())# lstrip, rstrip, stripa = &quot; OK &quot;print(a.lstrip())print(a.rstrip())print(a.strip()) a,b,c,d,e,f,g HELLO hello OK OK OK 123456789# replacea = &quot;That's right!&quot;print(a.replace('right', 'wrong'))# splita = &quot;I Love You&quot;print(a.split()) # 공백 기준b = &quot;a:b:c:d&quot;print(b.split(':')) # 특정 구분자 기준 That's wrong! ['I', 'Love', 'You'] ['a', 'b', 'c', 'd']","link":"/2022/03/22/Python/Basic/python_basic_1/"},{"title":"Python Basic 2","text":"리스트 시퀀스 데이터 타입 데이터에 순서가 존재하며, 인덱싱 및 슬라이싱 가능 대괄호(‘[값1, 값2, 값3]’)를 사용하여 표현 12345678910111213a = [] # 빈 리스트a_func = list() # 함수를 통해 생성b = [1]c = ['apple']d = [1,2,['apple']] # 리스트 안에 리스트print(a)print(a_func)print(b)print(c)print(d)print(type(d)) [] [] [1] ['apple'] [1, 2, ['apple']] &lt;class 'list'&gt; 리스트 Indexing, Slicing12345678910a = [1,2,3,4,5,6,7,8,9,10]print(a)print(a[0])print(a[5])print(a[:5])print(a[8:])print(a[3:9:2])print(a[:-3:3])print(a[::-1]) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 1 6 [1, 2, 3, 4, 5] [9, 10] [4, 6, 8] [1, 4, 7] [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] 1234a = [[&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;], 10]print(a[0])print(a[0][1])print(a[0][2][2]) ['apple', 'banana', 'cherry'] banana e 리스트 연산자 사용12345a = [&quot;john&quot;, &quot;evan&quot;]b = [&quot;alice&quot;, &quot;eva&quot;]c = a + b # 리스트가 하나로 합쳐짐print(c) ['john', 'evan', 'alice', 'eva'] 1234c = a * 3d = b * 0print(&quot;a * 3 =&quot;, c) # 숫자만큼 반복print(&quot;b * 0 =&quot;, d) # 빈 리스트 출력 a * 3 = ['john', 'evan', 'john', 'evan', 'john', 'evan'] b * 0 = [] 리스트 수정 및 삭제123a = [0, 1, 2]a[1] = 'b'print(a) [0, 'b', 2] 리스트 값 추가123456a = [100,200,300]a.append(400)print(a)a.append([500,600]) # 리스트 자체를 요소로 추가print(a) [100, 200, 300, 400] [100, 200, 300, 400, [500, 600]] 123456a = [100,200,300]a.append(400)print(a)a.extend([500,600]) # 리스트의 값들을 요소로 추가print(a) [100, 200, 300, 400] [100, 200, 300, 400, 500, 600] 123a = [0,1,2]a.insert(1, 100) # 원하는 위치에 원하는 값 추가print(a) [0, 100, 1, 2] 리스트 값 삭제1234a = [4,3,2,1,&quot;A&quot;]a.remove(1) # 해당되는 값 제거 a.remove(&quot;A&quot;)print(a) [4, 3, 2] 123456a = [1,2,3,4,5,6,7,8,9,10]del a[1] # 인덱스 번호를 이용하여 제거print(a)del a[1:5]print(a) [1, 3, 4, 5, 6, 7, 8, 9, 10] [1, 7, 8, 9, 10] 1234567b = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;]x = b.pop(2)print(x)print(b)y = b.pop() # 인덱스를 지정하지 않으면 마지막 요소 추출 및 제거print(y)print(b) c ['a', 'b', 'd', 'e'] e ['a', 'b', 'd'] 그 외 메서드12345a = [0,1,2,3]print(a)a.clear()print(a) [0, 1, 2, 3] [] 12a = [&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]print(a.index(&quot;b&quot;)) # 해당 요소가 처음으로 등장하는 위치 2 12345678a = [1,4,5,2,3]b = [1,4,5,2,3]a.sort() # 오름차순print(a)b.sort(reverse=True) # 내림차순print(b) [1, 2, 3, 4, 5] [5, 4, 3, 2, 1] 12345678c = ['d','bye','five','a']d = ['d','bye','five','a']c.sort()print(c)d.sort(reverse=True)print(d) ['a', 'bye', 'd', 'five'] ['five', 'd', 'bye', 'a'] 튜플 리스트와 비슷한 형태로 Indexing, Slicing 가능 리스트와 달리 수정 및 삭제가 안 됨 소괄호(‘(값1, 값2, 값3)’)를 사용하여 표현 123456789tuple1 = (0) # 끝에 comma(,)를 붙이지 않으면 int 자료형tuple2 = (0,) # 끝에 comma(,)를 붙여야 tuple 자료형tuple3 = 0, 1, 2print(tuple1)print(type(tuple1))print(tuple2)print(type(tuple2))print(tuple3)print(type(tuple3)) 0 &lt;class 'int'&gt; (0,) &lt;class 'tuple'&gt; (0, 1, 2) &lt;class 'tuple'&gt; 123456789a = (0,1,2,3,'a')print(type(a))# del a[4] : 튜플에서는 수정, 삭제 안 됨b = list(a)print(b)b[1] = 'b'a = tuple(b)print(a) &lt;class 'tuple'&gt; [0, 1, 2, 3, 'a'] (0, 'b', 2, 3, 'a') 튜플 Indexing, Slicing1234567a = (0,1,2,3,'a')print(type(a))print(a[1])print(a[-2])print(a[1:3])print(a[::2]) &lt;class 'tuple'&gt; 1 3 (1, 2) (0, 2, 'a') 튜플 연산자 사용123456t1 = (0,1,2)t2 = (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)print(t1 + t2)print(t1 * 3)print(t1 * 0) (0, 1, 2, 'a', 'b', 'c') (0, 1, 2, 0, 1, 2, 0, 1, 2) () 딕셔너리 Key와 Value로 구분됨 중괄호({‘키1’:’값1’, ‘키2’:’값2’})를 사용하여 표현 12345678dict_01 = {'teacher' : 'evan', 'class' : '601호', 'open' : '2022-03-10', 'students' : 24, 'names' : ['A', 'B', 'R', 'Z']}print(dict_01['teacher'])print(dict_01['open'])print(dict_01['names']) evan 2022-03-10 ['A', 'B', 'R', 'Z'] 123print(dict_01.keys())print(type(dict_01.keys()))print(list(dict_01.keys())) # 다양한 연산과 메서드를 적용할 수 있는 리스트형으로 변환 dict_keys(['teacher', 'class', 'open', 'students', 'names']) &lt;class 'dict_keys'&gt; ['teacher', 'class', 'open', 'students', 'names'] 123print(dict_01.values())print(type(dict_01.values()))print(list(dict_01.values())) # 다양한 연산과 메서드를 적용할 수 있는 리스트형으로 변환 dict_values(['evan', '601호', '2022-03-10', 24, ['A', 'B', 'R', 'Z']]) &lt;class 'dict_values'&gt; ['evan', '601호', '2022-03-10', 24, ['A', 'B', 'R', 'Z']] 1dict_01.items() # 각 key와 value가 튜플 형태로 출력됨 dict_items([('teacher', 'evan'), ('class', '601호'), ('open', '2022-03-10'), ('students', 24), ('names', ['A', 'B', 'R', 'Z'])]) 1234567print(dict_01.get(&quot;teacher&quot;))# print(dict_01['선생님'])print(dict_01.get(&quot;선생님&quot;)) # key가 없으면 None을 반환print(dict_01.get(&quot;선생님&quot;, &quot;없음&quot;)) # key가 없을 때 대체값 지정 가능print(dict_01.get(&quot;class&quot;))# 그냥 값을 출력해도 되지만, get 메서드를 사용하면 key가 없더라도 에러 없이 출력 가능 evan None 없음 601호 조건문 &amp; 반복문조건문12345weather = '맑음'if weather == &quot;비&quot;: print(&quot;우산을 가져간다.&quot;)else: print(&quot;우산을 가져가지 않는다.&quot;) 우산을 가져가지 않는다. 1234567# 60점 이상 합격score = int(input(&quot;점수를 입력하시오. : &quot;))if score &gt;= 60: print(&quot;합격입니다.&quot;)else: print(&quot;불합격입니다.&quot;) 점수를 입력하시오. : 50 불합격입니다. 12345678910111213141516# 90점 이상은 A, 80점 이상은 B, 70점 이상은 C, 나머지는 Fscore = int(input(&quot;점수를 입력하시오. : &quot;))grade = &quot;&quot;if score &gt;= 90: grade = &quot;A&quot;elif score &gt;= 80: grade = &quot;B&quot;elif score &gt;= 70: grade = &quot;C&quot;elif score &gt;= 60: grade = &quot;D&quot;else: grade = &quot;F&quot; print(grade) 점수를 입력하시오. : 68 D 반복문12for i in range(4): print(i+1, &quot;안녕하세요!&quot;) 1 안녕하세요! 2 안녕하세요! 3 안녕하세요! 4 안녕하세요! 123456789count = range(5)print(count)for n in count: print(str(n+1) + &quot;번째&quot;) if (n+1) == 3: print(&quot;stop!&quot;) break print(&quot;shoot!&quot;) range(0, 5) 1번째 shoot! 2번째 shoot! 3번째 stop! 1234567a = &quot;hello&quot;for x in a: if x=='l': break print(x) h e 반복문 작성 방식 : zip, range, enumerate, len, etc 12345alphabets = ['A', 'B', 'C']# enumerate는 인덱스와 값을 튜플 형태로 묶어주는 객체for i, value in enumerate(alphabets): print(i, value) 0 A 1 B 2 C","link":"/2022/03/22/Python/Basic/python_basic_2/"},{"title":"Python Basic 3","text":"기초 문법 리뷰리스트, 튜플, 딕셔너리1234567891011121314# 리스트book_list = ['A', 'B', 'C']print(book_list)# append, extend, insert, remove, pop, etc# 튜플book_tuple = ('A', 'B', 'C')print(book_tuple)# 수정, 삭제 불가능# 딕셔너리book_dictionary = {&quot;title&quot; : [&quot;A&quot;, &quot;B&quot;], &quot;year&quot; : [2011, 2002]}print(book_dictionary)# keys(), values(), items(), get() ['A', 'B', 'C'] ('A', 'B', 'C') {'title': ['A', 'B'], 'year': [2011, 2002]} 조건문 &amp; 반복문123456if True: print(&quot;코드 실행&quot;) # 들여쓰기 주의elif True: print(&quot;코드 실행&quot;)else: print(&quot;코드 실행&quot;) 12for i in range(3): print(i+1, &quot;안녕하세요&quot;) 1 안녕하세요 2 안녕하세요 3 안녕하세요 123456789101112131415161718book_list = [&quot;R&quot;, &quot;Python&quot;]for book in book_list: print(book, end=&quot; &quot;)print(&quot;\\n&quot;)strings01 = &quot;Hello&quot;for char in strings01: print(char, end=&quot; &quot;)num_tuple = (1, 2, 3, 4)for num in num_tuple: print(num, end=&quot; &quot;)print(&quot;\\n&quot;)num_dict = {&quot;A&quot;:1, &quot;B&quot;:2}for num in num_dict: print(num, end=&quot; &quot;) # key 값 print(num_dict[num], end=&quot; &quot;) # value 값 R Python H e l l o 1 2 3 4 A 1 B 2 반복문의 필요성123456789name_list = [&quot;요구르트&quot;, &quot;우유&quot;, &quot;콜라&quot;, &quot;사이다&quot;, &quot;과자&quot;]price_list = [1000, 1500, 1200, 1200, 1000]quantity_list = [5, 3, 1, 2, 4]for i in range(len(name_list)): name = name_list[i] sales = price_list[i] * quantity_list[i] print(name + &quot;의 매출액 : &quot; + str(sales) + &quot;원&quot;) 요구르트의 매출액 : 5000원 우유의 매출액 : 4500원 콜라의 매출액 : 1200원 사이다의 매출액 : 2400원 과자의 매출액 : 4000원 while 조건식이 들어간 반복문 1234count = 5while count &gt; 0: print(count, &quot;안녕하세요.&quot;) count = count - 1 5 안녕하세요. 4 안녕하세요. 3 안녕하세요. 2 안녕하세요. 1 안녕하세요. 리스트 컴프리핸션 for-loop 반복문을 한 줄로 처리 123456789letters = []for char in &quot;helloworld&quot;: letters.append(char)print(&quot;for-loop 반복문 사용 :&quot;)print(&quot;\\t&quot;, letters)letters2 = [char for char in &quot;helloworld&quot;]print(&quot;리스트 컴프리핸션 사용 :&quot;)print(&quot;\\t&quot;, letters2) for-loop 반복문 사용 : ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd'] 리스트 컴프리핸션 사용 : ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd'] 1234567891011121314# 중첩 리스트를 단일 리스트로my_list = [[10],[20,30]]print(my_list)# for-loop 중첩 반복문 사용flattened_list1 = []for value_list in my_list: for value in value_list: flattened_list1.append(value)print(&quot;중첩 반복문 사용 :&quot;, flattened_list1)# 리스트 컴프리핸션 사용flattened_list2 = [value for value_list in my_list for value in value_list]print(&quot;리스트 컴프리핸션 사용 :&quot;, flattened_list2) [[10], [20, 30]] 중첩 반복문 사용 : [10, 20, 30] 리스트 컴프리핸션 사용 : [10, 20, 30] 사용자 정의 함수1234567891011121314151617181920def plus(a,b): c = a + b return cdef minus(a,b): c = a - b return cdef multiply(a,b): c = a * b return cdef divide(a,b): c = a / b return cprint(plus(1,5))print(minus(10,3))print(multiply(2,4))print(divide(8,2)) 6 7 8 4.0 basic.py로 저장할 때 예시 1!which python /usr/local/bin/python 12345678910111213# /usr/local/bin/python# -*- coding: utf-8 -*-def add(a, b): c = a + b return cif __name__ == &quot;__main__&quot;: a = 1 b = 2 c= add(a, b) print(c) 3 파이썬 함수 주석 처리 Docstring(문서화) 1234567891011121314151617181920212223# /usr/local/bin/python# -*- coding: utf-8 -*-def temp(content, letter): &quot;&quot;&quot; content 안에 있는 문자를 세는 함수입니다. Args: content(str) : 탐색 문자열 letter(str) : 찾을 문자열 Returns: int &quot;&quot;&quot; print(&quot;함수 테스트&quot;) cnt = len([char for char in content if char == letter]) return cntif __name__ == &quot;__main__&quot;: # help(temp) print(temp.__doc__) content 안에 있는 문자를 세는 함수입니다. Args: content(str) : 탐색 문자열 letter(str) : 찾을 문자열 Returns: int 12345678910111213141516171819202122232425262728def mean_and_median(value_list): &quot;&quot;&quot; 숫자 리스트 요소들의 평균과 중간값을 구하는 함수 Args: value_list (iterable of int / float) : A list of int numbers Returns: tuple(float, float) &quot;&quot;&quot; # 평균 mean = sum(value_list) / len(value_list) # 중간값 midpoint = int(len(value_list) / 2) if len(value_list) % 2 == 0: median = (value_list[midpoint - 1] + value_list[midpoint]) / 2 else: median = value_list[midpoint] return mean, medianif __name__ == &quot;__main__&quot;: value_list = [1, 1, 2, 2, 3, 4, 5] avg, median = mean_and_median(value_list) print(&quot;avg:&quot;, avg) print(&quot;median:&quot;, median) avg: 2.5714285714285716 median: 2 12345678910111213141516171819202122232425262728293031def calculation(num1,num2): &quot;&quot;&quot; 두 수에 대한 사칙연산을 수행하는 함수 Args: num1 : float number num2 : float number Returns: tuple(float, float, float, float) &quot;&quot;&quot; # 덧셈 plus_num = num1 + num2 # 뺄셈 minus_num = num1 - num2 # 곱셈 multiply_num = num1 * num2 # 나눗셈(소수점 둘째 자리까지) divide_num = round(num1 / num2, 2) return plus_num, minus_num, multiply_num, divide_numif __name__ == &quot;__main__&quot;: num1 = 13 num2 = 7 plus, minus, multiply, divide = calculation(num1, num2) print(&quot;+ :&quot;, plus) print(&quot;- :&quot;, minus) print(&quot;* :&quot;, multiply) print(&quot;/ :&quot;, divide) + : 20 - : 6 * : 91 / : 1.86 이터레이터, 제너레이터, 데코레이터 변수명 immutable or mutable, context manager","link":"/2022/03/22/Python/Basic/python_basic_3/"},{"title":"Python Basic 4","text":"클래스(Class) 목적 : 코드의 간결화, 코드의 재사용, 유지보수 용이 여러 클래스가 모여서 하나의 라이브러리가 됨 장고 / 웹개발 / 머신러닝 / 시각화 / 전처리 클래스명은 대문자로 시작해야 함 1234567891011121314151617class Person: # class attribute (선택) country = &quot;korean&quot; # instance attribute (필수) def __init__(self, name, age): self.name = name self.age = ageif __name__ == &quot;__main__&quot;: kim = Person(&quot;Kim&quot;, 30) lee = Person(&quot;Lee&quot;, 28) # access class attribute print(&quot;Kim은 {}&quot;.format(kim.__class__.country)) print(&quot;Lee는 {}&quot;.format(lee.__class__.country)) Kim은 korean Lee는 korean 인스턴스 메서드 생성 list.append(), list.extend() 123456789101112131415161718192021class Person: # class attribute (선택) country = &quot;korean&quot; # instance attribute (필수) def __init__(self, name, age): self.name = name self.age = age # instance method 정의 def singing(self, songtitle): return &quot;{}: '{}' 노래를 부릅니다.&quot;.format(self.name, songtitle)if __name__ == &quot;__main__&quot;: kim = Person(&quot;Kim&quot;, 30) lee = Person(&quot;Lee&quot;, 28) # call instance method print(kim.singing(&quot;creep&quot;)) print(lee.singing(&quot;peaches&quot;)) Kim: 'creep' 노래를 부릅니다. Lee: 'peaches' 노래를 부릅니다. 클래스 상속12345678910111213141516171819202122232425262728293031323334353637383940class Parent: # init constructor def __init__(self, name, age): self.name = name self.age = age # instance method def whoAmI(self): print(&quot;I am Parent!&quot;) def singing(self, songtitle): return &quot;{}: '{}' 노래를 부릅니다.&quot;.format(self.name, songtitle) def dancing(self): return &quot;{}: 춤을 춥니다.&quot;.format(self.name)class Child(Parent): # instance attribute def __init__(self, name, age): super().__init__(name, age) # 부모 클래스의 생성자 그대로 가져오기 print(&quot;Child Class On.&quot;) # instance method def whoAmI(self): print(&quot;I am Child!&quot;) def studying(self, subject): return &quot;{} : {} 공부를 합니다.&quot;.format(self.name, subject)if __name__ == &quot;__main__&quot;: child_kim = Child(&quot;kim&quot;, 13) parent_kim = Parent(&quot;kim&quot;, 49) child_kim.whoAmI() parent_kim.whoAmI() print(parent_kim.dancing()) # print(parent_kim.studying()) -&gt; AttributeError 발생 print(child_kim.singing(&quot;fake love&quot;)) print(child_kim.studying(&quot;math&quot;)) Child Class On. I am Child! I am Parent! kim: 춤을 춥니다. kim: 'fake love' 노래를 부릅니다. kim : math 공부를 합니다. 123456789101112131415161718192021222324252627class TV: def __init__(self): # private variable (외부 접근 불가능) self.__maxprice = 500 def sell(self): print(&quot;Selling Price: {}&quot;.format(self.__maxprice)) # set method, get method def setMaxPrice(self, price): self.__maxprice = price print(&quot;Price Updated&quot;) def getMaxPrice(self): return self.__maxprice if __name__==&quot;__main__&quot;: tv = TV() tv.sell() # 강제로 값을 변경할 수 없음 tv.__maxprice = 100 tv.sell() # 별도의 method를 통해 변경 가능 tv.setMaxPrice(400) tv.sell() Selling Price: 500 Selling Price: 500 Price Updated Selling Price: 400 클래스 내부 조건문 init constructor 1234567891011121314151617181920212223242526272829303132class Employee: # init constructor def __init__(self, name, salary = 0): self.name = name # public variable (외부 접근 가능) if salary &gt; 0: self.salary = salary else: self.salary = 0 print(&quot;급여는 0원이 될 수 없습니다. 다시 입력하세요.&quot;) def update_salary(self, amount): self.salary += amount def weekly_salary(self): return int(self.salary / 7)if __name__==&quot;__main__&quot;: emp1 = Employee(&quot;David&quot;, -50000) print(&quot;{}의 급여는 {}원입니다.&quot;.format(emp1.name, emp1.salary)) emp1.salary = emp1.salary + 1500 print(&quot;{}의 급여는 {}원입니다.&quot;.format(emp1.name, emp1.salary)) emp1.update_salary(3000) print(&quot;{}의 급여는 {}원입니다.&quot;.format(emp1.name, emp1.salary)) week_salary = emp1.weekly_salary() print(&quot;{}의 주 급여는 {}원입니다.&quot;.format(emp1.name, week_salary)) 급여는 0원이 될 수 없습니다. 다시 입력하세요. David의 급여는 0원입니다. David의 급여는 1500원입니다. David의 급여는 4500원입니다. David의 주 급여는 642원입니다. 클래스 Docstring12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Person: &quot;&quot;&quot; 사람을 표현하는 클래스 *** Attributes ---------- name: str Name of the person age: int Age of the person Methods ------- info(additional=&quot;&quot;): Prints the person's name and age &quot;&quot;&quot; def __init__(self, name, age): &quot;&quot;&quot; Constructs all the neccessary attributes for the person object Parameters ---------- name: str Name of the person age: int Age of the person &quot;&quot;&quot; self.name = name self.age = age def info(self, additional=None): &quot;&quot;&quot; Prints the person's information Parameters ---------- additional: str, optional more info to be diplayed (Default is None) / A, B, C Returns ------- None &quot;&quot;&quot; print(f'My name is {self.name}. I am {self.age} years old. ' + additional)if __name__==&quot;__main__&quot;: print(Person.__doc__) person = Person(&quot;Jiwon&quot;, age = 27) person.info(&quot;I wanna be a data analyst.&quot;) 사람을 표현하는 클래스 *** Attributes ---------- name: str Name of the person age: int Age of the person Methods ------- info(additional=&quot;&quot;): Prints the person's name and age My name is Jiwon. I am 27 years old. I wanna be a data analyst.","link":"/2022/03/22/Python/Basic/python_basic_4/"},{"title":"Crawling Headline News","text":"Check the Website Info Access Developer Tools of the website and enter the Nework tab. Type ctrl + R and enter the Doc tap. Enter a site and check the Headers tap with the site. Copy the value of referer and user-agent. Crawling Code step01_headlinenews.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445import warningsimport requestsfrom bs4 import BeautifulSoupwarnings.filterwarnings('ignore')import pandas as pddef crawling(soup): div = soup.find(&quot;div&quot;, class_=&quot;list_issue&quot;) print(type(div)) titles = [] urls = [] for a in div.find_all(&quot;a&quot;): titles.append(a.get_text()) urls.append(a['href']) results = (titles, urls) return(results)def df_csv(tp): df = pd.DataFrame({&quot;newstitle&quot; : tp[0], &quot;url&quot; : tp[1]}) print(df) df.to_csv(&quot;headlinecrawling.csv&quot;, index=False) print(&quot;Crawling is done!&quot;)def main(): CUSTOM_HEADER = { 'referer' : 'https://www.naver.com/', 'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36' } url = 'https://www.naver.com/' req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) # 200 : Good # 404 : URL Error # 503 : Server Down soup = BeautifulSoup(req.text, 'html.parser', from_encoding='utf-8') df_csv(crawling(soup))if __name__ == &quot;__main__&quot;: main()","link":"/2022/04/22/Python/Crawling/Crawling%20Headline%20News/"},{"title":"ML Practice 1_3","text":"Market and Machine LearningClassify Bream and SmeltBream Data123456bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] 123456import matplotlib.pyplot as pltplt.scatter(bream_length, bream_weight)plt.xlabel('length')plt.ylabel('weight')plt.show() Smelt Data12345678smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]plt.scatter(bream_length, bream_weight)plt.scatter(smelt_length, smelt_weight)plt.xlabel('length')plt.ylabel('weight')plt.show() 1st ML Program12345length = bream_length + smelt_lengthweight = bream_weight + smelt_weightfish_data = [[l,w] for l, w in zip(length, weight)]print(fish_data) [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]] 12fish_target = [1]*35 + [0]*14print(fish_target) [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] K-Nearest Neighbor12345from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(fish_data, fish_target)kn.score(fish_data, fish_target) 1.0 1kn.predict([[30,600]]) array([1]) 12print(kn._fit_X)print(kn._y) [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 6.7] [ 10.5 7.5] [ 10.6 7. ] [ 11. 9.7] [ 11.2 9.8] [ 11.3 8.7] [ 11.8 10. ] [ 11.8 9.9] [ 12. 9.8] [ 12.2 12.2] [ 12.4 13.4] [ 13. 12.2] [ 14.3 19.7] [ 15. 19.9]] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 123kn49 = KNeighborsClassifier(n_neighbors=49)kn49.fit(fish_data, fish_target)kn49.score(fish_data, fish_target) 0.7142857142857143 123456for n in range(5, 50): kn.n_neighbors = n score = kn.score(fish_data, fish_target) if score &lt; 1: print(n, score) break 18 0.9795918367346939 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/26/Python/ML/ML_ch_1_3/"},{"title":"Crawling Music Chart Top100","text":"Website Info Request URL : https://music.bugs.co.kr/chart Request Method : GET User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36 Crawling Code step03_bugsTop100.py 12345678910111213141516171819202122232425262728293031323334353637383940import requestsimport warningsfrom bs4 import BeautifulSoupwarnings.filterwarnings('ignore')import pandas as pddef crawling(soup): chart = soup.find(&quot;table&quot;, class_=&quot;list trackList byChart&quot;) titles = [] artists = [] for p in chart.find_all(&quot;p&quot;, class_=&quot;title&quot;): titles.append(p.get_text()[1:-1]) for p in chart.find_all(&quot;p&quot;, class_=&quot;artist&quot;): artists.append(p.get_text()[1:-1]) return(titles, artists)def df_csv(tp): df = pd.DataFrame({&quot;title&quot; : tp[0], &quot;artist&quot; : tp[1]}) print(df) df.to_csv(&quot;top100.csv&quot;, index=False) print(&quot;Crawling is done!&quot;)def main(): CUSTOM_HEADER = { 'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36' } url = 'https://music.bugs.co.kr/chart' req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) soup = BeautifulSoup(req.text, 'html.parser') print(type(soup)) df_csv(crawling(soup))if __name__ == &quot;__main__&quot;: main() top100.csv","link":"/2022/04/22/Python/Crawling/Crawling%20Music%20Chart%20Top100/"},{"title":"Crawling Data from Web","text":"Step 1. Set virtual environment Create a new directory under the C drive and virtual environment. 123$ mkdir crawling &amp;&amp; cd crawling$ virtualenv venv$ sourve venv/Scipts/activate Install some required packages. 123$ pip install beautifulsoup4$ pip install numpy pandas matplotlib seaborn$ pip install requests Step 2. Crawling Practice 1 Create a HTML file index.html 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;titl&gt;test&lt;/titl&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;aaaaaaaa&lt;/h1&gt; &lt;h2&gt;dddd&lt;/h2&gt; &lt;div class=&quot;chapter01&quot;&gt; &lt;p&gt;Don't Crawl here &lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;chapter02&quot;&gt; &lt;p&gt;Just Crawling here&lt;/p&gt; &lt;/div&gt; &lt;div id=&quot;main&quot;&gt; &lt;p&gt; Crawling .................. &lt;/p&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; Create a python file main.py crawling text from index.html 12345678910111213141516from bs4 import BeautifulSoupdef main(): # Convert index.html to BeautifulSoup Object soup = BeautifulSoup(open(&quot;index.html&quot;, encoding='UTF-8'), &quot;html.parser&quot;) print(type(soup)) print(soup.find(&quot;p&quot;)) print(&quot;----------------&quot;) print(soup.find_all(&quot;p&quot;)) print(&quot;----------------&quot;) print(soup.find(&quot;div&quot;, class_ = &quot;chapter02&quot;)) print(&quot;----------------&quot;) print(soup.find(&quot;div&quot;, id = &quot;main&quot;).find(&quot;p&quot;).get_text())if __name__ == &quot;__main__&quot;: main() Run the main.py and check the result printed. 1234567891011$ python main.py&lt;class 'bs4.BeautifulSoup'&gt;&lt;p&gt;Don't crawl here!&lt;/p&gt;----------------[&lt;p&gt;Don't crawl here!&lt;/p&gt;, &lt;p&gt;Just Crawl here!&lt;/p&gt;, &lt;p&gt; Crawling .................. &lt;/p&gt;]----------------&lt;div class=&quot;chapter02&quot;&gt;&lt;p&gt;Just Crawl here!&lt;/p&gt;&lt;/div&gt;---------------- Step 3. Quick Start BeautifulSoup4 URL : https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start index2.html 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; temp1.py 1234from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index2.html&quot;), 'html.parser')print(soup.prettify()) 123456789101112131415161718192021222324252627282930313233343536$ python temp1.py&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt; &lt;b&gt; The Dormouse's story &lt;/b&gt; &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; Elsie &lt;/a&gt; , &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt; Lacie &lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt; Tillie &lt;/a&gt; ; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; ... &lt;/p&gt; &lt;/body&gt;&lt;/html&gt; temp2.py 1234567891011121314151617181920from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index2.html&quot;), 'html.parser')print(soup.title)print(&quot;----------------&quot;)print(soup.title.name)print(&quot;----------------&quot;)print(soup.title.string)print(&quot;----------------&quot;)print(soup.title.parent.name)print(&quot;----------------&quot;)print(soup.p)print(&quot;----------------&quot;)print(soup.p['class'])print(&quot;----------------&quot;)print(soup.a)print(&quot;----------------&quot;)print(soup.find_all('a'))print(&quot;----------------&quot;)print(soup.find(id=&quot;link3&quot;)) 12345678910111213141516171819$ python temp2.py&lt;title&gt;The Dormouse's story&lt;/title&gt;----------------title----------------The Dormouse's story----------------head----------------&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;----------------['title']----------------&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;----------------[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]----------------&lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; temp3.py 1234567from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index2.html&quot;), 'html.parser')for link in soup.find_all('a'): print(link.get('href'))print(soup.get_text()) 1234567891011121314151617$ python temp3.pyhttp://example.com/elsiehttp://example.com/laciehttp://example.com/tillieThe Dormouse's storyThe Dormouse's story Once upon a time there were three little sisters; and their names were Elsie, Lacieand Tillie; and they lived at the bottom of a well....","link":"/2022/04/22/Python/Crawling/Crawling%20Data%20from%20Web/"},{"title":"ML Practice 3_2","text":"Data Set12345678910111213141516171819import numpy as npperch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] ) 12345from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((42,), (14,), (42,), (14,)) 1234train_input = train_input.reshape(-1,1)test_input = test_input.reshape(-1,1)print(train_input.shape, test_input.shape) (42, 1) (14, 1) KNN Regression12345from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor(n_neighbors=3)knr.fit(train_input, train_target)knr.score(test_input, test_target) 0.9746459963987609 Predict a data 1 the weight of a 50-centimeter-long perch 1print(knr.predict([[50]])) [1033.33333333] 123456789import matplotlib.pyplot as pltdistances, indexes = knr.kneighbors([[50]])fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.scatter(train_input[indexes], train_target[indexes], marker=&quot;D&quot;) # 3 neighborsax.scatter(50, knr.predict([[50]]), marker='^') # new dataplt.show() Predict a data 2 the weight of a 100-centimeter-long perch 1print(knr.predict([[100]])) [1033.33333333] 1234567distances, indexes = knr.kneighbors([[100]])fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.scatter(train_input[indexes], train_target[indexes], marker=&quot;D&quot;) # 3 neighborsax.scatter(100, knr.predict([[100]]), marker='^') # new dataplt.show() Beyond the scope of the new training set, incorrect values can be predicted. No matter how big the length is, the weight doesn’t increase anymore. ※ Machine learning models must be trained periodically. MLOps (Machine Learning &amp; Opearations) the essential skill for data scientist, ML engineer. Linear Regression in statistics: The process of finding causal relationships is more important. 4 assumptions (linearity, normality, independence, equal variance) in ML: Predicting results is more important. R-squared, MAE, RMSE, etc Predict a data123456from sklearn.linear_model import LinearRegressionlr = LinearRegression()lr.fit(train_input, train_target)print(lr.predict([[50]])) [1241.83860323] 12345fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.scatter(train_input[indexes], train_target[indexes], marker=&quot;D&quot;) # 3 neighborsax.scatter(50, lr.predict([[50]]), marker='^') # new dataplt.show() Regression equation coef_ : regression coefficient(weight) intercept_ : regression intercept $y = a + bx$ coefficient &amp; intercept : model parameter Linear Regression is a model-based learning. KNN Regression is a case-based learning. 1print(lr.coef_, lr.intercept_) [39.01714496] -709.0186449535477 123456789101112fig, ax = plt.subplots()# scatter plot of training setax.scatter(train_input, train_target)# linear equation from 0 to 50ax.plot([0,50], [0*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])ax.scatter(50, lr.predict([[50]]), marker=&quot;^&quot;)ax.set_label(&quot;length&quot;)ax.set_label(&quot;weight&quot;)plt.show() 12print(lr.score(train_input, train_target))print(lr.score(test_input, test_target)) # Underfitting 0.939846333997604 0.8247503123313558 The model is so simple that it is underfit overall. It seems that polynomial regression is needed. Polynomial Regression coef_ : regression coefficients(weights) intercept_ : regression intercept $y = a + b_1x_1 + b_2x_2 + … + b_nx_n$ Predict a data1234# Broadcasting in Numpytrain_poly = np.column_stack((train_input ** 2, train_input))test_poly = np.column_stack((test_input ** 2, test_input))print(train_poly.shape, test_poly.shape) (42, 2) (14, 2) ※ Broadcasting in Numpy tutorial : https://numpy.org/doc/stable/user/basics.broadcasting.html 123lr2 = LinearRegression()lr2.fit(train_poly, train_target)print(lr2.predict([[50**2, 50]])) [1573.98423528] Regression equation1print(lr2.coef_, lr2.intercept_) [ 1.01433211 -21.55792498] 116.0502107827827 123456789point = np.arange(15,50)fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.plot(point, lr2.coef_[0]*point**2 + lr2.coef_[1]*point + lr2.intercept_)ax.scatter(50, lr2.predict([[50**2, 50]]), marker=&quot;^&quot;)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 12print(lr2.score(train_poly, train_target))print(lr2.score(test_poly, test_target)) # Underfitting 0.9706807451768623 0.9775935108325122 The model has improved a lot, but it is still underfit. It seems that a more complex model is needed. Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/28/Python/ML/ML_ch_3_2/"},{"title":"ML Practice 2_2","text":"Prepare data with Numpy12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 123import numpy as npfish_data = np.column_stack((fish_length, fish_weight))print(fish_data[:5]) [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ]] 12fish_target = np.concatenate((np.ones(35), np.zeros(14)))print(fish_target) [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Split data with Scikit-learn1234from sklearn.model_selection import train_test_split# stratify: spliting data according to class proportionstrain_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42) 123print(train_input.shape, test_input.shape)print(train_target.shape, test_target.shape)print(test_target) (36, 2) (13, 2) (36,) (13,) [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] KNN 1KNN fitting1234from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 Predicting new data1print(kn.predict([[25,150]])) # the actual data is a bream, but predicted to be smelt. [0.] 123456789import matplotlib.pyplot as plt# Scatter plot with new datafig, ax = plt.subplots()ax.scatter(train_input[:,0], train_input[:,1])ax.scatter(25, 150, marker=&quot;^&quot;)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 12345678910distances, indexes = kn.kneighbors([[25,150]]) # the nearest neighbors (default: 5)# Scatter plot with 5 nearest neighborsfig, ax = plt.subplots()ax.scatter(train_input[:,0], train_input[:,1])ax.scatter(25, 150, marker=&quot;^&quot;)ax.scatter(train_input[indexes,0], train_input[indexes,1], marker='D') # rhombus markerax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 123456789# Scatter plot on the same scalefig, ax = plt.subplots()ax.scatter(train_input[:,0], train_input[:,1])ax.scatter(25, 150, marker=&quot;^&quot;)ax.scatter(train_input[indexes,0], train_input[indexes,1], marker='D') # rhombus markerax.set_xlim((0,1000)) # change the x scaleax.set_xlabel('length')ax.set_ylabel('weight')plt.show() KNN 2Data Preprocessing1234# standard scoremean = np.mean(train_input, axis=0) # axis=0 : for each featurestd = np.std(train_input, axis=0)train_scaled = (train_input - mean) / std # broadcasting in numpy 12345678# Scatter plot with standard scorenew = ([25, 150] - mean ) / stdfig, ax = plt.subplots()ax.scatter(train_scaled[:,0], train_scaled[:,1])ax.scatter(new[0], new[1], marker=&quot;^&quot;)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() KNN fitting123test_scaled = (test_input - mean) / stdkn.fit(train_scaled, train_target)kn.score(test_scaled, test_target) # 1.0 Predicting new data1print(kn.predict([new])) # the actual data is a bream, and predicted to be bream. [1.] 12345678910distances, indexes = kn.kneighbors([new])# Scatter plot with 5 nearest neighborsfig, ax = plt.subplots()ax.scatter(train_scaled[:,0], train_scaled[:,1])ax.scatter(new[0], new[1], marker=&quot;^&quot;)ax.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D') # rhombus markerax.set_xlabel('length')ax.set_ylabel('weight')plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/28/Python/ML/ML_ch_2_2/"},{"title":"ML Practice 2_1","text":"ML AlgorithmSupervised Learning(지도 학습) Input(입력; independent variable) &amp; Target(타깃; dependent variable) Question with a correct answer Type 1: Classification(분류) Type 2: Regression(예측) Feature(특성) = independent variable(column) Unspervised Learning(비지도 학습) only Input, not Target Question without an answer algorithm automatically categorizes Data set12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 12fish_data = [[l,w] for l, w in zip(fish_length, fish_weight)]fish_target = [1]*35 + [0]*14 # 1: bream, 0: smelt KNN 1Create KNN12from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier() Data Split train set &amp; test set 1234train_input = fish_data[:35]train_target = fish_target[:35]test_input = fish_data[35:]test_target = fish_target[35:] Model fitting12kn = kn.fit(train_input, train_target)kn.score(test_input, test_target) # Sampling bias 0.0 KNN 2Numpy array123456import numpy as npinput_arr = np.array(fish_data)target_arr = np.array(fish_target)input_arr.shape, target_arr.shape ((49, 2), (49,)) Data Shuffle and Split12345np.random.seed(42)index = np.arange(49)print(index)np.random.shuffle(index)print(index) [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] 1234train_input = input_arr[index[:35]]train_target = target_arr[index[:35]]test_input = input_arr[index[35:]]test_target = target_arr[index[35:]] Scatter Plot12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(train_input[:,0],train_input[:,1])ax.scatter(test_input[:,0],test_input[:,1])ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() Model fitting12kn = kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 1print(kn.predict(test_input) == test_target) [ True True True True True True True True True True True True True True] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/28/Python/ML/ML_ch_2_1/"},{"title":"ML Practice 3_3","text":"Prepare Data123import pandas as pddf = pd.read_csv('https://bit.ly/perch_csv_data')perch_full = df.to_numpy() # Convert Pandas DataFrame to Numpy Array 12345678import numpy as npperch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) 1234from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_full, perch_weight, random_state=42) Transform Data※ Scikit-Learn Class Estimator(추정기; model class) : Fitting and predicting KNeighborsClassifier, LinearRegression, etc. common method : fit(), score(), predict() Transformer(변환기) and Pre-processors : transforming or imputing data PolynomialFeatures, StandardScaler, etc common method : fit(), transform() ※ Feature engineering(특성 공학) extracting new features using existing features existing features, square features of each, and features multiplied by each other. Import transformer1from sklearn.preprocessing import PolynomialFeatures Transform sample data case 1: Including a bias 123poly = PolynomialFeatures()poly.fit([[2,3]])print(poly.transform([[2,3]])) [[1. 2. 3. 4. 6. 9.]] &gt; existing features : 2, 3 &gt; new features : 1(for intercept), 4(2^2), 6(2*3), 9(3^2) case 2: Not including a bias (recommended) 123poly = PolynomialFeatures(include_bias=False)poly.fit([[2,3]])print(poly.transform([[2,3]])) [[2. 3. 4. 6. 9.]] &gt; existing features : 2, 3 &gt; new features : 4(2^2), 6(2*3), 9(3^2) Transform perch data1234567poly = PolynomialFeatures(include_bias=False)poly.fit(train_input)train_poly = poly.transform(train_input)print(train_input.shape) # have 3 featuresprint(train_poly.shape) # have 9 featuresprint(poly.get_feature_names_out()) (42, 3) (42, 9) ['x0' 'x1' 'x2' 'x0^2' 'x0 x1' 'x0 x2' 'x1^2' 'x1 x2' 'x2^2'] 123test_poly = poly.transform(test_input)print(test_input.shape) # have 3 featuresprint(test_poly.shape) # have 9 features (14, 3) (14, 9) Mutiple Regression same process as training a linear regression model linear regression using multiple features degree 212345from sklearn.linear_model import LinearRegressionlr = LinearRegression()lr.fit(train_poly, train_target)print(lr.score(train_poly, train_target))print(lr.score(test_poly, test_target)) 0.9903183436982124 0.9714559911594134 Multiple regression solves the linear model’s underfitting problem. The score for the training set is very high. degree 5123456poly = PolynomialFeatures(degree=5, include_bias=False)poly.fit(train_input)train_poly = poly.transform(train_input)test_poly = poly.transform(test_input)print(train_poly.shape)print(test_poly.shape) (42, 55) (14, 55) 123lr.fit(train_poly, train_target)print(lr.score(train_poly, train_target))print(lr.score(test_poly, test_target)) 0.9999999999991097 -144.40579242684848 The score for the training set is almost perfect. But the score for the testing set is extremely negative. The model appears to be too overfitting to the training set. Regularization(규제) preventing the model from overfitting the training set linear regression model : reducing the size of the coefficient multiplied by the feature. hyperparameter: alpha parameter which has to be set in advance increase/decrease in regulatory intensity adjusted to increase the performance of the model Conceptual understanding is important, but it doesn’t mean much in practice. No guarantee of performance compared to working hours. More than 100 libraries in scikit-learn, and the types and numbers of hyperparameters vary. Better to use the existing hyperparameters, for unfamiliar models. Normalize feature scales using StandardScaler class in scikit-learn 1234567from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_poly)train_scaled = ss.transform(train_poly)test_scaled = ss.transform(test_poly)print(train_poly.shape)print(test_poly.shape) (42, 55) (14, 55) Ridge regression based on the square value of the coefficient 12345from sklearn.linear_model import Ridgeridge = Ridge()ridge.fit(train_scaled, train_target)print(ridge.score(train_scaled, train_target))print(ridge.score(test_scaled, test_target)) 0.9896101671037343 0.9790693977615397 Many features are used, but they’re not overfitting the training set and perform well on the test set. 123import matplotlib.pyplot as plttrain_score = []test_score = [] 123456alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]for alpha in alpha_list: ridge = Ridge(alpha=alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) 123456fig, ax = plt.subplots()ax.plot(np.log10(alpha_list), train_score)ax.plot(np.log10(alpha_list), test_score)ax.set_xlabel('log10(alpha)')ax.set_ylabel('R^2')plt.show() left side : overfitting right side : underfitting appropriate alpha : 0.1 1234ridge = Ridge(alpha=0.1)ridge.fit(train_scaled, train_target)print(ridge.score(train_scaled, train_target))print(ridge.score(test_scaled, test_target)) 0.9903815817570366 0.9827976465386926 Lasso regression based on the absolute value of the coefficient The coefficient can be completely zero. 12345from sklearn.linear_model import Lassolasso = Lasso()lasso.fit(train_scaled, train_target)print(lasso.score(train_scaled, train_target))print(lasso.score(test_scaled, test_target)) 0.989789897208096 0.9800593698421883 Many features are used, but they’re not overfitting the training set and perform well on the test set. 123456789train_score = []test_score = []alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]for alpha in alpha_list: lasso = Lasso(alpha=alpha, max_iter=10000) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+04, tolerance: 5.183e+02 coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e+04, tolerance: 5.183e+02 coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive 123456fig, ax = plt.subplots()ax.plot(np.log10(alpha_list), train_score)ax.plot(np.log10(alpha_list), test_score)ax.set_xlabel('log10(alpha)')ax.set_ylabel('R^2')plt.show() left side : overfitting right side : underfitting appropriate alpha : 10 1234lasso = Lasso(alpha=10)lasso.fit(train_scaled, train_target)print(lasso.score(train_scaled, train_target))print(lasso.score(test_scaled, test_target)) 0.9888067471131867 0.9824470598706695 1print(np.sum(lasso.coef_==0)) 40 40 coefficients became zero Of the 55 features, only 15 were finally used. Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/29/Python/ML/ML_ch_3_3/"},{"title":"ML Practice 4_2","text":"Gradient Descent(경사 하강법): Algorithm for finding the minimum value of a loss function using a sample of a training set stochastic gradient descent(확률적 경사 하강법; SGD) method of randomly selecting one sample from a training set minibatch gradient descent(미니배치 경사 하강법) method of randomly selecting several samples from a training set batch gradient descent(배치 경사 하강법) method of selecting all the samples from a training set at once Sampling method is different from the existing model. (more detailed approach) It aims to correct errors by reducing the slope of the loss function SGDClassifier : Create a classification model using SGD. SGDRegressor : Create a regression model using SGD. Epoch : process of using the entire training set once Usage Deep learning algorithm (especially, image and text) Tree algorithm + Gradient Descent = Boosting ex) LightGBM, Xgboost, Catboost Loss function(손실 함수) Cost function 비용 함수) Loss is the difference between the predicted value and the actual value of the model (equivalent to error) Loss function is a function that expresses loss of the model an indicator of how poorly a model processes data Loss function must be differentiable. Prepare Data12345# Importimport pandas as pdfish = pd.read_csv(&quot;https://bit.ly/fish_csv_data&quot;)fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()fish_target = fish['Species'].to_numpy() 123456# Splitfrom sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((119, 5), (40, 5), (119,), (40,)) 1234567# Normalizefrom sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) ※ To prevent data leakage, make sure to convert the test set to the statistics learned from the training set.※ Data leakage : containing the information you want to predict in the data used for model training SGD ClassifierFitting model set 2 parameter in SGD Classifier loss : specifying the type of loss function max_iter : specifying the number of epochs to be executed In the case of a multi-classification model, if loss is set as ‘log’, a binary classification model is created for each class. 123456from sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss='log', max_iter=10, random_state=42)sc.fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.773109243697479 0.775 /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, 1234# partial_fit() : continue training one epoch per callsc.partial_fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.8151260504201681 0.85 Finding appropriate epoch12345678910import numpy as npsc = SGDClassifier(loss='log', random_state=42)train_score = []test_score = []classes = np.unique(train_target)for _ in range(0, 300): # _ : temporal variable sc.partial_fit(train_scaled, train_target, classes=classes) train_score.append(sc.score(train_scaled, train_target)) test_score.append(sc.score(test_scaled, test_target)) 12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(train_score)ax.plot(test_score)ax.set_xlabel('epoch')ax.set_ylabel('accuracy')plt.show() In the early stages of epoch, the scores of training sets and test sets are low because they are underfitting. After epoch 100, the score difference between the training set and the test set gradually increases. Epoch 100 appears to be the most appropriate number of iterations. 123456# SGD classifier stops by itself, if performance does not improve during a certain epoch.# tol = None : to repeat unconditionally untill max_itersc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)sc.fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.957983193277311 0.925 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/29/Python/ML/ML_ch_4_2/"},{"title":"ML Practice 3_1","text":"Prepare DataData Set12345678910111213141516171819import numpy as npperch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] ) Visualize Data12345678import matplotlib.pyplot as plt# object orientationfig, ax = plt.subplots()ax.scatter(perch_length, perch_weight)ax.set_label(&quot;length&quot;)ax.set_label(&quot;weight&quot;)plt.show() KNN Regression low importance Split Data12345from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((42,), (14,), (42,), (14,)) 12345# change data set to two-dimensional arraytrain_input = train_input.reshape(-1,1)test_input = test_input.reshape(-1,1)print(train_input.shape, test_input.shape) (42, 1) (14, 1) Model fitting12345from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor()knr.fit(train_input, train_target)knr.score(test_input, test_target) # Coefficient of Determination (R-squared) 0.992809406101064 MAE Returns the average of absolute value errors between targets and predictions. 12from sklearn.metrics import mean_absolute_errortest_prediction = knr.predict(test_input) 12mae = mean_absolute_error(test_target, test_prediction)print(mae) # On average, about 19.2 grams different from the target. 19.157142857142862 Overfitting vs. Underfitting Overfitting: Good prediction from training data and poor prediction from testing data difficulty in finding and solving Underfitting: Poor prediction from training data and good prediction from testing data Or, poor prediction on both sides The amount of data is small or the model is too simple. 12print(knr.score(train_input, train_target))print(knr.score(test_input, test_target)) 0.9698823289099254 0.992809406101064 123456789# Set the number of neighbors to 3.knr.n_neighbors = 3knr.fit(train_input, train_target)print(knr.score(train_input, train_target))print(knr.score(test_input, test_target))test_prediction = knr.predict(test_input)mae = mean_absolute_error(test_target, test_prediction)print(mae) # On average, about 35.4 grams different from the target. 0.9804899950518966 0.9746459963987609 35.42380952380951 Conclusion k=5 : R^2= 0.99, MAE=19.2 k=3 : R^2= 0.97, MAE=35.4 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/28/Python/ML/ML_ch_3_1/"},{"title":"ML Practice 4_1","text":"Prepare DataImport data set1234import pandas as pdfish = pd.read_csv('https://bit.ly/fish_csv_data')print(fish.head()) Species Weight Length Diagonal Height Width 0 Bream 242.0 25.4 30.0 11.5200 4.0200 1 Bream 290.0 26.3 31.2 12.4800 4.3056 2 Bream 340.0 26.5 31.1 12.3778 4.6961 3 Bream 363.0 29.0 33.5 12.7300 4.4555 4 Bream 430.0 29.0 34.0 12.4440 5.1340 1print(pd.unique(fish['Species'])) ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt'] Convert to Numpy array123fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()print(fish_input.shape)print(fish_input[:3]) (159, 5) [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961]] 12fish_target = fish['Species'].to_numpy()print(fish_target.shape) (159,) Split and Standardize1234from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state=42) 12345from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) 12print(train_input[:3])print(train_scaled[:3]) [[720. 35. 40.6 16.3618 6.09 ] [500. 45. 48. 6.96 4.896 ] [ 7.5 10.5 11.6 1.972 1.16 ]] [[ 0.91965782 0.60943175 0.81041221 1.85194896 1.00075672] [ 0.30041219 1.54653445 1.45316551 -0.46981663 0.27291745] [-1.0858536 -1.68646987 -1.70848587 -1.70159849 -2.0044758 ] [-0.79734143 -0.60880176 -0.67486907 -0.82480589 -0.27631471] [-0.71289885 -0.73062511 -0.70092664 -0.0802298 -0.7033869 ]] [[-0.88741352 -0.91804565 -1.03098914 -0.90464451 -0.80762518] [-1.06924656 -1.50842035 -1.54345461 -1.58849582 -1.93803151] [-0.54401367 0.35641402 0.30663259 -0.8135697 -0.65388895] [-0.34698097 -0.23396068 -0.22320459 -0.11905019 -0.12233464] [-0.68475132 -0.51509149 -0.58801052 -0.8998784 -0.50124996]] KNN ClassifierModel fitting1234567from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors=3)kn.fit(train_scaled, train_target)print(kn.score(train_scaled, train_target))print(kn.score(test_scaled, test_target)) 0.8907563025210085 0.85 Multi-class Classfication1234import numpy as npproba = kn.predict_proba(test_scaled[:5])print(kn.classes_)print(np.round(proba, decimals=4)) ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] 12distances, indexes = kn.kneighbors(test_scaled[3:4]) # Two-dimensional array must be inputprint(train_target[indexes]) [['Roach' 'Perch' 'Perch']] The probability calculated by the model is the ratio of the nearest neighbor. In this model(k=3), the probability values are 0, 1/3, 2/3, and 1. If k is set as 5, the probability values may be 0, 0.2, 0.4, 0.6, 0.8 and 1. 123456kn = KNeighborsClassifier(n_neighbors=5)kn.fit(train_scaled, train_target)proba = kn.predict_proba(test_scaled[:5])print(kn.classes_)print(np.round(proba, decimals=4))print(kn.predict(test_scaled[:5])) ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] [[0. 0. 0.6 0. 0.4 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0.2 0.8 0. 0. 0. ] [0. 0. 0.8 0. 0.2 0. 0. ] [0. 0. 0.8 0. 0.2 0. 0. ]] ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch'] Logistic Regression: Estimating a model with a regression equation for categorical dependent variables. Despite its name, a classification model rather than regression model Highly important model used as basic statistics (especially medical statistics) the basis of the machine learning classification model. early model of deep learning To overcome the linearity assumption problem of general regression equation Logit transformation : the log of the odds ratio Using the logit of Y as the dependent variable of the regression Sigmoid function also called a logistic function Convert the value z calculated by linear regression to a probability value between 0 and 1 z &lt; 0: the function approaches 0 z &gt; 0: the function approaches 1 z = 0: the function value is 0.5 1234567891011import numpy as npimport matplotlib.pyplot as pltz = np.arange(-5, 5, 0.1)phi = 1 / (1 + np.exp(-z)) # sigmoid functionfig, ax = plt.subplots()ax.plot(z, phi)ax.set_xlabel('z', fontsize=12)ax.set_ylabel('phi', fontsize=12)ax.set_title(&quot;Sigmoid Function for Logistic Regression&quot;, fontsize=15)plt.show() Binary classification12345# Boolean Indexing: using a boolean vector to filter the data. # Choose only Bream and Smelt from the training set.bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes] 12345678from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt)print(lr.classes_) # 0: Bream / 1: Smeltproba = lr.predict_proba(train_bream_smelt[:5])print(np.round(proba, decimals=3)) # 5 rows, 2 columnsprint(lr.predict(train_bream_smelt[:5])) ['Bream' 'Smelt'] [[0.998 0.002] [0.027 0.973] [0.995 0.005] [0.986 0.014] [0.998 0.002]] ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] 1print(lr.coef_, lr.intercept_) [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132] z = - 0.404 * Weight - 0. 576 * Length - 0.663 * Diagonal - 1.013 * Height - 0.732 * Width - 2.162 12decisions = lr.decision_function(train_bream_smelt[:5])print(decisions) # original z-value of positive class(Smelt) [-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] 12from scipy.special import expitprint(expit(decisions)) # probability value through sigmoid function [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] Multi-class classification basically use iterative algorithms (max_iter, default 100) in this model, set max_iter as 1000 (for sufficient training) L2 Regularization based on the square value of the coefficient such as ridge regression hyperparameter; C ( default 1) the smaller the value, the greater the regulation. in this model, set C as 20 (in order to ease regulations a little) 1234lr = LogisticRegression(C=20, max_iter=1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target)) 0.9327731092436975 0.925 1234print(lr.classes_)proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals=3)) # 5 rows, 7 columnsprint(lr.predict(test_scaled[:5])) ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 1print(lr.coef_.shape, lr.intercept_.shape) (7, 5) (7,) [[-1.49002087 -1.02912886 2.59345551 7.70357682 -1.2007011 ] [ 0.19618235 -2.01068181 -3.77976834 6.50491489 -1.99482722] [ 3.56279745 6.34357182 -8.48971143 -5.75757348 3.79307308] [-0.10458098 3.60319431 3.93067812 -3.61736674 -1.75069691] [-1.40061442 -6.07503434 5.25969314 -0.87220069 1.86043659] [-1.38526214 1.49214574 1.39226167 -5.67734118 -4.40097523] [ 0.62149861 -2.32406685 -0.90660867 1.71599038 3.6936908 ]] [-0.09205179 -0.26290885 3.25101327 -0.14742956 2.65498283 -6.78782948 1.38422358] The z value is calculated one by one for each class and classified into the class that outputs the highest value. Softmax function also called a normalized exponential function (because of using exponential functions) The outputs of several linear equations are compressed from 0 to 1, and the total sum is 1. e_sum = e^z1 + e^z2 + … + e^z7 s1 = e^z1/e_sum, s2 = e^z2/e_sum, … , s7 = e^z7/e_sum 12decision = lr.decision_function(test_scaled[:5])print(np.round(decision, decimals=3)) # original z value [[ -6.498 1.032 5.164 -2.729 3.339 0.327 -0.634] [-10.859 1.927 4.771 -2.398 2.978 7.841 -4.26 ] [ -4.335 -6.233 3.174 6.487 2.358 2.421 -3.872] [ -0.683 0.453 2.647 -1.187 3.265 -5.753 1.259] [ -6.397 -1.993 5.816 -0.11 3.503 -0.112 -0.707]] 123from scipy.special import softmaxproba = softmax(decision, axis=1)print(np.round(proba, decimals=3)) # probability value through softmax function [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/29/Python/ML/ML_ch_4_1/"},{"title":"ML Practice 5_3","text":"Ensemble algorithm that performs best in dealing with structured data Bagging : A method of aggregating results by taking multiple bootstrap samples and training each model. (parallel learning) Random Forest Boosting : (sequential learning) GBM –&gt; XGBoost –&gt; LightGBM Random Forest Create decision trees randomly and make final predictions based on each tree’s predictions. Classification : Average the probabilities for each class of each tree and uses the class with the highest probability as a prediction. Regression : Average the predictions of each tree. Bootstrap : method of sampling data by permitting duplication in a dataset 1234567891011import numpy as npimport pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol', 'sugar', 'pH']].to_numpy()target = wine['class'].to_numpy()from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42) 12345from sklearn.model_selection import cross_validatefrom sklearn.ensemble import RandomForestClassifierrf = RandomForestClassifier(n_jobs=-1, random_state=42)scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # overfitting 0.9973541965122431 0.8905151032797809 12rf.fit(train_input, train_target)print(rf.feature_importances_) [0.23167441 0.50039841 0.26792718] OOB(out of bag) Sample : remaining sample not included in bootstrap sample same effect as cross-validation using a verification set 123rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)rf.fit(train_input, train_target)print(rf.oob_score_) 0.8934000384837406 GBM(Gradient Boosting Machine) Correct errors in previous trees by using shallow trees. Adjust the speed (step width) through the learning rate parameter less likely to overfit but speed is slow 1234from sklearn.ensemble import GradientBoostingClassifiergb = GradientBoostingClassifier(random_state=42)scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # good fitting 0.8881086892152563 0.8720430147331015 1234# n_estimators = 500 (default 100), learning rate = 0.2 (default 0.1)gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # good fitting 0.9464595437171814 0.8780082549788999 Overall Flow of ML Data preprocessing, EDA, Visualization Design the entire flow as a basic model compare multiple models with default hyperparameter Cross-validation and Hyperparameter tuning Repeat the above process until finding the best result Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/30/Python/ML/ML_ch_5_3/"},{"title":"ML Practice 5_2","text":"Cross Validation: Repeated process of spliting validation set and evaluating model. Train set : Validation set : Test set = 6 : 2 : 2 (generally) Test sets are not used in the model learning process. In Kagge competition, test sets are given separately. purpose : To make a good model A good model doesn’t mean high-performance model. A good model means low-error and stable model. Because it takes a long time, it is useful when there is not much data. Prepare data1234import pandas as pdwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol','sugar','pH']].to_numpy()target = wine[['class']].to_numpy() 1234567from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42)sub_input, val_input, sub_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42) 1sub_input.shape, val_input.shape, test_input.shape ((4157, 3), (1040, 3), (1300, 3)) Create model12345from sklearn.tree import DecisionTreeClassifierdt = DecisionTreeClassifier(random_state=42)dt.fit(sub_input, sub_target)print(dt.score(sub_input, sub_target))print(dt.score(val_input, val_target)) # overfitting 0.9971133028626413 0.864423076923077 Validate model1234from sklearn.model_selection import cross_validatescores = cross_validate(dt, train_input, train_target) # dictionary typefor item in scores.items(): print(item) ('fit_time', array([0.01251197, 0.00755358, 0.0074594 , 0.00742102, 0.00734329])) ('score_time', array([0.00133634, 0.00079608, 0.0007925 , 0.00083232, 0.00076413])) ('test_score', array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])) 12import numpy as npprint(np.mean(scores['test_score'])) 0.855300214703487 In cross-validation, a splitter must be specified to mix training sets. Regression model &gt; KFold Classification model &gt; StratifiedKFold 1234from sklearn.model_selection import StratifiedKFoldsplitter = StratifiedKFold(shuffle=True, random_state=42) # default : 5 foldscores = cross_validate(dt, train_input, train_target, cv=splitter)print(np.mean(scores['test_score'])) 0.8539548012141852 123splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # 10 foldscores = cross_validate(dt, train_input, train_target, cv=splitter)print(np.mean(scores['test_score'])) 0.8574181117533719 Hyperparameter Tuning ex) max_depth=3, accuracy=0.84 Finding the best value by adjusting multiple parameters simultaneously. AutoML : technology that automatically performs hyperparameter tuning without intervention of person. Grid Search, Random Search Grid Search Perform hyperparameter tuning and cross-validation simultaneously Find the optimal hyperparameters based on all combinations of predetermined values. 12345678%%timefrom sklearn.model_selection import GridSearchCVparams = { 'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)gs.fit(train_input, train_target) CPU times: user 70.1 ms, sys: 6.06 ms, total: 76.1 ms Wall time: 183 ms 123dt = gs.best_estimator_print(dt)print(dt.score(train_input, train_target)) DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42) 0.9615162593804117 12print(gs.cv_results_['mean_test_score'])print(gs.best_params_) [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605] {'min_impurity_decrease': 0.0001} 123456789101112%%timefrom sklearn.model_selection import GridSearchCVparams = { 'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005], 'max_depth' : [3, 4, 5, 6, 7]}# Change the values in params and create a total of 5 models with each value.# n_jobs=-1 : to enable all cores in the systemgs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)gs.fit(train_input, train_target) CPU times: user 167 ms, sys: 4.85 ms, total: 172 ms Wall time: 585 ms 123dt = gs.best_estimator_print(dt)print(dt.score(train_input, train_target)) DecisionTreeClassifier(max_depth=7, min_impurity_decrease=0.0005, random_state=42) 0.8830094285164518 12print(gs.cv_results_['mean_test_score']) # 5*5=25print(gs.best_params_) [0.84125583 0.84125583 0.84125583 0.84125583 0.84125583 0.85337806 0.85337806 0.85337806 0.85337806 0.85318557 0.85780355 0.85799604 0.85857352 0.85857352 0.85838102 0.85645721 0.85799678 0.85876675 0.85972866 0.86088306 0.85607093 0.85761031 0.85799511 0.85991893 0.86280466] {'max_depth': 7, 'min_impurity_decrease': 0.0005} The optimal value of ‘min_impurity_decrease’ varies when the value of ‘max_depth’ changes. Random Search Find the optimal hyperparameters based on possible combinations within a predetermined range of values. Delivers probability distribution objects that can sample parameters. 1234567# randint : sampling int# uniform : sampling floatfrom scipy.stats import uniform, randintparams = { 'min_impurity_decrease' : uniform(0.0001, 0.001), 'max_depth' : randint(20, 50)} 123456%%timefrom sklearn.model_selection import RandomizedSearchCVgs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)gs.fit(train_input, train_target) CPU times: user 629 ms, sys: 15.8 ms, total: 645 ms Wall time: 2.54 s 123dt = gs.best_estimator_print(dt)print(dt.score(train_input, train_target)) DecisionTreeClassifier(max_depth=29, min_impurity_decrease=0.000437615171403628, random_state=42) 0.8903213392341736 1print(gs.best_params_) {'max_depth': 29, 'min_impurity_decrease': 0.000437615171403628} Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/30/Python/ML/ML_ch_5_2/"},{"title":"ML Practice 5_1","text":"Prepare Data Import wine data set class 0: red wine class 1: white wine 123import pandas as pdwine = pd.read_csv(&quot;https://bit.ly/wine_csv_data&quot;)print(wine.head()) alcohol sugar pH class 0 9.4 1.9 3.51 0.0 1 9.8 2.6 3.20 0.0 2 9.8 2.3 3.26 0.0 3 9.8 1.9 3.16 0.0 4 9.4 1.9 3.51 0.0 12# checking missing value and types of variablewine.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB 1print(wine.describe()) # vary in scale of variables, in need of standardization alcohol sugar pH class count 6497.000000 6497.000000 6497.000000 6497.000000 mean 10.491801 5.443235 3.218501 0.753886 std 1.192712 4.757804 0.160787 0.430779 min 8.000000 0.600000 2.720000 0.000000 25% 9.500000 1.800000 3.110000 1.000000 50% 10.300000 3.000000 3.210000 1.000000 75% 11.300000 8.100000 3.320000 1.000000 max 14.900000 65.800000 4.010000 1.000000 Split data into training sets and test sets 12data = wine[['alcohol', 'sugar', 'pH']].to_numpy()target = wine['class'].to_numpy() 12345from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42)print(train_input.shape, test_input.shape) (5197, 3) (1300, 3) Standardize Data Decision tree does not require standardized preprocessing, but it is recommended to perform standardization basically. 12345from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) Logistic Regression Model12345from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target)) 0.7808350971714451 0.7776923076923077 12print(lr.predict_proba(train_scaled[:5]))print(lr.predict(train_scaled[:5])) [[0.06189333 0.93810667] [0.21742616 0.78257384] [0.40703571 0.59296429] [0.45226659 0.54773341] [0.00530794 0.99469206]] [1. 1. 1. 1. 1.] 1print(lr.coef_, lr.intercept_) [[ 0.51270274 1.6733911 -0.68767781]] [1.81777902] Decision Tree Model: a non-parametric supervised learning method used for classification and regression to predict the value of a target variable by learning simple decision rules inferred from the data features Simple to understand and to interpret More likely to be overfitting the training set. New Algorithm Using Decision Tree Algorithms XGBoost, LightGBM, CatBoost, etc In particular, LightGBM is now widely used in practice. DecisionTreeClassifier12345from sklearn.tree import DecisionTreeClassifierdt = DecisionTreeClassifier(random_state=42)dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled, test_target)) # appear to be overfitting 0.996921300750433 0.8592307692307692 12345678910import matplotlib.pyplot as pltfrom sklearn.tree import plot_treefig, ax = plt.subplots(figsize=(18,10))plot_tree(dt, filled=True, feature_names=['alcohol','sugar','pH'])plt.show()# - conditions for testing : sugar# - impurity : gini# - samples : total number of samples# - value : number of samples by class Impurity(불순도) parameter criterion; default ‘gini’ gini = 1 - (negative_prop^2 + positive_prop^2) best : 0 (pure node) worst : 0.5 (exactly half and half) entropy = - negative_prop * log_2(negative_prop) - positive_prop * log_2(positive_prop) Information gain(정보 이득) : impurity differences between parent node and child node Decision tree splits nodes to maximize information gain using impurity criteria. Pruning(가지치기) In order to prevent overfitting the training set By specifying the maximum depth of a tree that can grow 1234dt = DecisionTreeClassifier(max_depth=3, random_state=42)dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled, test_target)) # successful in reducing overfitting 0.8454877814123533 0.8415384615384616 123fig, ax = plt.subplots(figsize=(18,10))plot_tree(dt, filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 123456789# Tree Plot Image Downloadimport graphvizfrom sklearn import treedot_data = tree.export_graphviz( dt, out_file=None, feature_names = ['alcohol','sugar','pH'], filled=True)graph = graphviz.Source(dot_data, format=&quot;png&quot;) graph 1graph.render(&quot;decision_tree_graphivz&quot;) 'decision_tree_graphivz.png' 12345678910111213141516# Customize color of nodesfrom matplotlib.colors import ListedColormap, to_rgbimport numpy as npplt.figure(figsize=(20, 15))artists = plot_tree(dt, filled = True, feature_names = ['alcohol','sugar','pH'])colors = ['blue', 'red']for artist, impurity, value in zip(artists, dt.tree_.impurity, dt.tree_.value): r, g, b = to_rgb(colors[np.argmax(value)]) f = impurity * 2 artist.get_bbox_patch().set_facecolor((f + (1-f)*r, f + (1-f)*g, f + (1-f)*b)) artist.get_bbox_patch().set_edgecolor('black')plt.show() parameter min_impurity_decrease; default 0.0 Split nodes if this split induces a decrease of the impurity greater than or equal to this value. More likely to be asymmetric tree 12345678dt = DecisionTreeClassifier(min_impurity_decrease=0.0005, random_state=42)dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled, test_target))fig, ax = plt.subplots(figsize=(18, 10))plot_tree(dt, filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 0.8874350586877044 0.8615384615384616 Feature importance: an indicator of the degree to which each feature contributed to reducing impurities Multiply the information gain and the ratio of the total sample by each node, and add it up by feature. 1print(dt.feature_importances_) # Sugar is the most important feature. [0.12345626 0.86862934 0.0079144 ] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/30/Python/ML/ML_ch_5_1/"},{"title":"ML Practice 6_2","text":"K-means Clustering Find mean of pixel value : cluster center, centroid Determine the centers of k clusters at random. Find the nearest cluster center from each sample and designate it as a sample of that cluster. Change the center of the cluster to the average value of the samples belonging to the cluster. Repeat 2~3 until there is no change in the center of the cluster. Import Data1!wget https://bit.ly/fruits_300_data -O fruits_300.npy --2022-03-31 02:09:21-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-03-31 02:09:21-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.114.3 Connecting to github.com (github.com)|140.82.114.3|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-03-31 02:09:22-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.01s 2022-03-31 02:09:22 (223 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] 1234import numpy as npfruits = np.load('fruits_300.npy')print(fruits.shape) (300, 100, 100) 12fruits_2d = fruits.reshape(-1, 100*100)print(fruits_2d.shape) (300, 10000) KMeans Class123from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruits_2d) # no target KMeans(n_clusters=3, random_state=42) 1print(km.labels_) # labels : [0, 1, 2] [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 12print(np.unique(km.labels_, return_counts=True))# label 0: 111 samples / label 1: 98 samples / label 2: 91 samples (array([0, 1, 2], dtype=int32), array([111, 98, 91])) Images of each label1234567891011121314import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # the number of sample rows = int(np.ceil(n/10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() 1draw_fruits(fruits[km.labels_==0]) 1draw_fruits(fruits[km.labels_==1]) 1draw_fruits(fruits[km.labels_==2]) label 0: mostly pineapples label 1: mostly bananas label 2: mostly apples Centroid1print(km.cluster_centers_.shape) (6, 10000) (6, 100, 100) 1draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) 1print(km.transform(fruits_2d[100:101])) # two-dimension array input required [[3393.8136117 8837.37750892 5267.70439881]] 1print(km.predict(fruits_2d[100:101])) [0] 1draw_fruits(fruits[100:101]) Finding the best K (Elbow method) inertia : sum of squares of the distance between centroid and each sample As K increases, inertia decreases. Set the optimal K at the point where the inertia graph is bent. 1234567891011121314151617181920inertia = []for k in range(2,7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)slope = []lst = []for idx, val in enumerate(inertia): if idx==0: slope.append(0) lst.append(0) else: slope.append(val - inertia[idx-1]) lst.append(slope[idx-1]-slope[idx])fig, ax = plt.subplots()ax.plot(range(2,7), inertia)ax.scatter(2+np.argmax(lst), inertia[np.argmax(lst)], marker=&quot;o&quot;, color=&quot;red&quot;)plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/31/Python/ML/ML_ch_6_2/"},{"title":"ML Practice 6_1","text":"Unsupervised Learning No dependent variables and targets. (↔ Supervised Learning) Clustering (Multiple class) Must be many different types of data Linked to deep learning Dimensionality reduction Import Numpy Data1!wget https://bit.ly/fruits_300_data -O fruits_300.npy --2022-03-31 01:12:51-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10 Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-03-31 01:12:51-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.113.4 Connecting to github.com (github.com)|140.82.113.4|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-03-31 01:12:51-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.02s 2022-03-31 01:12:51 (157 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] 12345import numpy as np# 100 apples, 100 pineapples, 100 bananasfruits = np.load('fruits_300.npy')print(fruits.shape) (300, 100, 100) image samples of three dimensions dimension 1: the number of samples dimension 2: the height of image dimension 3: the width of image 300 pieces of image sample of 100 x 100 size. Visualize Image Data black-and-white photographs integer value from 0 to 255 1234import matplotlib.pyplot as pltplt.imshow(fruits[0], cmap='gray') # 0: black, 255: whiteplt.show() 12plt.imshow(fruits[0], cmap='gray_r') # 0: white, 255: blackplt.show() multiple images 1234fig, ax = plt.subplots(1,2)ax[0].imshow(fruits[100], cmap='gray_r')ax[1].imshow(fruits[200], cmap='gray_r')plt.show() Pixel value analysis12345# convert 100*100 images to one-dimensional array with a length of 10000apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100)print(apple.shape, pineapple.shape, banana.shape) (100, 10000) (100, 10000) (100, 10000) average comparison of pixel values for each image 12345plt.hist(np.mean(apple, axis=1), alpha=0.8)plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend(['apple','pineapple','banana'])plt.show() 12345fig, ax = plt.subplots(1,3,figsize=(15,5))ax[0].bar(range(10000),np.mean(apple, axis=0))ax[1].bar(range(10000),np.mean(pineapple, axis=0))ax[2].bar(range(10000),np.mean(banana, axis=0))plt.show() representative image using pixel mean 123456789apple_mean = np.mean(apple, axis=0).reshape(100,100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100,100)banana_mean = np.mean(banana, axis=0).reshape(100,100)fig, ax = plt.subplots(1,3,figsize=(15,5))ax[0].imshow(apple_mean, cmap='gray_r')ax[1].imshow(pineapple_mean, cmap='gray_r')ax[2].imshow(banana_mean, cmap='gray_r')plt.show() 100 images close to the average value 1234# MAE(Mean Absolute Error)abs_diff = np.abs(fruits - apple_mean)abs_mean = np.mean(abs_diff, axis=(1,2))print(abs_mean.shape) # one-dimensions array (300,) 1234567apple_index = np.argsort(abs_mean)[:100] # extract 100 indexes in the smallest order of MAEfig, ax = plt.subplots(10,10,figsize=(10,10))for i in range(10): for j in range(10): ax[i,j].imshow(fruits[apple_index[i*10+j]], cmap='gray_r') ax[i,j].axis('off') # remove axisplt.show() 33 48 70 57 87 12 78 59 1 74 86 38 50 92 69 27 68 30 66 24 76 98 15 84 47 90 3 94 53 23 14 71 32 7 73 36 55 77 21 10 17 39 99 95 11 35 65 6 61 22 56 89 2 13 80 0 97 4 58 34 40 43 75 82 54 16 31 49 93 37 63 64 41 28 67 25 96 8 83 46 19 79 72 5 85 29 20 60 81 9 45 51 88 62 91 26 52 18 44 42 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/31/Python/ML/ML_ch_6_1/"},{"title":"ML Practice 6_3","text":"Dimensionaliy Reduction: Decreasing the size of the data by selecting some features that best represent the data To prevent overfitting and improve model performance PCA(Principal Component Analysis), LDA(Linear Discriminant Analysis), etc PCA principal component(PC) axis of data with the highest variance when projected on an axis expressed as a linear combination of existing variables Generally, it can be found as many as the features of the data as possible. To explain the overall variation with 2 to 3 principal component Import Data1234!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100) --2022-03-31 06:10:20-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-03-31 06:10:20-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 192.30.255.112 Connecting to github.com (github.com)|192.30.255.112|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-03-31 06:10:20-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.04s 2022-03-31 06:10:21 (79.3 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] PCA Model1234from sklearn.decomposition import PCApca = PCA(n_components=50)pca.fit(fruits_2d)print(pca.components_.shape) (50, 10000) 1234567891011121314import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # the number of sample rows = int(np.ceil(n/10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() 1draw_fruits(pca.components_.reshape(-1, 100, 100)) 123print(fruits_2d.shape) # 10000 featuresfruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape) # 50 features (300, 10000) (300, 50) reduced to 1/200 compared to the original size of the data Reconstruction of original data1234fruits_inverse = pca.inverse_transform(fruits_pca)print(fruits_inverse.shape)fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)print(fruits_reconstruct.shape) (300, 10000) (300, 100, 100) 1draw_fruits(fruits_reconstruct[0:100]) 1draw_fruits(fruits_reconstruct[100:200]) 1draw_fruits(fruits_reconstruct[200:300]) Even though 10,000 features were reduced to 50, the original data were preserved fairly well. Explained Variance: How well the principal component represents the variance of the original data.1print(np.cumsum(pca.explained_variance_ratio_)) 0.9215624972723878 [0.42357017 0.52298772 0.58876636 0.62907807 0.66324682 0.69606011 0.72179277 0.7423424 0.75606517 0.76949289 0.78101436 0.79046031 0.79924263 0.8077096 0.8146401 0.82109198 0.82688094 0.83199296 0.83685678 0.84166025 0.8461386 0.85051178 0.85459218 0.85848695 0.86221133 0.86580421 0.86911888 0.87229685 0.87534014 0.87837793 0.8812672 0.88402533 0.88667509 0.88923363 0.89175254 0.8942257 0.89662179 0.89893062 0.90115012 0.90331513 0.90544476 0.90740924 0.90933715 0.91123892 0.91308592 0.91491101 0.91664894 0.91833369 0.91995394 0.9215625 ] 123fig, ax = plt.subplots()ax.plot(pca.explained_variance_ratio_)plt.show() The first 10 PC represent most variance of the data. Subsequent PC could hardly explain the variance of the data. Use with other algorithms. Logistic Regression of 3 classes 1234from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()target = np.array([0]*100 + [1]*100 + [2]*100) # create target values cross-validation with original data 1234from sklearn.model_selection import cross_validatescores = cross_validate(lr, fruits_2d, target)print(np.mean(scores['test_score']))print(np.mean(scores['fit_time'])) 0.9966666666666667 1.511155652999878 cross-validation with reduced data in PCA 123scores = cross_validate(lr, fruits_pca, target)print(np.mean(scores['test_score']))print(np.mean(scores['fit_time'])) 1.0 0.07492985725402831 Specify the variance ratio123pca = PCA(n_components=0.5)pca.fit(fruits_2d)print(pca.n_components_) # 2 PC needed 2 12fruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape) (300, 2) 123scores = cross_validate(lr, fruits_pca, target)print(np.mean(scores['test_score']))print(np.mean(scores['fit_time'])) 0.9933333333333334 0.03829236030578613 /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, 12345from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruits_pca)print(np.unique(km.labels_, return_counts=True))# label 0: 110 / label 1: 99 / label 2: 91 (array([0, 1, 2], dtype=int32), array([110, 99, 91])) 1draw_fruits(fruits[km.labels_==0]) 1draw_fruits(fruits[km.labels_==1]) 1draw_fruits(fruits[km.labels_==2]) 123456fig, ax = plt.subplots()for label in range(3): data = fruits_pca[km.labels_==label] ax.scatter(data[:,0], data[:,1])ax.legend(['apple','banana','pineapple'])plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/31/Python/ML/ML_ch_6_3/"},{"title":"ML Practice 7_3","text":"Create DNN Model12345678910from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = \\ keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step 26435584/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step Define function of create model 1234567891011def model_fn(a_layer=None): model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28,28))) model.add(keras.layers.Dense(100, activation='relu')) if a_layer: model.add(a_layer) model.add(keras.layers.Dense(10, activation='softmax')) return model model = model_fn()model.summary &lt;bound method Model.summary of &lt;keras.engine.sequential.Sequential object at 0x7f9181716750&gt;&gt; Loss Curve12model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs = 5, verbose = 0) verbose default 1: print the indicator along with the progress bar per epoch. verbose 2: print the indicator without the progress bar per epoch. verbose 0: print none 123print(history) # classprint(history.history) # dictionaryprint(history.history.keys()) &lt;keras.callbacks.History object at 0x7f917bab27d0&gt; {'loss': [0.5360119342803955, 0.3935061991214752, 0.3552784025669098, 0.33411645889282227, 0.31946054100990295], 'accuracy': [0.8113541603088379, 0.8598541617393494, 0.8732083439826965, 0.8820000290870667, 0.8862708210945129]} dict_keys(['loss', 'accuracy']) loss curve (epoch 5) 123456import matplotlib.pyplot as pltplt.plot(history.history['loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.show() accuracy curve (epoch 5) 1234plt.plot(history.history['accuracy'])plt.xlabel('epoch')plt.ylabel('accuracy')plt.show() loss curve (epoch 20) 12345678model = model_fn()model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=20, verbose=0)plt.plot(history.history['loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.show() Validation Loss123456789101112model = model_fn()model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target))plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show() Epoch 1/20 1500/1500 [==============================] - 6s 4ms/step - loss: 0.5344 - accuracy: 0.8118 - val_loss: 0.4414 - val_accuracy: 0.8471 Epoch 2/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3950 - accuracy: 0.8577 - val_loss: 0.3638 - val_accuracy: 0.8668 Epoch 3/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3573 - accuracy: 0.8702 - val_loss: 0.3754 - val_accuracy: 0.8682 Epoch 4/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3365 - accuracy: 0.8791 - val_loss: 0.3783 - val_accuracy: 0.8701 Epoch 5/20 1500/1500 [==============================] - 5s 4ms/step - loss: 0.3191 - accuracy: 0.8865 - val_loss: 0.3576 - val_accuracy: 0.8772 Epoch 6/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3085 - accuracy: 0.8898 - val_loss: 0.3556 - val_accuracy: 0.8806 Epoch 7/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2982 - accuracy: 0.8948 - val_loss: 0.3736 - val_accuracy: 0.8807 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2910 - accuracy: 0.8976 - val_loss: 0.3443 - val_accuracy: 0.8869 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2841 - accuracy: 0.8998 - val_loss: 0.3757 - val_accuracy: 0.8832 Epoch 10/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2755 - accuracy: 0.9031 - val_loss: 0.4034 - val_accuracy: 0.8766 Epoch 11/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2700 - accuracy: 0.9059 - val_loss: 0.4085 - val_accuracy: 0.8792 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2655 - accuracy: 0.9075 - val_loss: 0.3936 - val_accuracy: 0.8835 Epoch 13/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2589 - accuracy: 0.9105 - val_loss: 0.4122 - val_accuracy: 0.8812 Epoch 14/20 1500/1500 [==============================] - 5s 4ms/step - loss: 0.2545 - accuracy: 0.9116 - val_loss: 0.4056 - val_accuracy: 0.8842 Epoch 15/20 1500/1500 [==============================] - 6s 4ms/step - loss: 0.2506 - accuracy: 0.9137 - val_loss: 0.4048 - val_accuracy: 0.8815 Epoch 16/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2454 - accuracy: 0.9159 - val_loss: 0.4132 - val_accuracy: 0.8808 Epoch 17/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2410 - accuracy: 0.9177 - val_loss: 0.4343 - val_accuracy: 0.8831 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2356 - accuracy: 0.9190 - val_loss: 0.4574 - val_accuracy: 0.8767 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2326 - accuracy: 0.9201 - val_loss: 0.4499 - val_accuracy: 0.8817 Epoch 20/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2284 - accuracy: 0.9204 - val_loss: 0.4834 - val_accuracy: 0.8751 There is a large difference in loss between training data and verification data. This is a typical overfitting model. 12345678910111213model = model_fn()model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target))plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show() Epoch 1/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5231 - accuracy: 0.8183 - val_loss: 0.4741 - val_accuracy: 0.8357 Epoch 2/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3940 - accuracy: 0.8576 - val_loss: 0.3736 - val_accuracy: 0.8671 Epoch 3/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3557 - accuracy: 0.8703 - val_loss: 0.3567 - val_accuracy: 0.8712 Epoch 4/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3273 - accuracy: 0.8796 - val_loss: 0.3398 - val_accuracy: 0.8790 Epoch 5/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3091 - accuracy: 0.8872 - val_loss: 0.3324 - val_accuracy: 0.8803 Epoch 6/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2904 - accuracy: 0.8925 - val_loss: 0.3194 - val_accuracy: 0.8842 Epoch 7/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2802 - accuracy: 0.8967 - val_loss: 0.3333 - val_accuracy: 0.8796 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2679 - accuracy: 0.9010 - val_loss: 0.3265 - val_accuracy: 0.8830 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2588 - accuracy: 0.9040 - val_loss: 0.3298 - val_accuracy: 0.8858 Epoch 10/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2482 - accuracy: 0.9068 - val_loss: 0.3282 - val_accuracy: 0.8840 Epoch 11/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2413 - accuracy: 0.9094 - val_loss: 0.3098 - val_accuracy: 0.8889 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2315 - accuracy: 0.9131 - val_loss: 0.3250 - val_accuracy: 0.8867 Epoch 13/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2260 - accuracy: 0.9141 - val_loss: 0.3164 - val_accuracy: 0.8911 Epoch 14/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2181 - accuracy: 0.9185 - val_loss: 0.3511 - val_accuracy: 0.8774 Epoch 15/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2128 - accuracy: 0.9200 - val_loss: 0.3397 - val_accuracy: 0.8817 Epoch 16/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2059 - accuracy: 0.9222 - val_loss: 0.3219 - val_accuracy: 0.8903 Epoch 17/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2021 - accuracy: 0.9240 - val_loss: 0.3423 - val_accuracy: 0.8859 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.1952 - accuracy: 0.9277 - val_loss: 0.3313 - val_accuracy: 0.8916 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.1918 - accuracy: 0.9272 - val_loss: 0.3396 - val_accuracy: 0.8871 Epoch 20/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.1879 - accuracy: 0.9295 - val_loss: 0.3354 - val_accuracy: 0.8904 Overfitting has decreased a little, but it is still necessary to improve. Dropout Basically, it is a principle to calculate all parameters. Neurons without some output are excluded from the calculation. 12model = model_fn(keras.layers.Dropout(0.3)) # drop out 30%model.summary() Model: &quot;sequential_5&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_5 (Flatten) (None, 784) 0 dense_10 (Dense) (None, 100) 78500 dropout (Dropout) (None, 100) 0 dense_11 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 123456789101112model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target))plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show() Epoch 1/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5967 - accuracy: 0.7907 - val_loss: 0.4495 - val_accuracy: 0.8294 Epoch 2/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4413 - accuracy: 0.8409 - val_loss: 0.4071 - val_accuracy: 0.8472 Epoch 3/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4044 - accuracy: 0.8547 - val_loss: 0.3616 - val_accuracy: 0.8674 Epoch 4/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3833 - accuracy: 0.8603 - val_loss: 0.3605 - val_accuracy: 0.8651 Epoch 5/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3688 - accuracy: 0.8646 - val_loss: 0.3423 - val_accuracy: 0.8750 Epoch 6/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3542 - accuracy: 0.8696 - val_loss: 0.3479 - val_accuracy: 0.8744 Epoch 7/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3439 - accuracy: 0.8725 - val_loss: 0.3449 - val_accuracy: 0.8752 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3356 - accuracy: 0.8763 - val_loss: 0.3356 - val_accuracy: 0.8802 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3280 - accuracy: 0.8796 - val_loss: 0.3361 - val_accuracy: 0.8801 Epoch 10/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3212 - accuracy: 0.8781 - val_loss: 0.3394 - val_accuracy: 0.8734 Epoch 11/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3200 - accuracy: 0.8813 - val_loss: 0.3327 - val_accuracy: 0.8763 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3115 - accuracy: 0.8852 - val_loss: 0.3325 - val_accuracy: 0.8776 Epoch 13/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3061 - accuracy: 0.8860 - val_loss: 0.3216 - val_accuracy: 0.8860 Epoch 14/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3034 - accuracy: 0.8860 - val_loss: 0.3193 - val_accuracy: 0.8864 Epoch 15/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2961 - accuracy: 0.8880 - val_loss: 0.3198 - val_accuracy: 0.8846 Epoch 16/20 1500/1500 [==============================] - 4s 2ms/step - loss: 0.2913 - accuracy: 0.8900 - val_loss: 0.3310 - val_accuracy: 0.8823 Epoch 17/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2870 - accuracy: 0.8933 - val_loss: 0.3162 - val_accuracy: 0.8848 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2838 - accuracy: 0.8931 - val_loss: 0.3321 - val_accuracy: 0.8838 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2829 - accuracy: 0.8935 - val_loss: 0.3320 - val_accuracy: 0.8840 Epoch 20/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2814 - accuracy: 0.8942 - val_loss: 0.3218 - val_accuracy: 0.8882 Overfitting has improved a lot. Save and Load Model123456model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')history = model.fit(train_scaled, train_target, epochs=10, verbose=0, validation_data=(val_scaled, val_target)) save_weights() : method of saving the parameters of a model save() : method of saving both the parameters and structure of a model ‘.h5’ : HDF5 format 12model.save_weights('model-weights.h5')model.save('model-whole.h5') 1!ls -al *.h5 -rw-r--r-- 1 root root 982664 Apr 5 02:37 best-model.h5 -rw-r--r-- 1 root root 333448 Apr 5 02:42 model-weights.h5 -rw-r--r-- 1 root root 982664 Apr 5 02:42 model-whole.h5 load previously saved parameters 12model = model_fn(keras.layers.Dropout(0.3))model.load_weights('model-weights.h5') 1234# Returns the largest value in the predict method resultimport numpy as npval_labels = np.argmax(model.predict(val_scaled), axis=-1)print(np.mean(val_labels == val_target)) 0.8825833333333334 load previously saved model 12model = keras.models.load_model('model-whole.h5')model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.8826 [0.3247545063495636, 0.8825833201408386] Callback12345678910model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5', save_best_only=True)model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb]) Epoch 1/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.5955 - accuracy: 0.7909 - val_loss: 0.4305 - val_accuracy: 0.8437 Epoch 2/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4369 - accuracy: 0.8436 - val_loss: 0.3847 - val_accuracy: 0.8572 Epoch 3/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4027 - accuracy: 0.8533 - val_loss: 0.3737 - val_accuracy: 0.8633 Epoch 4/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3833 - accuracy: 0.8607 - val_loss: 0.3648 - val_accuracy: 0.8628 Epoch 5/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3675 - accuracy: 0.8662 - val_loss: 0.3481 - val_accuracy: 0.8703 Epoch 6/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3544 - accuracy: 0.8710 - val_loss: 0.3434 - val_accuracy: 0.8758 Epoch 7/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3435 - accuracy: 0.8736 - val_loss: 0.3388 - val_accuracy: 0.8781 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3360 - accuracy: 0.8759 - val_loss: 0.3333 - val_accuracy: 0.8760 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3261 - accuracy: 0.8777 - val_loss: 0.3333 - val_accuracy: 0.8755 Epoch 10/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3203 - accuracy: 0.8808 - val_loss: 0.3319 - val_accuracy: 0.8807 Epoch 11/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3154 - accuracy: 0.8822 - val_loss: 0.3275 - val_accuracy: 0.8794 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3063 - accuracy: 0.8849 - val_loss: 0.3206 - val_accuracy: 0.8842 Epoch 13/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3024 - accuracy: 0.8871 - val_loss: 0.3239 - val_accuracy: 0.8815 Epoch 14/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3002 - accuracy: 0.8882 - val_loss: 0.3249 - val_accuracy: 0.8838 Epoch 15/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2928 - accuracy: 0.8911 - val_loss: 0.3237 - val_accuracy: 0.8827 Epoch 16/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2891 - accuracy: 0.8911 - val_loss: 0.3216 - val_accuracy: 0.8839 Epoch 17/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2854 - accuracy: 0.8918 - val_loss: 0.3301 - val_accuracy: 0.8844 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2834 - accuracy: 0.8942 - val_loss: 0.3315 - val_accuracy: 0.8833 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2776 - accuracy: 0.8959 - val_loss: 0.3381 - val_accuracy: 0.8790 Epoch 20/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2758 - accuracy: 0.8965 - val_loss: 0.3273 - val_accuracy: 0.8830 &lt;keras.callbacks.History at 0x7f916df86590&gt; 12model = keras.models.load_model('best-model.h5')model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3206 - accuracy: 0.8842 [0.32058343291282654, 0.8842499852180481] early stopping : to stop training before overfitting begins 12345678910model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) 1print(early_stopping_cb.stopped_epoch) 5 123456plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show() It stopped early in 5 epoch, and the issue of overfitting was solved. Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/05/Python/ML/ML_ch_7_3/"},{"title":"ML Practice 7_1","text":"Fashion MNISTDeep Learning Library tensorflow : https://www.tensorflow.org/ pytorch : https://pytorch.org/ 12import tensorflowprint(tensorflow.__version__) 2.8.0 Load Data12from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() 60,000 images, which is 28 * 28 size 12print(train_input.shape, train_target.shape)print(test_input.shape, test_target.shape) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) image visualization 123456import matplotlib.pyplot as pltfig, axs = plt.subplots(1, 10, figsize=(10, 10))for i in range(10): axs[i].imshow(train_input[i], cmap=&quot;gray_r&quot;) axs[i].axis('off')plt.show() list of target values 1print([train_target[i] for i in range(10)]) [9, 0, 0, 3, 0, 2, 7, 2, 5, 5] real target values 6,000 images per label. 12import numpy as npprint(np.unique(train_target, return_counts=True)) (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) Classify by Logistic Regression123train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)print(train_scaled.shape) (60000, 784) 1234567from sklearn.model_selection import cross_validatefrom sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss='log', max_iter=10, random_state=42)scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)print(np.mean(scores['test_score'])) # 0.82 0.8243124999999999 Is it reasonable to apply a linear or nonlinear model to unstructured data? : No One alternative is artificial neural networks. Is it reasonable to apply artificial neural networks and deep learning models to structured data? : No Classify by Artificial Neural Network1234from sklearn.model_selection import train_test_splittrain_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) 12print(train_scaled.shape, train_target.shape)print(val_scaled.shape, val_target.shape) (48000, 784) (48000,) (12000, 784) (12000,) A dense connection is called a fully connected layer. Specify activation functions to be applied to neuronal output binary classification : Sigmoid function multi classification : Softmax function specifying the type of loss function binary classification : binary_crossentropy multi classification : catogorical_crossentropy The integer target value should be one-hot encoded as 0, 1, 2, etc.but it can distort the operation of the artificial neural network. In tensorflow, by using sparse_categorical_crossentropy as a loss function, an integer target value can be used as it is. 123dense = keras.layers.Dense(10, activation = &quot;softmax&quot;, input_shape=(784, ))model = keras.Sequential(dense)model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy') 1print(train_target[:10]) [7 3 5 8 6 9 3 3 9 9] 1model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.6125 - accuracy: 0.7900 Epoch 2/5 1500/1500 [==============================] - 2s 2ms/step - loss: 0.4797 - accuracy: 0.8402 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4562 - accuracy: 0.8479 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4457 - accuracy: 0.8524 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4365 - accuracy: 0.8549 &lt;keras.callbacks.History at 0x7efd2ea9ded0&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 1ms/step - loss: 0.4553 - accuracy: 0.8475 [0.45534512400627136, 0.8475000262260437] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/04/Python/ML/ML_ch_7_1/"},{"title":"ML Practice 7_2","text":"Prepare Dataset123from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() 1234567from sklearn.model_selection import train_test_splittrain_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) DNN Layer12345# hidden layer dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))# output layer dense2 = keras.layers.Dense(10, activation='softmax') 12model = keras.Sequential([dense1, dense2])model.summary() Model: &quot;sequential_3&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_9 (Dense) (None, 100) 78500 dense_10 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ Another way to add layers123456model = keras.Sequential([ keras.layers.Dense(12, activation='sigmoid', input_shape=(16,), name='hidden'), keras.layers.Dense(10, activation='softmax', name='hidden_2'), keras.layers.Dense(1, activation='softmax', name='output')], name='fashion MNIST')model.summary() Model: &quot;fashion MNIST&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= hidden (Dense) (None, 12) 204 hidden_2 (Dense) (None, 10) 130 output (Dense) (None, 1) 11 ================================================================= Total params: 345 Trainable params: 345 Non-trainable params: 0 _________________________________________________________________ 1234model = keras.Sequential()model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))model.add(keras.layers.Dense(10, activation='softmax'))model.summary() Model: &quot;sequential_4&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_11 (Dense) (None, 100) 78500 dense_12 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 12model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5627 - accuracy: 0.8077 Epoch 2/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.4080 - accuracy: 0.8529 Epoch 3/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3740 - accuracy: 0.8660 Epoch 4/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3508 - accuracy: 0.8720 Epoch 5/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3345 - accuracy: 0.8810 &lt;keras.callbacks.History at 0x7fcf5e9c8810&gt; Relu function12345model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))model.summary() Model: &quot;sequential_6&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 784) 0 dense_16 (Dense) (None, 100) 78500 dense_17 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 1model.summary() Model: &quot;sequential_6&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 784) 0 dense_16 (Dense) (None, 100) 78500 dense_17 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 123456(train_input, train_target), (test_input, test_target) =\\ keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_traget = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) 12model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5362 - accuracy: 0.8096 Epoch 2/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3953 - accuracy: 0.8578 Epoch 3/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3570 - accuracy: 0.8722 Epoch 4/5 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3360 - accuracy: 0.8808 Epoch 5/5 1500/1500 [==============================] - 6s 4ms/step - loss: 0.3200 - accuracy: 0.8871 &lt;keras.callbacks.History at 0x7fcf5e813250&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3565 - accuracy: 0.8775 [0.35651248693466187, 0.8774999976158142] Optimizer a variety of gradient descent algorithms provided by Keras Optimizer have to consider both step direction and width direction : GD, SGD, Momentum, NAG width : GD, SGD, Adagrad, RMSProp direction &amp; width : Adam (generally, the best performance) SGDLearning rate: default 0.011234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax')) 123model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.8096 - accuracy: 0.7362 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.5421 - accuracy: 0.8163 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4886 - accuracy: 0.8329 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4604 - accuracy: 0.8426 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4404 - accuracy: 0.8490 &lt;keras.callbacks.History at 0x7fcf5e524250&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 1ms/step - loss: 0.4474 - accuracy: 0.8464 [0.44738978147506714, 0.8464166522026062] Learning rate: 0.11234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax')) 1234sgd = keras.optimizers.SGD(learning_rate=0.1)model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.5663 - accuracy: 0.7985 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4148 - accuracy: 0.8493 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3765 - accuracy: 0.8620 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3503 - accuracy: 0.8707 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3315 - accuracy: 0.8777 &lt;keras.callbacks.History at 0x7fcf5e614e90&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3469 - accuracy: 0.8744 [0.3468727171421051, 0.8744166493415833] Nesterov momentum1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax')) 1234sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.5365 - accuracy: 0.8099 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4051 - accuracy: 0.8562 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3659 - accuracy: 0.8690 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3448 - accuracy: 0.8737 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3255 - accuracy: 0.8802 &lt;keras.callbacks.History at 0x7fcf5e1de1d0&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3611 - accuracy: 0.8718 [0.36112430691719055, 0.871833324432373] Adagrad1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax')) 1234adagrad = keras.optimizers.Adagrad()model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 2ms/step - loss: 1.1751 - accuracy: 0.6441 Epoch 2/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.7733 - accuracy: 0.7556 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.6848 - accuracy: 0.7837 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.6372 - accuracy: 0.7972 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.6071 - accuracy: 0.8053 &lt;keras.callbacks.History at 0x7fcf5e480390&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 1ms/step - loss: 0.6081 - accuracy: 0.8025 [0.6081421375274658, 0.8025000095367432] RMSprop1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax')) 1234rmsprop = keras.optimizers.RMSprop()model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5261 - accuracy: 0.8139 Epoch 2/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3931 - accuracy: 0.8598 Epoch 3/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3556 - accuracy: 0.8733 Epoch 4/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3349 - accuracy: 0.8806 Epoch 5/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3173 - accuracy: 0.8869 &lt;keras.callbacks.History at 0x7fcf5e374710&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3817 - accuracy: 0.8742 [0.3816753029823303, 0.8741666674613953] Adam1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax')) 123model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.5273 - accuracy: 0.8153 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3943 - accuracy: 0.8584 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3522 - accuracy: 0.8727 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3264 - accuracy: 0.8815 Epoch 5/5 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3074 - accuracy: 0.8882 &lt;keras.callbacks.History at 0x7fcf5e47b050&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3356 - accuracy: 0.8788 [0.33555400371551514, 0.8788333535194397] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/05/Python/ML/ML_ch_7_2/"},{"title":"ML Practice 8_1","text":"CNN(Convolution Neural Network) Neural network operations can also be applied to two-dimensional arrays by CNN. Neuron in CNN is called filter or kernel. 12from tensorflow import keraskeras.layers.Conv2D(10, kernel_size=(3,3), activation=&quot;relu&quot;) &lt;keras.layers.convolutional.Conv2D at 0x7effd27dea10&gt; Padding &amp; StridePadding : Filling the border of the input array with virtual elements To prevent the loss of the original features of the image even if you resize the array, Same padding : Padding to zero around the input to make the input and feature map the same size Valid padding : Convolution only in a pure input array without padding Stride : Size of the filter moving over the input layer (default 1)1keras.layers.Conv2D(10, kernel_size=(3,3), activation='relu', padding='same', strides=1) &lt;keras.layers.convolutional.Conv2D at 0x7effceb4fb10&gt; Pooling Reducing the size of the feature map while maintaining the original features of the image Max pooling, Average pooling, etc 1keras.layers.MaxPooling2D(2, strides=2, padding=&quot;valid&quot;) &lt;keras.layers.pooling.MaxPooling2D at 0x7effce850fd0&gt; 1keras.layers.AveragePooling2D(2, strides=2, padding=&quot;valid&quot;) &lt;keras.layers.pooling.AveragePooling2D at 0x7effcea305d0&gt; Overall process in CNN Input Image Data CNN Layer kernel_size, padding, stride activation function Calculate each feature map Pooling Layer Maxpooling / Averagepooling final feature map Repeat the above process Fully Connected Layer Calculate classification predictions (Softmax) Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/06/Python/ML/ML_ch_8_1/"},{"title":"ML Practice 8_2","text":"Prepare Fashion Mnist Data12345678from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) =\\ keras.datasets.fashion_mnist.load_data()train_scaled = train_input.reshape(-1, 28, 28, 1)/255.0 # standardization (0~255 -&gt; 0~1)train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step 26435584/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step Create CNN Model1234567891011121314model = keras.Sequential()model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1))) # convolution layer (32 kernels of 3*3 size)model.add(keras.layers.MaxPooling2D(2)) # pooling layer (max pooling of 2*2 size)model.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')) # convolution layer (64 kernels of 3*3 size)model.add(keras.layers.MaxPooling2D(2)) # pooling layer (max pooling of 2*2 size)model.add(keras.layers.Flatten()) # the two-dimensional input array into one dimensionmodel.add(keras.layers.Dense(100, activation='relu')) # hidden layermodel.add(keras.layers.Dropout(0.4)) # drop out 40%model.add(keras.layers.Dense(10, activation='softmax')) # output layermodel.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 32) 320 max_pooling2d (MaxPooling2D (None, 14, 14, 32) 0 ) conv2d_1 (Conv2D) (None, 14, 14, 64) 18496 max_pooling2d_1 (MaxPooling (None, 7, 7, 64) 0 2D) flatten (Flatten) (None, 3136) 0 dense (Dense) (None, 100) 313700 dropout (Dropout) (None, 100) 0 dense_1 (Dense) (None, 10) 1010 ================================================================= Total params: 333,526 Trainable params: 333,526 Non-trainable params: 0 _________________________________________________________________ 1keras.utils.plot_model(model) 1keras.utils.plot_model(model, show_shapes=True) Compile and Fit123456789101112# optimizer: adammodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')# callback and early stoppingcheckpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/20 1500/1500 [==============================] - 65s 43ms/step - loss: 0.5331 - accuracy: 0.8090 - val_loss: 0.3290 - val_accuracy: 0.8807 Epoch 2/20 1500/1500 [==============================] - 69s 46ms/step - loss: 0.3533 - accuracy: 0.8729 - val_loss: 0.2771 - val_accuracy: 0.8973 Epoch 3/20 1500/1500 [==============================] - 64s 43ms/step - loss: 0.3032 - accuracy: 0.8901 - val_loss: 0.2525 - val_accuracy: 0.9049 Epoch 4/20 1500/1500 [==============================] - 64s 43ms/step - loss: 0.2691 - accuracy: 0.9030 - val_loss: 0.2521 - val_accuracy: 0.9081 Epoch 5/20 1500/1500 [==============================] - 63s 42ms/step - loss: 0.2445 - accuracy: 0.9096 - val_loss: 0.2351 - val_accuracy: 0.9107 Epoch 6/20 1500/1500 [==============================] - 66s 44ms/step - loss: 0.2262 - accuracy: 0.9150 - val_loss: 0.2259 - val_accuracy: 0.9162 Epoch 7/20 1500/1500 [==============================] - 65s 43ms/step - loss: 0.2075 - accuracy: 0.9229 - val_loss: 0.2284 - val_accuracy: 0.9153 Epoch 8/20 1500/1500 [==============================] - 63s 42ms/step - loss: 0.1921 - accuracy: 0.9276 - val_loss: 0.2376 - val_accuracy: 0.9173 Validation and Predict loss function (visualize train loss and validation loss) 12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(history.history['loss'])ax.plot(history.history['val_loss'])ax.set_xlabel('epoch')ax.set_ylabel('loss')ax.legend(['train', 'val'])plt.show() The epoch 6 appears to be optimal. 1model.evaluate(val_scaled, val_target) # same as output of epoch 6 375/375 [==============================] - 5s 12ms/step - loss: 0.2259 - accuracy: 0.9162 [0.22585801780223846, 0.9161666631698608] Confirm prediction of the first sample 123fig, ax = plt.subplots()ax.imshow(val_scaled[0].reshape(28,28), cmap='gray_r')plt.show() 1234# The fit(), predict(), and evaluate() all expect the first dimension of the input to be the batch dimension.# Slicing, unlike indexing, maintains the entire dimension even if there is one element.preds = model.predict(val_scaled[0:1])print(preds) [[1.88540562e-13 1.07813886e-17 3.65283819e-17 1.44768129e-14 7.86518068e-17 2.58884016e-15 2.82545543e-15 2.49630367e-13 1.00000000e+00 1.07822686e-14]] 12345fig, ax = plt.subplots()ax.bar(range(1,11),preds[0])ax.set_xlabel('class')ax.set_ylabel('prob.')plt.show() 12345classes = ['t-shirt', 'pants', 'sweater', 'dress', 'coat', 'sandal', 'shirt', 'sneakers', 'bag', 'boots']import numpy as npprint(classes[np.argmax(preds)]) bag Model Test12test_scaled = test_input.reshape(-1,28,28,1)/255.0model.evaluate(test_scaled, test_target) 313/313 [==============================] - 4s 13ms/step - loss: 0.2519 - accuracy: 0.9093 [0.25190481543540955, 0.9093000292778015] GPU check in Google Colab12345678'''import tensorflow as tfdevice_name = tf.test.gpu_device_name()if device_name != '/device:GPU:0': raise SystemError('GPU device not found')print('Found GPU at: {}'.format(device_name))''' 123456789101112131415'''import tensorflow as tfmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)with tf.device('/device:GPU:0'): history = model.fit(train_scaled, train_target, epochs=10, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb])''' Visualize CNN12model = keras.models.load_model('best-cnn-model.h5')model.layers [&lt;keras.layers.convolutional.Conv2D at 0x7f1fc8314c10&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f1fc82d34d0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7f1fc82d3490&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f1fc8393f10&gt;, &lt;keras.layers.core.flatten.Flatten at 0x7f1fc8332050&gt;, &lt;keras.layers.core.dense.Dense at 0x7f1fc83954d0&gt;, &lt;keras.layers.core.dropout.Dropout at 0x7f1fc8395110&gt;, &lt;keras.layers.core.dense.Dense at 0x7f1fc83f0710&gt;] check weights of the first convolution layer 12conv = model.layers[0]print(conv.weights[0].shape, conv.weights[1].shape) (3, 3, 1, 32) (32,) 12conv_weights = conv.weights[0].numpy()print(conv_weights.mean(), conv_weights.std()) -0.010425919 0.21259554 12345fig, ax = plt.subplots()ax.hist(conv_weights.reshape(-1,1))ax.set_xlabel('weight')ax.set_ylabel('count')plt.show() 1234567# 32 kernels of 3*3 sizefig, ax = plt.subplots(2, 16, figsize=(15,2))for i in range(2): for j in range(16): ax[i,j].imshow(conv_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) ax[i,j].axis('off')plt.show() Compare to empty CNN which is untrained 12345no_training_model = keras.Sequential()no_training_model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)))no_training_conv = no_training_model.layers[0]print(no_training_conv.weights[0].shape) (3, 3, 1, 32) 12no_training_weights = no_training_conv.weights[0].numpy()print(no_training_weights.mean(), no_training_weights.std()) -0.008161874 0.08076286 12345fig, ax = plt.subplots()ax.hist(no_training_weights.reshape(-1,1))ax.set_xlabel('weight')ax.set_ylabel('count')plt.show() It shows a relatively even distributionbecause tensorflow randomly select a value from an equal distribution at first. 123456fig, ax = plt.subplots(2, 16, figsize=(15,2))for i in range(2): for j in range(16): ax[i,j].imshow(no_training_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) ax[i,j].axis('off')plt.show() Visualize Feature Map feature map of the first convolution layer 12345print(model.input)conv_acti = keras.Model(model.input, model.layers[0].output) # functional API(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()plt.imshow(train_input[0], cmap='gray_r')plt.show() KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input'), name='conv2d_input', description=&quot;created by layer 'conv2d_input'&quot;) 123inputs = train_input[0:1].reshape(-1, 28, 28, 1)/255.0feature_maps = conv_acti.predict(inputs)print(feature_maps.shape) (1, 28, 28, 32) 123456fig, ax = plt.subplots(4, 8, figsize=(15,8))for i in range(4): for j in range(8): ax[i,j].imshow(feature_maps[0,:,:,i*8+j]) ax[i,j].axis('off')plt.show() feature map of the second convolution layer 123456789101112conv2_acti = keras.Model(model.input, model.layers[2].output)feature_maps = conv2_acti.predict(train_input[0:1].reshape(-1, 28, 28, 1)/255.0)print(feature_maps.shape)fig, axs = plt.subplots(8, 8, figsize=(12,12))for i in range(8): for j in range(8): axs[i, j].imshow(feature_maps[0,:,:,i*8 + j]) axs[i, j].axis('off')plt.show() (1, 14, 14, 64) Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/07/Python/ML/ML_ch_8_2/"},{"title":"ML Practice 9_2","text":"Text Normalization: Pre-processing text for use as input data Cleansing 텍스트 분석에 방해되는 불필요한 문자 및 기호를 사전에 제거 ex) HTML, XML 태그 제거 Tokenization Sentence Tokenization- 문장, 마침표, 개행문자 등 문장 마지막을 뜻하는 기호를 따라 분리- 문장이 가지는 의미가 시맨틱적으로 중요한 요소일 때 사용 Word Tokenization- 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리 Stop word elimination 필수 문법 요소이나 문맥적으로 큰 의미 없는 단어(ex. is, the, a, will)가 텍스트에 빈번하게 나타나면 중요한 단어로 인지될 수 있어서 사전 제거가 필요함 Stemming Lemmatization IMDB Review Classification with RNN a dataset that categorizes IMDB reviews as positive and negative based on comments 123from tensorflow.keras.datasets import imdb(train_input, train_target), (test_input, test_target) = imdb.load_data( num_words=500) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz 17465344/17464789 [==============================] - 0s 0us/step 17473536/17464789 [==============================] - 0s 0us/step Datasets are made of a one-dimensional array, because the length of the text is different 1print(train_input.shape, test_input.shape) (25000,) (25000,) 123print(len(train_input[0]))print(len(train_input[1]))print(len(train_input[2])) 218 189 141 1print(train_input[0]) [1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32] Target 0: negative review Target 1: positive review 1print(train_target[:20]) [1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1] Split data12345from sklearn.model_selection import train_test_splittrain_input, val_input, train_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42)train_input.shape, val_input.shape, train_target.shape, val_target.shape ((20000,), (5000,), (20000,), (5000,)) Visualize data mean and median of the number of words in each review 123import numpy as nplengths = np.array([len(x) for x in train_input])print(np.mean(lengths), np.median(lengths)) 239.00925 178.0 123456import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.hist(lengths)ax.set_xlabel(&quot;length&quot;)ax.set_ylabel(&quot;frequency&quot;)plt.show() Use only 100 words that are much shorter than the median Use padding to match the length of each review to 100 1234from tensorflow.keras.preprocessing.sequence import pad_sequencestrain_seq = pad_sequences(train_input, maxlen=100) # cut the front part of sequencesprint(train_seq.shape) # the number of data = 2000, length of each data = 100 (20000, 100) 1print(train_seq[0]) [ 10 4 20 9 2 364 352 5 45 6 2 2 33 269 8 2 142 2 5 2 17 73 17 204 5 2 19 55 2 2 92 66 104 14 20 93 76 2 151 33 4 58 12 188 2 151 12 215 69 224 142 73 237 6 2 7 2 2 188 2 103 14 31 10 10 451 7 2 5 2 80 91 2 30 2 34 14 20 151 50 26 131 49 2 84 46 50 37 80 79 6 2 46 7 14 20 10 10 470 158] 1print(train_input[0][-10:]) [6, 2, 46, 7, 14, 20, 10, 10, 470, 158] 1print(train_seq[5]) [ 0 0 0 0 1 2 195 19 49 2 2 190 4 2 352 2 183 10 10 13 82 79 4 2 36 71 269 8 2 25 19 49 7 4 2 2 2 2 2 10 10 48 25 40 2 11 2 2 40 2 2 5 4 2 2 95 14 238 56 129 2 10 10 21 2 94 364 352 2 2 11 190 24 484 2 7 94 205 405 10 10 87 2 34 49 2 7 2 2 2 2 2 290 2 46 48 64 18 4 2] 1val_seq = pad_sequences(val_input, maxlen=100) RNN Model 100 : Length of each text data 500 : Numer of words 1234from tensorflow import kerasmodel = keras.Sequential()model.add(keras.layers.SimpleRNN(8, input_shape=(100,500)))model.add(keras.layers.Dense(1, activation='sigmoid')) one-hot encoding 12train_oh = keras.utils.to_categorical(train_seq)print(train_oh.shape) (20000, 100, 500) 1print(train_oh[0][0][:12]) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] 1print(np.sum(train_oh[0][0])) 1.0 12val_oh = keras.utils.to_categorical(val_seq)print(val_oh.shape) (5000, 100, 500) model structure 1model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn (SimpleRNN) (None, 8) 4072 dense (Dense) (None, 1) 9 ================================================================= Total params: 4,081 Trainable params: 4,081 Non-trainable params: 0 _________________________________________________________________ model fitting 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model.fit(train_oh, train_target, epochs=100, batch_size=64, validation_data=(val_oh, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 47s 135ms/step - loss: 0.6977 - accuracy: 0.5094 - val_loss: 0.6916 - val_accuracy: 0.5338 Epoch 2/100 313/313 [==============================] - 41s 131ms/step - loss: 0.6828 - accuracy: 0.5623 - val_loss: 0.6780 - val_accuracy: 0.5792 Epoch 3/100 313/313 [==============================] - 72s 230ms/step - loss: 0.6685 - accuracy: 0.6031 - val_loss: 0.6649 - val_accuracy: 0.6114 Epoch 4/100 313/313 [==============================] - 57s 183ms/step - loss: 0.6525 - accuracy: 0.6366 - val_loss: 0.6478 - val_accuracy: 0.6438 Epoch 5/100 313/313 [==============================] - 41s 131ms/step - loss: 0.6280 - accuracy: 0.6789 - val_loss: 0.6241 - val_accuracy: 0.6850 Epoch 6/100 313/313 [==============================] - 41s 131ms/step - loss: 0.6086 - accuracy: 0.7050 - val_loss: 0.6063 - val_accuracy: 0.7042 Epoch 7/100 313/313 [==============================] - 42s 134ms/step - loss: 0.5897 - accuracy: 0.7224 - val_loss: 0.5904 - val_accuracy: 0.7158 Epoch 8/100 313/313 [==============================] - 41s 131ms/step - loss: 0.5732 - accuracy: 0.7354 - val_loss: 0.5774 - val_accuracy: 0.7188 Epoch 9/100 313/313 [==============================] - 54s 174ms/step - loss: 0.5576 - accuracy: 0.7492 - val_loss: 0.5626 - val_accuracy: 0.7364 Epoch 10/100 313/313 [==============================] - 41s 132ms/step - loss: 0.5432 - accuracy: 0.7583 - val_loss: 0.5500 - val_accuracy: 0.7444 Epoch 11/100 313/313 [==============================] - 43s 137ms/step - loss: 0.5301 - accuracy: 0.7650 - val_loss: 0.5391 - val_accuracy: 0.7480 Epoch 12/100 313/313 [==============================] - 51s 164ms/step - loss: 0.5188 - accuracy: 0.7713 - val_loss: 0.5304 - val_accuracy: 0.7608 Epoch 13/100 313/313 [==============================] - 40s 129ms/step - loss: 0.5078 - accuracy: 0.7771 - val_loss: 0.5276 - val_accuracy: 0.7526 Epoch 14/100 313/313 [==============================] - 42s 133ms/step - loss: 0.4980 - accuracy: 0.7824 - val_loss: 0.5108 - val_accuracy: 0.7698 Epoch 15/100 313/313 [==============================] - 45s 143ms/step - loss: 0.4889 - accuracy: 0.7865 - val_loss: 0.5043 - val_accuracy: 0.7708 Epoch 16/100 313/313 [==============================] - 42s 135ms/step - loss: 0.4807 - accuracy: 0.7901 - val_loss: 0.4944 - val_accuracy: 0.7752 Epoch 17/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4730 - accuracy: 0.7957 - val_loss: 0.4903 - val_accuracy: 0.7758 Epoch 18/100 313/313 [==============================] - 42s 135ms/step - loss: 0.4661 - accuracy: 0.7979 - val_loss: 0.4878 - val_accuracy: 0.7744 Epoch 19/100 313/313 [==============================] - 40s 129ms/step - loss: 0.4602 - accuracy: 0.7994 - val_loss: 0.4813 - val_accuracy: 0.7808 Epoch 20/100 313/313 [==============================] - 40s 127ms/step - loss: 0.4543 - accuracy: 0.8031 - val_loss: 0.4756 - val_accuracy: 0.7804 Epoch 21/100 313/313 [==============================] - 40s 128ms/step - loss: 0.4492 - accuracy: 0.8030 - val_loss: 0.4719 - val_accuracy: 0.7816 Epoch 22/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4448 - accuracy: 0.8059 - val_loss: 0.4724 - val_accuracy: 0.7800 Epoch 23/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4412 - accuracy: 0.8077 - val_loss: 0.4671 - val_accuracy: 0.7852 Epoch 24/100 313/313 [==============================] - 42s 134ms/step - loss: 0.4374 - accuracy: 0.8097 - val_loss: 0.4642 - val_accuracy: 0.7894 Epoch 25/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4338 - accuracy: 0.8113 - val_loss: 0.4635 - val_accuracy: 0.7862 Epoch 26/100 313/313 [==============================] - 40s 127ms/step - loss: 0.4304 - accuracy: 0.8121 - val_loss: 0.4596 - val_accuracy: 0.7900 Epoch 27/100 313/313 [==============================] - 40s 127ms/step - loss: 0.4278 - accuracy: 0.8130 - val_loss: 0.4588 - val_accuracy: 0.7928 Epoch 28/100 313/313 [==============================] - 40s 128ms/step - loss: 0.4255 - accuracy: 0.8135 - val_loss: 0.4585 - val_accuracy: 0.7930 Epoch 29/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4229 - accuracy: 0.8149 - val_loss: 0.4619 - val_accuracy: 0.7888 Epoch 30/100 313/313 [==============================] - 41s 130ms/step - loss: 0.4205 - accuracy: 0.8156 - val_loss: 0.4560 - val_accuracy: 0.7950 Epoch 31/100 313/313 [==============================] - 41s 130ms/step - loss: 0.4182 - accuracy: 0.8163 - val_loss: 0.4575 - val_accuracy: 0.7922 Epoch 32/100 313/313 [==============================] - 40s 128ms/step - loss: 0.4168 - accuracy: 0.8195 - val_loss: 0.4564 - val_accuracy: 0.7910 Epoch 33/100 313/313 [==============================] - 42s 133ms/step - loss: 0.4143 - accuracy: 0.8180 - val_loss: 0.4573 - val_accuracy: 0.7872 1234567fig, ax = plt.subplots()ax.plot(history.history['loss'])ax.plot(history.history['val_loss'])ax.set_xlabel('epoch')ax.set_ylabel('loss')ax.legend(['train', 'val'])plt.show() 1print(train_seq.nbytes, train_oh.nbytes) 8000000 4000000000 Word Embedding: Replace each word with a real number vector of fixed size. It solves memory inefficiencies in one-hot encoding. Since it receives an integer data as an input, train_seq can be used. 1234model2 = keras.Sequential()model2.add(keras.layers.Embedding(500, 16, input_length=100))model2.add(keras.layers.SimpleRNN(8, input_shape=(100,500)))model2.add(keras.layers.Dense(1, activation='sigmoid')) 1model2.summary() Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 16) 8000 simple_rnn_1 (SimpleRNN) (None, 8) 200 dense_1 (Dense) (None, 1) 9 ================================================================= Total params: 8,209 Trainable params: 8,209 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 52s 162ms/step - loss: 0.6938 - accuracy: 0.5105 - val_loss: 0.6909 - val_accuracy: 0.5236 Epoch 2/100 313/313 [==============================] - 51s 162ms/step - loss: 0.6747 - accuracy: 0.5947 - val_loss: 0.6529 - val_accuracy: 0.6548 Epoch 3/100 313/313 [==============================] - 53s 169ms/step - loss: 0.6280 - accuracy: 0.6870 - val_loss: 0.6142 - val_accuracy: 0.6996 Epoch 4/100 313/313 [==============================] - 52s 166ms/step - loss: 0.5937 - accuracy: 0.7228 - val_loss: 0.5887 - val_accuracy: 0.7214 Epoch 5/100 313/313 [==============================] - 52s 165ms/step - loss: 0.5699 - accuracy: 0.7440 - val_loss: 0.5712 - val_accuracy: 0.7298 Epoch 6/100 313/313 [==============================] - 51s 163ms/step - loss: 0.5495 - accuracy: 0.7571 - val_loss: 0.5569 - val_accuracy: 0.7476 Epoch 7/100 313/313 [==============================] - 51s 164ms/step - loss: 0.5361 - accuracy: 0.7618 - val_loss: 0.5451 - val_accuracy: 0.7444 Epoch 8/100 313/313 [==============================] - 52s 166ms/step - loss: 0.5230 - accuracy: 0.7689 - val_loss: 0.5360 - val_accuracy: 0.7514 Epoch 9/100 313/313 [==============================] - 53s 168ms/step - loss: 0.5127 - accuracy: 0.7724 - val_loss: 0.5302 - val_accuracy: 0.7510 Epoch 10/100 313/313 [==============================] - 52s 165ms/step - loss: 0.5043 - accuracy: 0.7758 - val_loss: 0.5213 - val_accuracy: 0.7554 Epoch 11/100 313/313 [==============================] - 52s 167ms/step - loss: 0.4956 - accuracy: 0.7806 - val_loss: 0.5188 - val_accuracy: 0.7544 Epoch 12/100 313/313 [==============================] - 53s 170ms/step - loss: 0.4899 - accuracy: 0.7823 - val_loss: 0.5170 - val_accuracy: 0.7604 Epoch 13/100 313/313 [==============================] - 53s 170ms/step - loss: 0.4858 - accuracy: 0.7839 - val_loss: 0.5110 - val_accuracy: 0.7604 Epoch 14/100 313/313 [==============================] - 52s 166ms/step - loss: 0.4809 - accuracy: 0.7873 - val_loss: 0.5086 - val_accuracy: 0.7626 Epoch 15/100 313/313 [==============================] - 53s 168ms/step - loss: 0.4763 - accuracy: 0.7897 - val_loss: 0.5061 - val_accuracy: 0.7658 Epoch 16/100 313/313 [==============================] - 53s 169ms/step - loss: 0.4733 - accuracy: 0.7912 - val_loss: 0.5111 - val_accuracy: 0.7576 Epoch 17/100 313/313 [==============================] - 54s 172ms/step - loss: 0.4692 - accuracy: 0.7901 - val_loss: 0.5097 - val_accuracy: 0.7556 Epoch 18/100 313/313 [==============================] - 54s 173ms/step - loss: 0.4676 - accuracy: 0.7922 - val_loss: 0.4955 - val_accuracy: 0.7692 Epoch 19/100 313/313 [==============================] - 54s 171ms/step - loss: 0.4653 - accuracy: 0.7943 - val_loss: 0.4997 - val_accuracy: 0.7694 Epoch 20/100 313/313 [==============================] - 54s 172ms/step - loss: 0.4656 - accuracy: 0.7929 - val_loss: 0.4959 - val_accuracy: 0.7700 Epoch 21/100 313/313 [==============================] - 54s 172ms/step - loss: 0.4618 - accuracy: 0.7964 - val_loss: 0.4962 - val_accuracy: 0.7676 1234567fig, ax = plt.subplots()ax.plot(history.history['loss'])ax.plot(history.history['val_loss'])ax.set_xlabel('epoch')ax.set_ylabel('loss')ax.legend(['train', 'val'])plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/08/Python/ML/ML_ch_9_2/"},{"title":"ML Practice Tree plot Example","text":"Goal : To change the color of tree plot1!pip install -U matplotlib Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.3) Collecting matplotlib Downloading matplotlib-3.5.1-cp39-cp39-win_amd64.whl (7.2 MB) Requirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1) Requirement already satisfied: numpy&gt;=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3) Requirement already satisfied: pillow&gt;=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0) Requirement already satisfied: cycler&gt;=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0) Requirement already satisfied: fonttools&gt;=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0) Requirement already satisfied: pyparsing&gt;=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4) Requirement already satisfied: python-dateutil&gt;=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2) Requirement already satisfied: packaging&gt;=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (21.0) Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler&gt;=0.10-&gt;matplotlib) (1.16.0) Installing collected packages: matplotlib Attempting uninstall: matplotlib Found existing installation: matplotlib 3.4.3 Uninstalling matplotlib-3.4.3: ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\__pycache__\\\\pylab.cpython-39.pyc' Consider using the `--user` option or check the permissions. Stackflow Ex.1234567891011121314151617181920212223242526from matplotlib import pyplot as pltfrom matplotlib.colors import ListedColormap, to_rgbimport numpy as npfrom sklearn import treeX = np.random.rand(50, 2) * np.r_[100, 50]y = X[:, 0] - X[:, 1] &gt; 20clf = tree.DecisionTreeClassifier(random_state=2021)clf = clf.fit(X, y)fig, ax = plt.subplots(figsize=(15, 10))colors = ['crimson', 'dodgerblue']artists = tree.plot_tree(clf, feature_names=[&quot;X&quot;, &quot;y&quot;], class_names=colors, filled=True, rounded=True)for artist, impurity, value in zip(artists, clf.tree_.impurity, clf.tree_.value): # let the max value decide the color; whiten the color depending on impurity (gini) r, g, b = to_rgb(colors[np.argmax(value)]) f = impurity * 2 # for N colors: f = impurity * N/(N-1) if N&gt;1 else 0 artist.get_bbox_patch().set_facecolor((f + (1-f)*r, f + (1-f)*g, f + (1-f)*b)) artist.get_bbox_patch().set_edgecolor('black')plt.tight_layout()plt.show() Iris Ex.Tree plot1234567891011121314151617181920212223%matplotlib inline import sklearnprint(sklearn.__version__)import matplotlibprint(matplotlib.__version__)from sklearn.datasets import load_irisfrom sklearn import tree import matplotlib.pyplot as pltiris = load_iris()print(iris.data.shape, iris.target.shape)print(&quot;feature names&quot;, iris.feature_names)print(&quot;class names&quot;, iris.target_names)dt = tree.DecisionTreeClassifier(random_state=0)dt.fit(iris.data, iris.target)fig, ax = plt.subplots(figsize=(18, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)plt.show() 0.24.2 3.4.3 (150, 4) (150,) feature names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] class names ['setosa' 'versicolor' 'virginica'] matplotlib.text.Annotation123456789%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)for i in range(0, len(ax)): print(type(ax[i])) &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; &lt;class 'matplotlib.text.Annotation'&gt; get_bbox_patch() method 123456789%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)for i in range(0, len(ax)): print(ax[i].get_bbox_patch()) # get patch properties (facecolor, edgewidth,,,) FancyBboxPatch((0, 0), width=120.875, height=56.4) FancyBboxPatch((0, 0), width=87.875, height=44.8) FancyBboxPatch((0, 0), width=127.25, height=56.4) FancyBboxPatch((0, 0), width=131.625, height=56.4) FancyBboxPatch((0, 0), width=30, height=33.2) FancyBboxPatch((0, 0), width=30, height=33.2) FancyBboxPatch((0, 0), width=131.625, height=56.4) FancyBboxPatch((0, 0), width=30, height=33.2) FancyBboxPatch((0, 0), width=30, height=33.2) set_boxstyle() 12345678910111213%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)for i in range(0, len(ax)): # set patch properties if i % 2 == 0: ax[i].get_bbox_patch().set_boxstyle(&quot;Rarrow&quot;, pad=0.3) else: ax[i].get_bbox_patch().set_boxstyle(&quot;Round&quot;, pad=0.3) Final ex.1234567import numpy as np colors = [&quot;indigo&quot;, &quot;violet&quot;, &quot;crimson&quot;]print(colors[np.argmax([[0., 0., 50.]])])print(colors[np.argmax([[50., 0., 0.]])])print(colors[np.argmax([[0., 50., 0.]])])print(colors[np.argmax([[50., 50., 50.]])]) crimson indigo violet indigo 12345678910111213141516171819202122232425from matplotlib.colors import to_rgb%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 3, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)i = 0colors = [&quot;yellow&quot;, &quot;violet&quot;, &quot;lavenderblush&quot;]for artist, impurity, value in zip(ax, dt.tree_.impurity, dt.tree_.value): r, g, b = to_rgb(colors[np.argmax(value)]) # 코드가 길어서 i로 재 저장 ip = impurity # print(ip + (1-ip)*r, ip + (1-ip)*g, ip + (1-ip)*b) if i % 2 == 0: # set_boxtyle 적용 ax[i].get_bbox_patch().set_boxstyle(&quot;round&quot;, pad=0.3) ax[i].get_bbox_patch().set_facecolor((ip + (1-ip)*r, ip + (1-ip)*g, ip + (1-ip)*b)) ax[i].get_bbox_patch().set_edgecolor('black') else: ax[i].get_bbox_patch().set_boxstyle(&quot;circle&quot;, pad=0.3) ax[i].get_bbox_patch().set_facecolor((ip + (1-ip)*r, ip + (1-ip)*g, ip + (1-ip)*b)) ax[i].get_bbox_patch().set_edgecolor('black') i = i+1 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/03/31/Python/ML/plot_tree_ex/"},{"title":"ML Practice 9_1","text":"Sequential data meaningful in order such as text data, time series data Requires the function to remember previously entered data Text data text mining (representatively, sentimental analysis) natural language processing (using chatbot) basic deep learning algorithm : RNN, LSTM tensorflow: https://wikidocs.net/book/2155 pytorch: https://wikidocs.net/32471 RNN (Recurrent Neural Network) the contrary of feedfoward neural network Fully connected layer + Loop that circulates the processing flow of previous data timestep : a step in processing a sample activation : default tanh (S-shape, -1 to 1) 1 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/08/Python/ML/ML_ch_9_1/"},{"title":"ML Practice 9_3","text":"LSTM(Long Short-Term Memory) When the sentence is long, the learning ability of RNN is poor. LSTM is designed to keep short-term memory long. 1234567891011from tensorflow.keras.datasets import imdbfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = imdb.load_data( num_words=500)train_input, val_input, train_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42)train_input.shape, val_input.shape, train_target.shape, val_target.shape Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz 17465344/17464789 [==============================] - 0s 0us/step 17473536/17464789 [==============================] - 0s 0us/step ((20000,), (5000,), (20000,), (5000,)) 12345from tensorflow.keras.preprocessing.sequence import pad_sequencestrain_seq = pad_sequences(train_input, maxlen=100)val_seq = pad_sequences(val_input, maxlen=100)train_seq.shape, val_seq.shape ((20000, 100), (5000, 100)) 123456from tensorflow import kerasmodel = keras.Sequential()model.add(keras.layers.Embedding(500, 16, input_length=100))model.add(keras.layers.LSTM(8))model.add(keras.layers.Dense(1, activation='sigmoid'))model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 16) 8000 lstm (LSTM) (None, 8) 800 dense (Dense) (None, 1) 9 ================================================================= Total params: 8,809 Trainable params: 8,809 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 16s 42ms/step - loss: 0.6925 - accuracy: 0.5433 - val_loss: 0.6916 - val_accuracy: 0.5986 Epoch 2/100 313/313 [==============================] - 20s 65ms/step - loss: 0.6902 - accuracy: 0.6076 - val_loss: 0.6883 - val_accuracy: 0.6470 Epoch 3/100 313/313 [==============================] - 16s 50ms/step - loss: 0.6842 - accuracy: 0.6512 - val_loss: 0.6783 - val_accuracy: 0.6680 Epoch 4/100 313/313 [==============================] - 14s 46ms/step - loss: 0.6591 - accuracy: 0.6861 - val_loss: 0.6237 - val_accuracy: 0.7128 Epoch 5/100 313/313 [==============================] - 14s 45ms/step - loss: 0.5975 - accuracy: 0.7211 - val_loss: 0.5842 - val_accuracy: 0.7220 Epoch 6/100 313/313 [==============================] - 16s 51ms/step - loss: 0.5716 - accuracy: 0.7355 - val_loss: 0.5643 - val_accuracy: 0.7408 Epoch 7/100 313/313 [==============================] - 14s 45ms/step - loss: 0.5510 - accuracy: 0.7534 - val_loss: 0.5470 - val_accuracy: 0.7458 Epoch 8/100 313/313 [==============================] - 17s 54ms/step - loss: 0.5328 - accuracy: 0.7619 - val_loss: 0.5319 - val_accuracy: 0.7564 Epoch 9/100 313/313 [==============================] - 14s 44ms/step - loss: 0.5147 - accuracy: 0.7739 - val_loss: 0.5130 - val_accuracy: 0.7724 Epoch 10/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4982 - accuracy: 0.7824 - val_loss: 0.5000 - val_accuracy: 0.7746 Epoch 11/100 313/313 [==============================] - 17s 53ms/step - loss: 0.4837 - accuracy: 0.7909 - val_loss: 0.4874 - val_accuracy: 0.7794 Epoch 12/100 313/313 [==============================] - 15s 47ms/step - loss: 0.4717 - accuracy: 0.7957 - val_loss: 0.4767 - val_accuracy: 0.7868 Epoch 13/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4620 - accuracy: 0.7990 - val_loss: 0.4696 - val_accuracy: 0.7892 Epoch 14/100 313/313 [==============================] - 15s 48ms/step - loss: 0.4534 - accuracy: 0.8033 - val_loss: 0.4662 - val_accuracy: 0.7908 Epoch 15/100 313/313 [==============================] - 15s 48ms/step - loss: 0.4470 - accuracy: 0.8067 - val_loss: 0.4606 - val_accuracy: 0.7946 Epoch 16/100 313/313 [==============================] - 15s 47ms/step - loss: 0.4414 - accuracy: 0.8067 - val_loss: 0.4558 - val_accuracy: 0.7924 Epoch 17/100 313/313 [==============================] - 14s 45ms/step - loss: 0.4366 - accuracy: 0.8087 - val_loss: 0.4516 - val_accuracy: 0.7972 Epoch 18/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4329 - accuracy: 0.8098 - val_loss: 0.4485 - val_accuracy: 0.7968 Epoch 19/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4301 - accuracy: 0.8088 - val_loss: 0.4461 - val_accuracy: 0.7962 Epoch 20/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4274 - accuracy: 0.8093 - val_loss: 0.4456 - val_accuracy: 0.7988 Epoch 21/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4248 - accuracy: 0.8102 - val_loss: 0.4429 - val_accuracy: 0.7994 Epoch 22/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4225 - accuracy: 0.8106 - val_loss: 0.4481 - val_accuracy: 0.7960 Epoch 23/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4211 - accuracy: 0.8117 - val_loss: 0.4417 - val_accuracy: 0.7974 Epoch 24/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4198 - accuracy: 0.8116 - val_loss: 0.4393 - val_accuracy: 0.8010 Epoch 25/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4180 - accuracy: 0.8127 - val_loss: 0.4464 - val_accuracy: 0.7890 Epoch 26/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4173 - accuracy: 0.8130 - val_loss: 0.4400 - val_accuracy: 0.8002 Epoch 27/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4161 - accuracy: 0.8123 - val_loss: 0.4370 - val_accuracy: 0.8018 Epoch 28/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4154 - accuracy: 0.8131 - val_loss: 0.4375 - val_accuracy: 0.8010 Epoch 29/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4143 - accuracy: 0.8127 - val_loss: 0.4361 - val_accuracy: 0.8020 Epoch 30/100 313/313 [==============================] - 14s 45ms/step - loss: 0.4131 - accuracy: 0.8136 - val_loss: 0.4370 - val_accuracy: 0.8022 Epoch 31/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4129 - accuracy: 0.8134 - val_loss: 0.4360 - val_accuracy: 0.8022 Epoch 32/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4126 - accuracy: 0.8141 - val_loss: 0.4355 - val_accuracy: 0.8014 Epoch 33/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4117 - accuracy: 0.8134 - val_loss: 0.4358 - val_accuracy: 0.8040 Epoch 34/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4112 - accuracy: 0.8134 - val_loss: 0.4346 - val_accuracy: 0.7986 Epoch 35/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4106 - accuracy: 0.8134 - val_loss: 0.4353 - val_accuracy: 0.7978 Epoch 36/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4100 - accuracy: 0.8144 - val_loss: 0.4346 - val_accuracy: 0.7976 Epoch 37/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4100 - accuracy: 0.8128 - val_loss: 0.4342 - val_accuracy: 0.8052 Epoch 38/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4092 - accuracy: 0.8141 - val_loss: 0.4350 - val_accuracy: 0.8052 Epoch 39/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4090 - accuracy: 0.8138 - val_loss: 0.4334 - val_accuracy: 0.8028 Epoch 40/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4083 - accuracy: 0.8152 - val_loss: 0.4325 - val_accuracy: 0.7992 Epoch 41/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4080 - accuracy: 0.8156 - val_loss: 0.4358 - val_accuracy: 0.7960 Epoch 42/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4075 - accuracy: 0.8149 - val_loss: 0.4322 - val_accuracy: 0.8016 Epoch 43/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4069 - accuracy: 0.8134 - val_loss: 0.4327 - val_accuracy: 0.8014 Epoch 44/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4067 - accuracy: 0.8155 - val_loss: 0.4333 - val_accuracy: 0.8042 Epoch 45/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4063 - accuracy: 0.8140 - val_loss: 0.4331 - val_accuracy: 0.8022 12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(history.history['loss'])ax.plot(history.history['val_loss'])ax.set_xlabel('epoch')ax.set_ylabel('loss')ax.legend(['train', 'val'])plt.show() dropout = 0.3 12345model2 = keras.Sequential()model2.add(keras.layers.Embedding(500, 16, input_length=100))model2.add(keras.layers.LSTM(8, dropout=0.3))model2.add(keras.layers.Dense(1, activation='sigmoid'))model2.summary() Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 100, 16) 8000 lstm_1 (LSTM) (None, 8) 800 dense_1 (Dense) (None, 1) 9 ================================================================= Total params: 8,809 Trainable params: 8,809 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])checkpoint_cb = keras.callbacks.ModelCheckpoint('best-dropout-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 18s 48ms/step - loss: 0.6925 - accuracy: 0.5239 - val_loss: 0.6913 - val_accuracy: 0.5654 Epoch 2/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6895 - accuracy: 0.5954 - val_loss: 0.6872 - val_accuracy: 0.6314 Epoch 3/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6816 - accuracy: 0.6561 - val_loss: 0.6736 - val_accuracy: 0.6828 Epoch 4/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6444 - accuracy: 0.7018 - val_loss: 0.6027 - val_accuracy: 0.7116 Epoch 5/100 313/313 [==============================] - 13s 43ms/step - loss: 0.5789 - accuracy: 0.7199 - val_loss: 0.5620 - val_accuracy: 0.7352 Epoch 6/100 313/313 [==============================] - 14s 44ms/step - loss: 0.5517 - accuracy: 0.7405 - val_loss: 0.5403 - val_accuracy: 0.7526 Epoch 7/100 313/313 [==============================] - 14s 44ms/step - loss: 0.5317 - accuracy: 0.7545 - val_loss: 0.5219 - val_accuracy: 0.7598 Epoch 8/100 313/313 [==============================] - 13s 43ms/step - loss: 0.5117 - accuracy: 0.7681 - val_loss: 0.5049 - val_accuracy: 0.7702 Epoch 9/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4968 - accuracy: 0.7747 - val_loss: 0.4917 - val_accuracy: 0.7760 Epoch 10/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4844 - accuracy: 0.7829 - val_loss: 0.4814 - val_accuracy: 0.7864 Epoch 11/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4741 - accuracy: 0.7871 - val_loss: 0.4728 - val_accuracy: 0.7892 Epoch 12/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4667 - accuracy: 0.7912 - val_loss: 0.4679 - val_accuracy: 0.7870 Epoch 13/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4584 - accuracy: 0.7942 - val_loss: 0.4614 - val_accuracy: 0.7932 Epoch 14/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4530 - accuracy: 0.7968 - val_loss: 0.4590 - val_accuracy: 0.7942 Epoch 15/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4499 - accuracy: 0.7979 - val_loss: 0.4546 - val_accuracy: 0.7956 Epoch 16/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4458 - accuracy: 0.7995 - val_loss: 0.4517 - val_accuracy: 0.7988 Epoch 17/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4415 - accuracy: 0.7990 - val_loss: 0.4481 - val_accuracy: 0.7984 Epoch 18/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4374 - accuracy: 0.8018 - val_loss: 0.4468 - val_accuracy: 0.7994 Epoch 19/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4342 - accuracy: 0.8030 - val_loss: 0.4516 - val_accuracy: 0.7964 Epoch 20/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4325 - accuracy: 0.8054 - val_loss: 0.4431 - val_accuracy: 0.8024 Epoch 21/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4305 - accuracy: 0.8057 - val_loss: 0.4400 - val_accuracy: 0.7996 Epoch 22/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4279 - accuracy: 0.8037 - val_loss: 0.4388 - val_accuracy: 0.7964 Epoch 23/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4250 - accuracy: 0.8075 - val_loss: 0.4392 - val_accuracy: 0.8014 Epoch 24/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4253 - accuracy: 0.8062 - val_loss: 0.4361 - val_accuracy: 0.7966 Epoch 25/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4241 - accuracy: 0.8077 - val_loss: 0.4357 - val_accuracy: 0.8008 Epoch 26/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4219 - accuracy: 0.8077 - val_loss: 0.4342 - val_accuracy: 0.8008 Epoch 27/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4205 - accuracy: 0.8097 - val_loss: 0.4331 - val_accuracy: 0.8002 Epoch 28/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4195 - accuracy: 0.8098 - val_loss: 0.4327 - val_accuracy: 0.7988 Epoch 29/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4197 - accuracy: 0.8058 - val_loss: 0.4326 - val_accuracy: 0.8006 Epoch 30/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4172 - accuracy: 0.8076 - val_loss: 0.4335 - val_accuracy: 0.7954 Epoch 31/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4160 - accuracy: 0.8116 - val_loss: 0.4308 - val_accuracy: 0.8012 Epoch 32/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4161 - accuracy: 0.8108 - val_loss: 0.4319 - val_accuracy: 0.7986 Epoch 33/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4140 - accuracy: 0.8119 - val_loss: 0.4304 - val_accuracy: 0.8006 Epoch 34/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4146 - accuracy: 0.8110 - val_loss: 0.4299 - val_accuracy: 0.7984 Epoch 35/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4140 - accuracy: 0.8101 - val_loss: 0.4293 - val_accuracy: 0.8008 Epoch 36/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4132 - accuracy: 0.8122 - val_loss: 0.4293 - val_accuracy: 0.7994 Epoch 37/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4127 - accuracy: 0.8110 - val_loss: 0.4307 - val_accuracy: 0.7980 Epoch 38/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4137 - accuracy: 0.8112 - val_loss: 0.4286 - val_accuracy: 0.8046 Epoch 39/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4127 - accuracy: 0.8109 - val_loss: 0.4277 - val_accuracy: 0.8040 Epoch 40/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4103 - accuracy: 0.8106 - val_loss: 0.4284 - val_accuracy: 0.8052 Epoch 41/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4105 - accuracy: 0.8106 - val_loss: 0.4281 - val_accuracy: 0.8002 Epoch 42/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4111 - accuracy: 0.8137 - val_loss: 0.4284 - val_accuracy: 0.7982 1234567fig, ax = plt.subplots()ax.plot(history.history['loss'])ax.plot(history.history['val_loss'])ax.set_xlabel('epoch')ax.set_ylabel('loss')ax.legend(['train', 'val'])plt.show() return_squences = True 123456model3 = keras.Sequential()model3.add(keras.layers.Embedding(500, 16, input_length=100))model3.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True))model3.add(keras.layers.LSTM(8, dropout=0.3))model3.add(keras.layers.Dense(1, activation='sigmoid'))model3.summary() Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_2 (Embedding) (None, 100, 16) 8000 lstm_2 (LSTM) (None, 100, 8) 800 lstm_3 (LSTM) (None, 8) 544 dense_2 (Dense) (None, 1) 9 ================================================================= Total params: 9,353 Trainable params: 9,353 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model3.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])checkpoint_cb = keras.callbacks.ModelCheckpoint('best-2rnn-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model3.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 29s 81ms/step - loss: 0.6908 - accuracy: 0.5653 - val_loss: 0.6870 - val_accuracy: 0.6378 Epoch 2/100 313/313 [==============================] - 25s 79ms/step - loss: 0.6632 - accuracy: 0.6561 - val_loss: 0.6162 - val_accuracy: 0.6984 Epoch 3/100 313/313 [==============================] - 24s 78ms/step - loss: 0.5830 - accuracy: 0.7079 - val_loss: 0.5565 - val_accuracy: 0.7352 Epoch 4/100 313/313 [==============================] - 24s 78ms/step - loss: 0.5463 - accuracy: 0.7387 - val_loss: 0.5279 - val_accuracy: 0.7536 Epoch 5/100 313/313 [==============================] - 25s 79ms/step - loss: 0.5207 - accuracy: 0.7556 - val_loss: 0.5072 - val_accuracy: 0.7636 Epoch 6/100 313/313 [==============================] - 25s 79ms/step - loss: 0.5026 - accuracy: 0.7671 - val_loss: 0.4941 - val_accuracy: 0.7730 Epoch 7/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4910 - accuracy: 0.7728 - val_loss: 0.4812 - val_accuracy: 0.7818 Epoch 8/100 313/313 [==============================] - 31s 100ms/step - loss: 0.4813 - accuracy: 0.7782 - val_loss: 0.4747 - val_accuracy: 0.7810 Epoch 9/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4730 - accuracy: 0.7832 - val_loss: 0.4661 - val_accuracy: 0.7878 Epoch 10/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4657 - accuracy: 0.7869 - val_loss: 0.4612 - val_accuracy: 0.7896 Epoch 11/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4614 - accuracy: 0.7872 - val_loss: 0.4589 - val_accuracy: 0.7898 Epoch 12/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4568 - accuracy: 0.7886 - val_loss: 0.4547 - val_accuracy: 0.7934 Epoch 13/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4519 - accuracy: 0.7911 - val_loss: 0.4577 - val_accuracy: 0.7874 Epoch 14/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4484 - accuracy: 0.7945 - val_loss: 0.4498 - val_accuracy: 0.7930 Epoch 15/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4472 - accuracy: 0.7955 - val_loss: 0.4498 - val_accuracy: 0.7914 Epoch 16/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4447 - accuracy: 0.7983 - val_loss: 0.4468 - val_accuracy: 0.7968 Epoch 17/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4441 - accuracy: 0.7944 - val_loss: 0.4444 - val_accuracy: 0.7968 Epoch 18/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4418 - accuracy: 0.7990 - val_loss: 0.4442 - val_accuracy: 0.7950 Epoch 19/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4375 - accuracy: 0.8007 - val_loss: 0.4477 - val_accuracy: 0.7936 Epoch 20/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4365 - accuracy: 0.8027 - val_loss: 0.4416 - val_accuracy: 0.7964 Epoch 21/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4354 - accuracy: 0.8012 - val_loss: 0.4415 - val_accuracy: 0.7944 Epoch 22/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4341 - accuracy: 0.8014 - val_loss: 0.4408 - val_accuracy: 0.7972 Epoch 23/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4345 - accuracy: 0.8008 - val_loss: 0.4390 - val_accuracy: 0.7960 Epoch 24/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4315 - accuracy: 0.8037 - val_loss: 0.4412 - val_accuracy: 0.7912 Epoch 25/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4305 - accuracy: 0.8049 - val_loss: 0.4397 - val_accuracy: 0.7928 Epoch 26/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4294 - accuracy: 0.8051 - val_loss: 0.4382 - val_accuracy: 0.7990 Epoch 27/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4281 - accuracy: 0.8025 - val_loss: 0.4378 - val_accuracy: 0.7954 Epoch 28/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4277 - accuracy: 0.8035 - val_loss: 0.4375 - val_accuracy: 0.7952 Epoch 29/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4255 - accuracy: 0.8067 - val_loss: 0.4365 - val_accuracy: 0.7984 Epoch 30/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4254 - accuracy: 0.8055 - val_loss: 0.4360 - val_accuracy: 0.7990 Epoch 31/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4248 - accuracy: 0.8065 - val_loss: 0.4350 - val_accuracy: 0.8000 Epoch 32/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4247 - accuracy: 0.8051 - val_loss: 0.4352 - val_accuracy: 0.8006 Epoch 33/100 313/313 [==============================] - 25s 80ms/step - loss: 0.4240 - accuracy: 0.8061 - val_loss: 0.4358 - val_accuracy: 0.7958 Epoch 34/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4239 - accuracy: 0.8051 - val_loss: 0.4348 - val_accuracy: 0.8020 Epoch 35/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4240 - accuracy: 0.8066 - val_loss: 0.4340 - val_accuracy: 0.7998 Epoch 36/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4230 - accuracy: 0.8044 - val_loss: 0.4358 - val_accuracy: 0.8024 Epoch 37/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4227 - accuracy: 0.8061 - val_loss: 0.4334 - val_accuracy: 0.7994 Epoch 38/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4207 - accuracy: 0.8063 - val_loss: 0.4337 - val_accuracy: 0.8000 Epoch 39/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4194 - accuracy: 0.8075 - val_loss: 0.4349 - val_accuracy: 0.8012 Epoch 40/100 313/313 [==============================] - 25s 80ms/step - loss: 0.4203 - accuracy: 0.8073 - val_loss: 0.4326 - val_accuracy: 0.8018 Epoch 41/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4208 - accuracy: 0.8064 - val_loss: 0.4347 - val_accuracy: 0.8036 Epoch 42/100 313/313 [==============================] - 25s 80ms/step - loss: 0.4196 - accuracy: 0.8066 - val_loss: 0.4371 - val_accuracy: 0.8006 Epoch 43/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4196 - accuracy: 0.8066 - val_loss: 0.4358 - val_accuracy: 0.8018 1234567fig, ax = plt.subplots()ax.plot(history.history['loss'])ax.plot(history.history['val_loss'])ax.set_xlabel('epoch')ax.set_ylabel('loss')ax.legend(['train', 'val'])plt.show() GRU(Gated Recurrent Unit) 12345model4 = keras.Sequential()model4.add(keras.layers.Embedding(500, 16, input_length=100))model4.add(keras.layers.GRU(8))model4.add(keras.layers.Dense(1, activation='sigmoid'))model4.summary() Model: &quot;sequential_3&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_3 (Embedding) (None, 100, 16) 8000 gru (GRU) (None, 8) 624 dense_3 (Dense) (None, 1) 9 ================================================================= Total params: 8,633 Trainable params: 8,633 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model4.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])checkpoint_cb = keras.callbacks.ModelCheckpoint('best-gru-model.h5', save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model4.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 16s 45ms/step - loss: 0.6925 - accuracy: 0.5303 - val_loss: 0.6916 - val_accuracy: 0.5616 Epoch 2/100 313/313 [==============================] - 13s 43ms/step - loss: 0.6905 - accuracy: 0.5642 - val_loss: 0.6895 - val_accuracy: 0.5804 Epoch 3/100 313/313 [==============================] - 13s 43ms/step - loss: 0.6875 - accuracy: 0.5913 - val_loss: 0.6861 - val_accuracy: 0.5956 Epoch 4/100 313/313 [==============================] - 14s 43ms/step - loss: 0.6826 - accuracy: 0.6100 - val_loss: 0.6803 - val_accuracy: 0.6116 Epoch 5/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6747 - accuracy: 0.6278 - val_loss: 0.6712 - val_accuracy: 0.6314 Epoch 6/100 313/313 [==============================] - 13s 43ms/step - loss: 0.6611 - accuracy: 0.6477 - val_loss: 0.6548 - val_accuracy: 0.6538 Epoch 7/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6369 - accuracy: 0.6689 - val_loss: 0.6239 - val_accuracy: 0.6790 Epoch 8/100 313/313 [==============================] - 13s 42ms/step - loss: 0.5868 - accuracy: 0.7036 - val_loss: 0.5557 - val_accuracy: 0.7230 Epoch 9/100 313/313 [==============================] - 13s 42ms/step - loss: 0.5165 - accuracy: 0.7487 - val_loss: 0.5120 - val_accuracy: 0.7520 Epoch 10/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4907 - accuracy: 0.7675 - val_loss: 0.4945 - val_accuracy: 0.7624 Epoch 11/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4746 - accuracy: 0.7786 - val_loss: 0.4819 - val_accuracy: 0.7712 Epoch 12/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4622 - accuracy: 0.7870 - val_loss: 0.4729 - val_accuracy: 0.7780 Epoch 13/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4530 - accuracy: 0.7927 - val_loss: 0.4647 - val_accuracy: 0.7824 Epoch 14/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4451 - accuracy: 0.7976 - val_loss: 0.4613 - val_accuracy: 0.7862 Epoch 15/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4388 - accuracy: 0.8009 - val_loss: 0.4538 - val_accuracy: 0.7864 Epoch 16/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4342 - accuracy: 0.8044 - val_loss: 0.4503 - val_accuracy: 0.7912 Epoch 17/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4302 - accuracy: 0.8057 - val_loss: 0.4499 - val_accuracy: 0.7906 Epoch 18/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4265 - accuracy: 0.8094 - val_loss: 0.4460 - val_accuracy: 0.7964 Epoch 19/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4239 - accuracy: 0.8113 - val_loss: 0.4442 - val_accuracy: 0.7972 Epoch 20/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4214 - accuracy: 0.8130 - val_loss: 0.4434 - val_accuracy: 0.7978 Epoch 21/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4195 - accuracy: 0.8127 - val_loss: 0.4436 - val_accuracy: 0.7994 Epoch 22/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4180 - accuracy: 0.8145 - val_loss: 0.4415 - val_accuracy: 0.7968 Epoch 23/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4164 - accuracy: 0.8141 - val_loss: 0.4401 - val_accuracy: 0.8006 Epoch 24/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4151 - accuracy: 0.8149 - val_loss: 0.4401 - val_accuracy: 0.7978 Epoch 25/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4136 - accuracy: 0.8159 - val_loss: 0.4398 - val_accuracy: 0.7988 Epoch 26/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4128 - accuracy: 0.8171 - val_loss: 0.4394 - val_accuracy: 0.7962 Epoch 27/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4116 - accuracy: 0.8181 - val_loss: 0.4384 - val_accuracy: 0.7980 Epoch 28/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4111 - accuracy: 0.8163 - val_loss: 0.4378 - val_accuracy: 0.8012 Epoch 29/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4102 - accuracy: 0.8189 - val_loss: 0.4378 - val_accuracy: 0.7974 Epoch 30/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4096 - accuracy: 0.8183 - val_loss: 0.4380 - val_accuracy: 0.7978 Epoch 31/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4087 - accuracy: 0.8187 - val_loss: 0.4376 - val_accuracy: 0.7978 Epoch 32/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4080 - accuracy: 0.8183 - val_loss: 0.4387 - val_accuracy: 0.7976 Epoch 33/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4076 - accuracy: 0.8184 - val_loss: 0.4352 - val_accuracy: 0.8018 Epoch 34/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4065 - accuracy: 0.8183 - val_loss: 0.4359 - val_accuracy: 0.7948 Epoch 35/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4060 - accuracy: 0.8185 - val_loss: 0.4350 - val_accuracy: 0.7974 Epoch 36/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4059 - accuracy: 0.8194 - val_loss: 0.4426 - val_accuracy: 0.7946 Epoch 37/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4048 - accuracy: 0.8208 - val_loss: 0.4329 - val_accuracy: 0.8018 Epoch 38/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4044 - accuracy: 0.8197 - val_loss: 0.4335 - val_accuracy: 0.7998 Epoch 39/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4040 - accuracy: 0.8202 - val_loss: 0.4327 - val_accuracy: 0.8010 Epoch 40/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4039 - accuracy: 0.8199 - val_loss: 0.4325 - val_accuracy: 0.8042 Epoch 41/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4031 - accuracy: 0.8197 - val_loss: 0.4316 - val_accuracy: 0.8024 Epoch 42/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4025 - accuracy: 0.8213 - val_loss: 0.4316 - val_accuracy: 0.8004 Epoch 43/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4020 - accuracy: 0.8198 - val_loss: 0.4320 - val_accuracy: 0.7986 Epoch 44/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4016 - accuracy: 0.8214 - val_loss: 0.4311 - val_accuracy: 0.8020 Epoch 45/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4011 - accuracy: 0.8214 - val_loss: 0.4301 - val_accuracy: 0.8040 Epoch 46/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4001 - accuracy: 0.8210 - val_loss: 0.4306 - val_accuracy: 0.8008 Epoch 47/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4001 - accuracy: 0.8224 - val_loss: 0.4375 - val_accuracy: 0.7950 Epoch 48/100 313/313 [==============================] - 14s 43ms/step - loss: 0.3998 - accuracy: 0.8222 - val_loss: 0.4306 - val_accuracy: 0.8018 1234567fig, ax = plt.subplots()ax.plot(history.history['loss'])ax.plot(history.history['val_loss'])ax.set_xlabel('epoch')ax.set_ylabel('loss')ax.legend(['train', 'val'])plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","link":"/2022/04/08/Python/ML/ML_ch_9_3/"},{"title":"10minutes to Pandas","text":"Import Library1234import numpy as npprint(&quot;numpy ver.&quot; + np.__version__)import pandas as pdprint(&quot;pandas ver.&quot; + pd.__version__) numpy ver.1.21.5 pandas ver.1.3.5 Object CreationCreating Series by passing a list of values, letting pandas create a default integer index 12s = pd.Series([1,3,5,np.nan, 6,8])print(s) 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 Creation DataFrame by passing a NumPy array, with a datetime index and labeled columns 12345dates = pd.date_range(&quot;20130101&quot;, periods=6)print(dates)print()df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&quot;ABCD&quot;))print(df) DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') A B C D 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 by passing a dictionary of objects that can be converted into a series-like structure 1234567891011121314df2 = pd.DataFrame( { &quot;A&quot;: 1.0, &quot;B&quot;: pd.Timestamp(&quot;20130102&quot;), &quot;C&quot;: pd.Series(1, index=list(range(4)), dtype=&quot;float32&quot;), &quot;D&quot;: np.array([3] * 4, dtype=&quot;int32&quot;), &quot;E&quot;: pd.Categorical([&quot;test&quot;, &quot;train&quot;, &quot;test&quot;, &quot;train&quot;]), &quot;F&quot;: &quot;foo&quot;, })print(df2)print()print(&quot;dtypes:&quot;)print(df2.dtypes) A B C D E F 0 1.0 2013-01-02 1.0 3 test foo 1 1.0 2013-01-02 1.0 3 train foo 2 1.0 2013-01-02 1.0 3 test foo 3 1.0 2013-01-02 1.0 3 train foo dtypes: A float64 B datetime64[ns] C float32 D int32 E category F object dtype: object 12# IPython# df2.&lt;TAB&gt; Viewing DataBasic Structure123print(&quot;Head 5:\\n&quot;, df.head())print()print(&quot;Tail 5:\\n&quot;, df.tail()) Head 5: A B C D 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 -1.707570 1.163615 Tail 5: A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 12print(&quot;Index :\\n&quot;, df.index)print(&quot;Columns:\\n&quot;, df.columns) Index : DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') Columns: Index(['A', 'B', 'C', 'D'], dtype='object') DataFrame to NumPy array This can be an expensive operation when your DataFrame has columns with different data types: NumPy arrays have one dtype for the entire array. Pandas DataFrames have one dtype per column. With heterogeneous data, the lowest common type will have to be used. For a mix of numeric and non-numeric types, the output array will have object dtype. 1df.values array([[-0.81889557, -0.40918379, 0.08744721, -0.93588677], [ 0.68154262, -1.39097644, 2.01310478, 0.64446846], [ 1.01791104, 0.03322364, -0.1039122 , 0.63445887], [-0.45043711, 0.50191541, 1.00377617, 0.69124864], [ 1.633764 , 0.32423439, -1.70757042, 1.16361464], [ 0.28240247, -0.92266305, -1.64131359, 0.50543323]]) 123# For df, our DataFrame of all floating-point values,# DataFrame.to_numpy() is fast and doesn't require copying data:df.to_numpy() array([[-0.81889557, -0.40918379, 0.08744721, -0.93588677], [ 0.68154262, -1.39097644, 2.01310478, 0.64446846], [ 1.01791104, 0.03322364, -0.1039122 , 0.63445887], [-0.45043711, 0.50191541, 1.00377617, 0.69124864], [ 1.633764 , 0.32423439, -1.70757042, 1.16361464], [ 0.28240247, -0.92266305, -1.64131359, 0.50543323]]) 1df2.values array([[1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo']], dtype=object) 123# For df2, the DataFrame with multiple dtypes,# DataFrame.to_numpy() is relatively expensive:df2.to_numpy() array([[1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo']], dtype=object) Describe, Transpose, Sort1print(df.describe()) A B C D count 6.000000 6.000000 6.000000 6.000000 mean 0.391048 -0.310575 -0.058078 0.450556 std 0.917121 0.739319 1.460690 0.715967 min -0.818896 -1.390976 -1.707570 -0.935887 25% -0.267227 -0.794293 -1.256963 0.537690 50% 0.481973 -0.187980 -0.008232 0.639464 75% 0.933819 0.251482 0.774694 0.679554 max 1.633764 0.501915 2.013105 1.163615 1print(df.T) 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06 A -0.818896 0.681543 1.017911 -0.450437 1.633764 0.282402 B -0.409184 -1.390976 0.033224 0.501915 0.324234 -0.922663 C 0.087447 2.013105 -0.103912 1.003776 -1.707570 -1.641314 D -0.935887 0.644468 0.634459 0.691249 1.163615 0.505433 12345print(&quot;Sorted by axis 1:&quot;)print(df.sort_index(axis=1, ascending=False))print()print(&quot;Sorted by values B:&quot;)print(df.sort_values(by=&quot;B&quot;)) Sorted by axis 1: D C B A 2013-01-01 -0.935887 0.087447 -0.409184 -0.818896 2013-01-02 0.644468 2.013105 -1.390976 0.681543 2013-01-03 0.634459 -0.103912 0.033224 1.017911 2013-01-04 0.691249 1.003776 0.501915 -0.450437 2013-01-05 1.163615 -1.707570 0.324234 1.633764 2013-01-06 0.505433 -1.641314 -0.922663 0.282402 Sorted by values B: A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-04 -0.450437 0.501915 1.003776 0.691249 Selecting DataGetting12print(df[&quot;A&quot;])# print(df.A) 2013-01-01 -0.818896 2013-01-02 0.681543 2013-01-03 1.017911 2013-01-04 -0.450437 2013-01-05 1.633764 2013-01-06 0.282402 Freq: D, Name: A, dtype: float64 12print(df[:3])print(df[&quot;20130102&quot;:&quot;20130104&quot;]) A B C D 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 Selection by Label1print(df.loc[dates[0]]) A -0.818896 B -0.409184 C 0.087447 D -0.935887 Name: 2013-01-01 00:00:00, dtype: float64 1print(df.loc[&quot;20130102&quot;:&quot;20130104&quot;, [&quot;A&quot;,&quot;B&quot;]]) A B 2013-01-02 0.681543 -1.390976 2013-01-03 1.017911 0.033224 2013-01-04 -0.450437 0.501915 1print(df.loc[&quot;20130102&quot;, [&quot;A&quot;,&quot;B&quot;]]) A 0.681543 B -1.390976 Name: 2013-01-02 00:00:00, dtype: float64 12print(df.loc[dates[0], &quot;A&quot;])print(df.at[dates[0],&quot;A&quot;]) # equivalent to the prior method -0.818895566676464 -0.818895566676464 Selection by Position1print(df.iloc[3]) A -0.450437 B 0.501915 C 1.003776 D 0.691249 Name: 2013-01-04 00:00:00, dtype: float64 1print(df.iloc[3:5, 0:2]) A B 2013-01-04 -0.450437 0.501915 2013-01-05 1.633764 0.324234 1print(df.iloc[[1,2,5],[1,3]]) B D 2013-01-02 -1.390976 0.644468 2013-01-03 0.033224 0.634459 2013-01-06 -0.922663 0.505433 1print(df.iloc[1:3,:]) A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 1print(df.iloc[:,1:3]) B C 2013-01-01 -0.409184 0.087447 2013-01-02 -1.390976 2.013105 2013-01-03 0.033224 -0.103912 2013-01-04 0.501915 1.003776 2013-01-05 0.324234 -1.707570 2013-01-06 -0.922663 -1.641314 12print(df.iloc[1,1])print(df.iat[1,1]) # equivalent to the prior method -1.3909764417520816 -1.3909764417520816 Boolean Indexing1print(df[df[&quot;A&quot;]&gt;0]) A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 1print(df[df&gt;0]) A B C D 2013-01-01 NaN NaN 0.087447 NaN 2013-01-02 0.681543 NaN 2.013105 0.644468 2013-01-03 1.017911 0.033224 NaN 0.634459 2013-01-04 NaN 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 NaN 1.163615 2013-01-06 0.282402 NaN NaN 0.505433 12345df2 = df.copy()df2[&quot;E&quot;] = ['one', 'one', 'two', 'three', 'four', 'three']print(df2)print()print(df2[df2[&quot;E&quot;].isin([&quot;two&quot;, &quot;four&quot;])]) A B C D E 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 one 2013-01-02 0.681543 -1.390976 2.013105 0.644468 one 2013-01-03 1.017911 0.033224 -0.103912 0.634459 two 2013-01-04 -0.450437 0.501915 1.003776 0.691249 three 2013-01-05 1.633764 0.324234 -1.707570 1.163615 four 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 three A B C D E 2013-01-03 1.017911 0.033224 -0.103912 0.634459 two 2013-01-05 1.633764 0.324234 -1.707570 1.163615 four Setting12345s1 = pd.Series([1,2,3,4,5,6], index = pd.date_range(&quot;20130102&quot;,periods=6))print(s1)print()df[&quot;F&quot;] = s1print(df) 2013-01-02 1 2013-01-03 2 2013-01-04 3 2013-01-05 4 2013-01-06 5 2013-01-07 6 Freq: D, dtype: int64 A B C D F 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 NaN 2013-01-02 0.681543 -1.390976 2.013105 0.644468 1.0 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2.0 2013-01-04 -0.450437 0.501915 1.003776 0.691249 3.0 2013-01-05 1.633764 0.324234 -1.707570 1.163615 4.0 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 5.0 1234df.at[dates[0], &quot;A&quot;] = 0 # setting values by labeldf.iat[0,1] = 0 # setting values by positiondf.loc[:, &quot;D&quot;] = np.array([5] * len(df)) # setting by assigning with a NumPy arrayprint(df) A B C D F 2013-01-01 0.000000 0.000000 0.087447 5 NaN 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 2013-01-05 1.633764 0.324234 -1.707570 5 4.0 2013-01-06 0.282402 -0.922663 -1.641314 5 5.0 123df2 = df.copy()df2[df2&gt;0] = -df2 # setting values by booleanprint(df2) A B C D F 2013-01-01 0.000000 0.000000 -0.087447 -5 NaN 2013-01-02 -0.681543 -1.390976 -2.013105 -5 -1.0 2013-01-03 -1.017911 -0.033224 -0.103912 -5 -2.0 2013-01-04 -0.450437 -0.501915 -1.003776 -5 -3.0 2013-01-05 -1.633764 -0.324234 -1.707570 -5 -4.0 2013-01-06 -0.282402 -0.922663 -1.641314 -5 -5.0 Missing Data123df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [&quot;E&quot;])df1.loc[dates[0] : dates[1], &quot;E&quot;] = 1print(df1) A B C D F E 2013-01-01 0.000000 0.000000 0.087447 5 NaN 1.0 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 NaN 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 NaN 123print(df1.dropna(how=&quot;any&quot;)) # how=&quot;any&quot; (default) : where any NA values are presentprint()print(df1.dropna(how=&quot;all&quot;)) # how=&quot;all&quot; : where all values are NA A B C D F E 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 A B C D F E 2013-01-01 0.000000 0.000000 0.087447 5 NaN 1.0 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 NaN 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 NaN 123print(pd.isna(df1))print()print(df1.fillna(value=5)) A B C D F E 2013-01-01 False False False False True False 2013-01-02 False False False False False False 2013-01-03 False False False False False True 2013-01-04 False False False False False True A B C D F E 2013-01-01 0.000000 0.000000 0.087447 5 5.0 1.0 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 5.0 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 5.0 OperationsStats12345print(&quot;axis 0 :&quot;)print(df.mean())print()print(&quot;axis 1 :&quot;)print(df.mean(1)) axis 0 : A 0.527531 B -0.242378 C -0.058078 D 5.000000 F 3.000000 dtype: float64 axis 1 : 2013-01-01 1.271862 2013-01-02 1.460734 2013-01-03 1.589444 2013-01-04 1.811051 2013-01-05 1.850086 2013-01-06 1.543685 Freq: D, dtype: float64 123456print(df)print()s = pd.Series([1, 3, 4, np.nan, 6, 8], index=dates).shift(2)print(s)print()print(df.sub(s, axis=0)) # equivalent to (dataframe - other) A B C D F 2013-01-01 0.000000 0.000000 0.087447 5 NaN 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 2013-01-05 1.633764 0.324234 -1.707570 5 4.0 2013-01-06 0.282402 -0.922663 -1.641314 5 5.0 2013-01-01 NaN 2013-01-02 NaN 2013-01-03 1.0 2013-01-04 3.0 2013-01-05 4.0 2013-01-06 NaN Freq: D, dtype: float64 A B C D F 2013-01-01 NaN NaN NaN NaN NaN 2013-01-02 NaN NaN NaN NaN NaN 2013-01-03 0.017911 -0.966776 -1.103912 4.0 1.0 2013-01-04 -3.450437 -2.498085 -1.996224 2.0 0.0 2013-01-05 -2.366236 -3.675766 -5.707570 1.0 0.0 2013-01-06 NaN NaN NaN NaN NaN Apply1print(df.apply(np.cumsum)) A B C D F 2013-01-01 0.000000 0.000000 0.087447 5 NaN 2013-01-02 0.681543 -1.390976 2.100552 10 1.0 2013-01-03 1.699454 -1.357753 1.996640 15 3.0 2013-01-04 1.249017 -0.855837 3.000416 20 6.0 2013-01-05 2.882781 -0.531603 1.292846 25 10.0 2013-01-06 3.165183 -1.454266 -0.348468 30 15.0 1print(df.apply(lambda x: x.max() - x.min())) A 2.084201 B 1.892892 C 3.720675 D 0.000000 F 4.000000 dtype: float64 Histogramming1234s = pd.Series(np.random.randint(0, 7, size=10))print(s)print()print(s.value_counts()) 0 1 1 6 2 5 3 1 4 1 5 3 6 2 7 1 8 1 9 6 dtype: int64 1 5 6 2 5 1 3 1 2 1 dtype: int64 String Methods1234s = pd.Series([&quot;A&quot;,&quot;Aaba&quot;, np.nan, &quot;CABA&quot;, &quot;cat&quot;])print(s.str.lower())print()print(s.str.upper()) 0 a 1 aaba 2 NaN 3 caba 4 cat dtype: object 0 A 1 AABA 2 NaN 3 CABA 4 CAT dtype: object MergeConcat combining together Series and DataFrame objects 12df = pd.DataFrame(np.random.randn(10,4))print(df) 0 1 2 3 0 -0.146700 1.278642 1.837775 0.644873 1 -0.332040 1.597295 -0.681229 -1.238212 2 0.751823 1.117058 -0.453366 0.953989 3 0.074173 -1.043050 0.276312 0.926186 4 -0.090674 1.679349 2.130480 0.950658 5 0.483778 -0.330530 0.370747 0.569736 6 -0.603331 2.363939 -0.052191 0.186119 7 -2.002784 -1.237193 -1.876920 -0.876104 8 1.121755 -0.104830 -1.675228 1.250540 9 0.008456 -1.287063 0.070528 -0.642563 1234pieces = [df[:3], df[3:7], df[7:]]print(pieces)print()print(pd.concat(pieces)) # Concatenating objects together [ 0 1 2 3 0 -0.146700 1.278642 1.837775 0.644873 1 -0.332040 1.597295 -0.681229 -1.238212 2 0.751823 1.117058 -0.453366 0.953989, 0 1 2 3 3 0.074173 -1.043050 0.276312 0.926186 4 -0.090674 1.679349 2.130480 0.950658 5 0.483778 -0.330530 0.370747 0.569736 6 -0.603331 2.363939 -0.052191 0.186119, 0 1 2 3 7 -2.002784 -1.237193 -1.876920 -0.876104 8 1.121755 -0.104830 -1.675228 1.250540 9 0.008456 -1.287063 0.070528 -0.642563] 0 1 2 3 0 -0.146700 1.278642 1.837775 0.644873 1 -0.332040 1.597295 -0.681229 -1.238212 2 0.751823 1.117058 -0.453366 0.953989 3 0.074173 -1.043050 0.276312 0.926186 4 -0.090674 1.679349 2.130480 0.950658 5 0.483778 -0.330530 0.370747 0.569736 6 -0.603331 2.363939 -0.052191 0.186119 7 -2.002784 -1.237193 -1.876920 -0.876104 8 1.121755 -0.104830 -1.675228 1.250540 9 0.008456 -1.287063 0.070528 -0.642563 Join SQL style merging 1234567left = pd.DataFrame({'key':['foo', 'foo'], 'lval':[1, 2]})right = pd.DataFrame({'key':['foo', 'foo'], 'rval':[4, 5]})print(left)print()print(right)print()print(pd.merge(left, right, on='key')) key lval 0 foo 1 1 foo 2 key rval 0 foo 4 1 foo 5 key lval rval 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 1234567left = pd.DataFrame({'key':['foo', 'bar'], 'lval':[1, 2]})right = pd.DataFrame({'key':['foo', 'bar'], 'rval':[4, 5]})print(left)print()print(right)print()print(pd.merge(left, right, on='key')) key lval 0 foo 1 1 bar 2 key rval 0 foo 4 1 bar 5 key lval rval 0 foo 1 4 1 bar 2 5 Append12df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])print(df) A B C D 0 0.120201 -0.286267 -0.205740 1.107503 1 -0.818479 -0.059564 0.192151 1.920259 2 0.687467 -0.833330 -0.426913 0.852926 3 -1.103335 0.213012 1.003000 -0.648385 4 -0.576006 1.414470 0.851227 -0.277232 5 0.189853 0.311170 -0.448372 -0.201059 6 1.707547 0.099129 -1.332999 0.478148 7 1.153861 1.831233 0.125928 -1.330353 12s = df.iloc[3]print(df.append(s, ignore_index=True)) A B C D 0 0.120201 -0.286267 -0.205740 1.107503 1 -0.818479 -0.059564 0.192151 1.920259 2 0.687467 -0.833330 -0.426913 0.852926 3 -1.103335 0.213012 1.003000 -0.648385 4 -0.576006 1.414470 0.851227 -0.277232 5 0.189853 0.311170 -0.448372 -0.201059 6 1.707547 0.099129 -1.332999 0.478148 7 1.153861 1.831233 0.125928 -1.330353 8 -1.103335 0.213012 1.003000 -0.648385 Grouping Splitting the data into groups based on some criteria Applying a function to each group independently Combining the results into a data structure 123456789df = pd.DataFrame( { &quot;A&quot;: [&quot;foo&quot;, &quot;bar&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;foo&quot;, &quot;foo&quot;], &quot;B&quot;: [&quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;two&quot;, &quot;two&quot;, &quot;one&quot;, &quot;three&quot;], &quot;C&quot;: np.random.randn(8), &quot;D&quot;: np.random.randn(8), })print(df) A B C D 0 foo one 0.031321 -0.238988 1 bar one -0.626332 -0.445851 2 foo two -1.448981 1.262838 3 bar three -0.424664 -0.639157 4 foo two 1.547849 2.378992 5 bar two -0.351304 -0.521492 6 foo one -1.903777 1.998802 7 foo three -0.613947 -0.422391 1print(df.groupby(&quot;A&quot;).sum()) C D A bar -1.402300 -1.606500 foo -2.387536 4.979252 1print(df.groupby([&quot;A&quot;,&quot;B&quot;]).sum()) C D A B bar one -0.626332 -0.445851 three -0.424664 -0.639157 two -0.351304 -0.521492 foo one -1.872457 1.759814 three -0.613947 -0.422391 two 0.098868 3.641830 1234df_min = df.groupby([&quot;A&quot;,&quot;B&quot;])[&quot;C&quot;].min()df_max = df.groupby([&quot;A&quot;,&quot;B&quot;])[&quot;C&quot;].max()print(pd.merge(df_min, df_max, on=['A',&quot;B&quot;], suffixes=('_min', '_max'))) C_min C_max A B bar one -0.626332 -0.626332 three -0.424664 -0.424664 two -0.351304 -0.351304 foo one -1.903777 0.031321 three -0.613947 -0.613947 two -1.448981 1.547849 ReshapingStack stack() : Compress a level in the DataFrame’s columns: 1234567891011121314tuples = list( zip( *[ [&quot;bar&quot;, &quot;bar&quot;, &quot;baz&quot;, &quot;baz&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;qux&quot;, &quot;qux&quot;], [&quot;one&quot;, &quot;two&quot;, &quot;one&quot;, &quot;two&quot;, &quot;one&quot;, &quot;two&quot;, &quot;one&quot;, &quot;two&quot;], ] ))index = pd.MultiIndex.from_tuples(tuples, names=[&quot;idx_1&quot;, &quot;idx_2&quot;])df = pd.DataFrame(np.random.randn(8,2), index=index, columns=[&quot;A&quot;,&quot;B&quot;])df2 = df[:4]print(df2) A B idx_1 idx_2 bar one -0.748802 0.560048 two -0.214015 -0.658540 baz one -1.968829 -0.806776 two -1.314742 -0.174498 12stacked = df2.stack()print(stacked) # level: idx_1 &gt; idx_2 &gt; columns idx_1 idx_2 bar one A -0.748802 B 0.560048 two A -0.214015 B -0.658540 baz one A -1.968829 B -0.806776 two A -1.314742 B -0.174498 dtype: float64 unstack() : the inverse operation of stack() 12345print(stacked.unstack()) # columns(last level) / default -1print()print(stacked.unstack(0)) # idx_1 print()print(stacked.unstack(1)) # idx_2 A B idx_1 idx_2 bar one -0.748802 0.560048 two -0.214015 -0.658540 baz one -1.968829 -0.806776 two -1.314742 -0.174498 idx_1 bar baz idx_2 one A -0.748802 -1.968829 B 0.560048 -0.806776 two A -0.214015 -1.314742 B -0.658540 -0.174498 idx_2 one two idx_1 bar A -0.748802 -0.214015 B 0.560048 -0.658540 baz A -1.968829 -1.314742 B -0.806776 -0.174498 Pivot Table12345678910df = pd.DataFrame( { &quot;A&quot;: [&quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;] * 3, &quot;B&quot;: [&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;] * 4, &quot;C&quot;: [&quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;] * 2, &quot;D&quot;: np.random.randn(12), &quot;E&quot;: np.random.randn(12), })print(df) A B C D E 0 one aa foo 1.055556 -0.342298 1 one bb foo -0.463657 -0.004332 2 two cc foo 0.953746 0.690613 3 three aa bar -0.980697 0.498251 4 one bb bar -0.352120 -0.503475 5 one cc bar 0.298470 -1.316212 6 two aa foo 0.580929 0.483970 7 three bb foo 0.391527 0.200354 8 one cc foo -1.314898 -1.183403 9 one aa bar -0.058855 -0.004713 10 two bb bar -0.253133 1.255313 11 three cc bar 0.882602 0.561369 1print(pd.pivot_table(df, values=&quot;D&quot;, index=[&quot;A&quot;,&quot;B&quot;], columns=&quot;C&quot;)) C bar foo A B one aa -0.058855 1.055556 bb -0.352120 -0.463657 cc 0.298470 -1.314898 three aa -0.980697 NaN bb NaN 0.391527 cc 0.882602 NaN two aa NaN 0.580929 bb -0.253133 NaN cc NaN 0.953746 Time Series12345rng = pd.date_range(&quot;1/1/2022&quot;, periods=100, freq=&quot;S&quot;) # secondly frequencyts = pd.Series(np.random.randint(0,500,len(rng)), index=rng)print(ts.resample(&quot;5Min&quot;).first())print(ts.resample(&quot;5Min&quot;).last())print(ts.resample(&quot;5Min&quot;).sum()) 2022-01-01 192 Freq: 5T, dtype: int64 2022-01-01 304 Freq: 5T, dtype: int64 2022-01-01 26586 Freq: 5T, dtype: int64 123rng = pd.date_range(&quot;3/6/2021 00:00&quot;, periods=5, freq=&quot;D&quot;) # daily frequencyts = pd.Series(np.random.randn(len(rng)), rng)print(ts) 2021-03-06 -1.465806 2021-03-07 0.938092 2021-03-08 2.811226 2021-03-09 -0.186533 2021-03-10 -1.048929 Freq: D, dtype: float64 12345ts_utc = ts.tz_localize(&quot;UTC&quot;) # UTC timezoneprint(&quot;UTC :&quot;)print(ts_utc)print(&quot;\\nUS/Eastern :&quot;)print(ts_utc.tz_convert(&quot;US/Eastern&quot;)) # US/Eastern timezone UTC : 2021-03-06 00:00:00+00:00 -1.465806 2021-03-07 00:00:00+00:00 0.938092 2021-03-08 00:00:00+00:00 2.811226 2021-03-09 00:00:00+00:00 -0.186533 2021-03-10 00:00:00+00:00 -1.048929 Freq: D, dtype: float64 US/Eastern : 2021-03-05 19:00:00-05:00 -1.465806 2021-03-06 19:00:00-05:00 0.938092 2021-03-07 19:00:00-05:00 2.811226 2021-03-08 19:00:00-05:00 -0.186533 2021-03-09 19:00:00-05:00 -1.048929 Freq: D, dtype: float64 12345678rng = pd.date_range(&quot;1/1/2022&quot;, periods=5, freq=&quot;M&quot;)ts = pd.Series(np.random.randn(len(rng)), index=rng)print(ts) # DatetimeIndexprint()ps = ts.to_period()print(ps) # PeriodIndexprint()print(ps.to_timestamp()) 2022-01-31 0.438135 2022-02-28 -0.052516 2022-03-31 -1.491175 2022-04-30 0.708521 2022-05-31 -1.794057 Freq: M, dtype: float64 DatetimeIndex(['2022-01-31', '2022-02-28', '2022-03-31', '2022-04-30', '2022-05-31'], dtype='datetime64[ns]', freq='M') 2022-01 0.438135 2022-02 -0.052516 2022-03 -1.491175 2022-04 0.708521 2022-05 -1.794057 Freq: M, dtype: float64 PeriodIndex(['2022-01', '2022-02', '2022-03', '2022-04', '2022-05'], dtype='period[M]') 2022-01-01 0.438135 2022-02-01 -0.052516 2022-03-01 -1.491175 2022-04-01 0.708521 2022-05-01 -1.794057 Freq: MS, dtype: float64 12345678prng = pd.period_range(&quot;1990Q1&quot;, &quot;2000Q4&quot;, freq=&quot;Q-NOV&quot;) # quarterly frequencyts = pd.Series(np.random.randn(len(prng)), prng)print(ts.head())print()# month end &amp; hour startts.index = (prng.asfreq(&quot;M&quot;, &quot;e&quot;) + 1).asfreq(&quot;H&quot;, &quot;s&quot;) + 9print(ts.head()) 1990Q1 1.479314 1990Q2 0.799696 1990Q3 0.148978 1990Q4 -0.571086 1991Q1 1.908509 Freq: Q-NOV, dtype: float64 1990-03-01 09:00 1.479314 1990-06-01 09:00 0.799696 1990-09-01 09:00 0.148978 1990-12-01 09:00 -0.571086 1991-03-01 09:00 1.908509 Freq: H, dtype: float64 Categoricals1234df = pd.DataFrame({&quot;id&quot;: [1, 2, 3, 4, 5, 6], &quot;raw_grade&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;e&quot;]})print(df) id raw_grade 0 1 a 1 2 b 2 3 b 3 4 a 4 5 a 5 6 e 123456df[&quot;grade&quot;] = df[&quot;raw_grade&quot;].astype(&quot;category&quot;)df[&quot;grade&quot;].cat.categories = [&quot;very good&quot;, &quot;good&quot;, &quot;very bad&quot;] # rename categoriesdf[&quot;grade&quot;] = df[&quot;grade&quot;].cat.set_categories( [&quot;very bad&quot;, &quot;bad&quot;,&quot;medium&quot;,&quot;good&quot;,&quot;very good&quot;])df[&quot;grade&quot;] 0 very good 1 good 2 good 3 very good 4 very good 5 very bad Name: grade, dtype: category Categories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good'] 1print(df.sort_values(by=&quot;grade&quot;).reset_index(drop=True)) id raw_grade grade 0 6 e very bad 1 2 b good 2 3 b good 3 1 a very good 4 4 a very good 5 5 a very good 1print(df.groupby(&quot;grade&quot;).size()) grade very bad 1 bad 0 medium 0 good 2 very good 3 dtype: int64 Plotting123456import matplotlib.pyplot as pltts = pd.Series(np.random.randn(1000), index=pd.date_range(&quot;1/1/2010&quot;, periods=1000))ts = ts.cumsum()ts.plot()plt.show() 12345df = pd.DataFrame(np.random.randn(1000,4), index=ts.index, columns=[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;])df = df.cumsum()df.plot()plt.legend(loc='best')plt.show() Getting data in/outCSV1df.to_csv(&quot;foo.csv&quot;) 1print(pd.read_csv(&quot;foo.csv&quot;)) Unnamed: 0 A B C D 0 2010-01-01 0.015846 0.025822 0.443290 -0.724835 1 2010-01-02 -0.084776 0.330946 -0.669265 -0.949676 2 2010-01-03 1.325583 -0.697926 -0.204375 -1.145391 3 2010-01-04 2.293831 -2.097458 -1.260249 -0.555542 4 2010-01-05 1.334001 -2.392123 -2.659330 -0.325232 .. ... ... ... ... ... 995 2012-09-22 -7.868011 -59.520981 -49.220218 -54.072522 996 2012-09-23 -7.288884 -59.584693 -49.307716 -54.783148 997 2012-09-24 -6.835061 -59.508180 -49.608991 -55.780242 998 2012-09-25 -6.863802 -58.711503 -50.458917 -55.218363 999 2012-09-26 -7.075728 -59.524064 -49.924592 -55.653215 [1000 rows x 5 columns] HDF51df.to_hdf(&quot;foo.h5&quot;, &quot;df&quot;) 1print(pd.read_hdf(&quot;foo.h5&quot;, &quot;df&quot;)) A B C D 2010-01-01 0.015846 0.025822 0.443290 -0.724835 2010-01-02 -0.084776 0.330946 -0.669265 -0.949676 2010-01-03 1.325583 -0.697926 -0.204375 -1.145391 2010-01-04 2.293831 -2.097458 -1.260249 -0.555542 2010-01-05 1.334001 -2.392123 -2.659330 -0.325232 ... ... ... ... ... 2012-09-22 -7.868011 -59.520981 -49.220218 -54.072522 2012-09-23 -7.288884 -59.584693 -49.307716 -54.783148 2012-09-24 -6.835061 -59.508180 -49.608991 -55.780242 2012-09-25 -6.863802 -58.711503 -50.458917 -55.218363 2012-09-26 -7.075728 -59.524064 -49.924592 -55.653215 [1000 rows x 4 columns] Excel1df.to_excel(&quot;foo.xlsx&quot;, sheet_name=&quot;Sheet1&quot;) 1print(pd.read_excel(&quot;foo.xlsx&quot;, &quot;Sheet1&quot;, index_col=None, na_values=[&quot;NA&quot;])) Unnamed: 0 A B C D 0 2010-01-01 0.015846 0.025822 0.443290 -0.724835 1 2010-01-02 -0.084776 0.330946 -0.669265 -0.949676 2 2010-01-03 1.325583 -0.697926 -0.204375 -1.145391 3 2010-01-04 2.293831 -2.097458 -1.260249 -0.555542 4 2010-01-05 1.334001 -2.392123 -2.659330 -0.325232 .. ... ... ... ... ... 995 2012-09-22 -7.868011 -59.520981 -49.220218 -54.072522 996 2012-09-23 -7.288884 -59.584693 -49.307716 -54.783148 997 2012-09-24 -6.835061 -59.508180 -49.608991 -55.780242 998 2012-09-25 -6.863802 -58.711503 -50.458917 -55.218363 999 2012-09-26 -7.075728 -59.524064 -49.924592 -55.653215 [1000 rows x 5 columns]","link":"/2022/03/24/Python/Tutorial/10m_to_pd/"},{"title":"Numpy tutorial","text":"파이썬 라이브러리 설치in R (코드에서 실행) install.package(“패키지명”) library(패키지명) in Python (터미널에서 실행) 방법1. conda 설치 (주 사용목적: 데이터 과학) 아나콘다 설치 후에 conda 설치 가능 방법2. pip 설치 (개발, 데이터 과학, 그 외) 아나콘다 없이도 python만 설치하면 됨 NumPy 라이브러리 파이썬의 대표적인 배열 라이브러리 파이썬 수치 연산과 관련된 모든 라이브러리의 기본 scikit learn, TensorFlow, PyTorch 등등 .array, .reshape, .matlib 등등 다양한 메서드 활용 Numpy 라이브러리 불러오기12import numpy as npprint(np.__version__) 1.21.5 NumPy 배열 생성리스트를 배열로 변환 NumPy 배열로 변환하여 저장 1234567temp = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]print(temp)print(type(temp))arr = np.array(temp)print(arr)print(type(arr)) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] &lt;class 'list'&gt; [ 1 2 3 4 5 6 7 8 9 10] &lt;class 'numpy.ndarray'&gt; NumPy 배열에도 인덱싱, 슬라이싱 동일하게 적용 123print(arr[4])print(arr[3:7])print(arr[::2]) 5 [4 5 6 7] [1 3 5 7 9] NumPy를 통한 기초 통계 함수 사용 123456print(&quot;sum:&quot;, np.sum(arr))print(&quot;mean:&quot;, np.mean(arr))print(&quot;median:&quot;, np.median(arr))print(&quot;min:&quot;, np.min(arr))print(&quot;max:&quot;, np.max(arr))print(&quot;std:&quot;, np.std(arr)) sum: 55 mean: 5.5 median: 5.5 min: 1 max: 10 std: 2.8722813232690143 사칙연산123456789math_scores = [90, 80, 88]english_scores = [80, 70, 90]total_scores = math_scores + english_scores # 단순한 리스트 연결 수행print(total_scores)math_arr = np.array(math_scores)english_arr = np.array(english_scores)total_arr = math_arr + english_arr # 각 요소의 덧셈 수행print(total_arr) [90, 80, 88, 80, 70, 90] [170 150 178] 123456789arr1 = np.array([2, 5, 4])arr2 = np.array([4, 2, 3])# 사칙연산print(&quot;덧셈:&quot;, np.add(arr1, arr2))print(&quot;뺄셈:&quot;, np.subtract(arr1, arr2))print(&quot;곱셈:&quot;, np.multiply(arr1, arr2))print(&quot;나눗셈:&quot;, np.divide(arr1, arr2))print(&quot;거듭제곱:&quot;, np.power(arr1, arr2)) 덧셈: [6 7 7] 뺄셈: [-2 3 1] 곱셈: [ 8 10 12] 나눗셈: [0.5 2.5 1.33333333] 거듭제곱: [16 25 64] 소수점 처리123456# 소수점 절삭temp_arr = np.trunc([-1.23, 1.23])print(temp_arr)temp_arr = np.fix([-1.23, 1.23])print(temp_arr) [-1. 1.] [-1. 1.] 1234567891011121314# 올림temp_arr = np.ceil([-1.23789, 1.23789])print(temp_arr)# 내림temp_arr = np.floor([-1.23789, 1.23789])print(temp_arr)# 반올림temp_arr = np.around([-1.23789, 1.23789], 2)print(temp_arr)temp_arr = np.around([-1.23789, 1.23789], 4)print(temp_arr) [-1. 2.] [-2. 1.] [-1.24 1.24] [-1.2379 1.2379] 배열의 형태 및 차원 0차원부터 3차원까지 생성하는 방법 .shape : axis 축 기준으로 배열의 형태 반환 123456# 0차원 배열temp_arr = np.array(20)print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) 20 &lt;class 'numpy.ndarray'&gt; 배열의 형태: () 배열의 차원: 0 123456# 1차원 배열temp_arr = np.array([1,2,3,5])print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [1 2 3 5] &lt;class 'numpy.ndarray'&gt; 배열의 형태: (4,) 배열의 차원: 1 123456# 2차원 배열temp_arr = np.array([[1,2,3,5],[4,5,1,6],[9,0,2,3]])print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [[1 2 3 5] [4 5 1 6] [9 0 2 3]] &lt;class 'numpy.ndarray'&gt; 배열의 형태: (3, 4) 배열의 차원: 2 1234567# 3차원 배열temp_arr = np.array([[[1,2,3,5],[4,5,1,6],[9,0,2,3]], [[1,1,2,3],[2,3,1,7],[4,9,5,6]]])print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [[[1 2 3 5] [4 5 1 6] [9 0 2 3]] [[1 1 2 3] [2 3 1 7] [4 9 5 6]]] &lt;class 'numpy.ndarray'&gt; 배열의 형태: (2, 3, 4) 배열의 차원: 3 123456# parameter를 활용한 배열의 최소 차원 명시temp_arr = np.array([1,2,3,4], ndmin=2)print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [[1 2 3 4]] &lt;class 'numpy.ndarray'&gt; 배열의 형태: (1, 4) 배열의 차원: 2 배열을 생성하는 다양한 방법12345temp_arr = np.arange(5) # [0:4]print(temp_arr)temp_arr = np.arange(1,11,3) # [1:11:3]print(temp_arr) [0 1 2 3 4] [ 1 4 7 10] 1234zero_arr = np.zeros((2,3)) # 0으로만 이루어진 배열print(zero_arr)print(type(zero_arr))print(&quot;데이터 타입:&quot;, zero_arr.dtype) [[0. 0. 0.] [0. 0. 0.]] &lt;class 'numpy.ndarray'&gt; 데이터 타입: float64 123456789temp_arr = np.ones((2,3)) # 1로만 이루어진 배열print(temp_arr)print(type(temp_arr))print(&quot;데이터 타입:&quot;, temp_arr.dtype)temp_arr = np.ones((2,3), dtype=&quot;int32&quot;) # 자체적인 데이터 형변환print(temp_arr)print(type(temp_arr))print(&quot;데이터 타입:&quot;, temp_arr.dtype) [[1. 1. 1.] [1. 1. 1.]] &lt;class 'numpy.ndarray'&gt; 데이터 타입: float64 [[1 1 1] [1 1 1]] &lt;class 'numpy.ndarray'&gt; 데이터 타입: int32 12345678910111213141516temp_arr = np.ones((2,6))print(temp_arr)print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim)print(&quot;\\n&quot;)temp_res = temp_arr.reshape(3,4)print(temp_res)print(&quot;배열의 형태:&quot;, temp_res.shape)print(&quot;배열의 차원:&quot;, temp_res.ndim)print(&quot;\\n&quot;)temp_res2 = temp_arr.reshape(2,2,3)print(temp_res2)print(&quot;배열의 형태:&quot;, temp_res2.shape)print(&quot;배열의 차원:&quot;, temp_res2.ndim) [[1 1 1 1 1 1] [1 1 1 1 1 1]] 배열의 형태: (2, 6) 배열의 차원: 2 [[1 1 1 1] [1 1 1 1] [1 1 1 1]] 배열의 형태: (3, 4) 배열의 차원: 2 [[[1 1 1] [1 1 1]] [[1 1 1] [1 1 1]]] 배열의 형태: (2, 2, 3) 배열의 차원: 3 12345678temp_arr = np.ones((12,6))temp_res = temp_arr.reshape(3,-1)print(temp_res)print(&quot;배열의 형태:&quot;, temp_res.shape)print(&quot;배열의 차원:&quot;, temp_res.ndim)# temp_res2 = temp_arr.reshape(5,-1)# ValueError: cannot reshape array of size 72 into shape (5,newaxis) [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]] 배열의 형태: (3, 24) 배열의 차원: 2 NumPy 조건식 조건식이 하나일 때: np.where 12345temp_arr = np.arange(10)print(temp_arr)# 5보다 큰 값은 기존 값 * 10print(np.where(temp_arr &gt; 5, temp_arr * 10, temp_arr)) [0 1 2 3 4 5 6 7 8 9] [ 0 1 2 3 4 5 60 70 80 90] 12345temp_arr = np.arange(21)print(temp_arr)# 10보다 작은 값은 기존 값 * 10print(np.where(temp_arr &lt; 10, temp_arr * 10, temp_arr)) [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [ 0 10 20 30 40 50 60 70 80 90 10 11 12 13 14 15 16 17 18 19 20] 조건식이 2개 이상일 때: np.select 1234567temp_arr = np.arange(10)print(temp_arr)# 5보다 큰 값은 기존 값 * 2, 2보다 작은 값은 기존 값 + 100condlist = [temp_arr &gt; 5, temp_arr &lt; 2] # The list of conditionschoicelist = [temp_arr * 2, temp_arr + 100] # The list of outputsnp.select(condlist, choicelist, default = temp_arr) [0 1 2 3 4 5 6 7 8 9] array([100, 101, 2, 3, 4, 5, 12, 14, 16, 18]) NumPy Broadcasting 서로 다른 크기의 배열을 계산할 때의 기본적인 규칙 url : https://numpy.org/doc/stable/user/basics.broadcasting.html?highlight=broadcasting 1","link":"/2022/03/24/Python/Tutorial/np_tutorial/"},{"title":"Pipeline Tutorial","text":"Pipeline : 데이터 누수(Data Leakge) 방지를 위한 모델링 기법 Pycaret, MLOps (Pipeline 형태로 구축) 머신러닝 코드의 자동화 및 운영 가능 기존 방식 데이터 불러오기 -&gt; 데이터 전처리 -&gt; 특성 공학 -&gt; 데이터셋 분리 -&gt; 모델링 -&gt; 평가 파이프라인 방식 데이터 불러오기 -&gt; 데이터 전처리 -&gt; 데이터셋 분리 -&gt; 파이프라인 구축(피처공학, 모델링) -&gt; 평가 데이터 불러오기1234import pandas as pdimport numpy as npdata = pd.read_csv('https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv')data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 731 entries, 0 to 730 Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 instant 731 non-null int64 1 dteday 731 non-null object 2 season 731 non-null int64 3 yr 731 non-null int64 4 mnth 731 non-null int64 5 holiday 731 non-null int64 6 weekday 731 non-null int64 7 workingday 731 non-null int64 8 weathersit 731 non-null int64 9 temp 731 non-null float64 10 atemp 731 non-null float64 11 hum 731 non-null float64 12 windspeed 731 non-null float64 13 rentals 731 non-null int64 dtypes: float64(4), int64(9), object(1) memory usage: 80.1+ KB 12345from sklearn.model_selection import train_test_splitX = data.drop('rentals',axis=1)y = data['rentals']X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123) Pipeline 구축데이터 전처리 파이프라인12345678910111213141516171819202122232425262728293031323334353637from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoderfrom sklearn.impute import SimpleImputerfrom sklearn.compose import ColumnTransformerfrom sklearn.pipeline import Pipeline# 수치형 데이터numeric_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='mean')) ,('scaler', StandardScaler())])# 서열형 데이터ordinal_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='constant')) ,('ordEncoder', OrdinalEncoder())])# 명목형 데이터onehot_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='constant')) ,('oheEncoder', OneHotEncoder()) ])# 수치형 데이터 및 Categorical 데이터 컬럼 분리numeric_features = ['temp', 'atemp', 'hum', 'windspeed']ordinal_features = ['holiday', 'weekday', 'workingday', 'weathersit']onehot_features = ['season', 'mnth']# numeric_features = data.select_dtypes(include=['int64', 'float64']).columns# categorical_features = data.select_dtypes(include=['object']).drop(['Loan_Status'], axis=1).columnspreprocessor = ColumnTransformer( transformers=[ ('numeric', numeric_transformer, numeric_features) , ('ord_categorical', ordinal_transformer, ordinal_features) , ('ohe_categorical', onehot_transformer, onehot_features)]) 모델 적용 파이프라인123456789from sklearn.ensemble import RandomForestRegressorpipeline = Pipeline(steps = [ ('preprocessor', preprocessor) # 전처리 파이프라인 ,('regressor', RandomForestRegressor()) # 모델 연결 ])rf_model = pipeline.fit(X_train, y_train)print(rf_model) Pipeline(steps=[('preprocessor', ColumnTransformer(transformers=[('numeric', Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())]), ['temp', 'atemp', 'hum', 'windspeed']), ('ord_categorical', Pipeline(steps=[('imputer', SimpleImputer(strategy='constant')), ('ordEncoder', OrdinalEncoder())]), ['holiday', 'weekday', 'workingday', 'weathersit']), ('ohe_categorical', Pipeline(steps=[('imputer', SimpleImputer(strategy='constant')), ('oheEncoder', OneHotEncoder())]), ['season', 'mnth'])])), ('regressor', RandomForestRegressor())]) 모델 평가123from sklearn.metrics import r2_scorepredictions = rf_model.predict(X_val)print (r2_score(y_val, predictions)) 0.7654903256614782 다중 모형 개발1234567891011121314151617181920from sklearn.ensemble import RandomForestRegressorfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.linear_model import LinearRegressionregressors = [ RandomForestRegressor(), DecisionTreeRegressor(), LinearRegression()]# regressors = [pipe_rf, pipe_dt]for regressor in regressors: pipeline = Pipeline(steps = [ ('preprocessor', preprocessor) ,('regressor',regressor) ]) model = pipeline.fit(X_train, y_train) predictions = model.predict(X_val) print(regressor) print(f'Model r2 score:{r2_score(predictions, y_val)}') RandomForestRegressor() Model r2 score:0.7447806201844671 DecisionTreeRegressor() Model r2 score:0.5885371412997458 LinearRegression() Model r2 score:0.5703227526319388","link":"/2022/04/06/Python/Tutorial/Pipeline_tutorial/"},{"title":"Pandas tutorial 2","text":"라이브러리 불러오기1234import numpy as npprint(&quot;numpy: ver.&quot;, np.__version__)import pandas as pdprint(&quot;pandas: ver.&quot;, pd.__version__) numpy: ver. 1.21.5 pandas: ver. 1.3.5 데이터 불러오기12from google.colab import drivedrive.mount('/content/drive') Mounted at /content/drive 123DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Data/supermarket_sales.csv'sales = pd.read_csv(DATA_PATH)print(sales.head(3)) Invoice ID Branch City Customer type Gender \\ 0 750-67-8428 A Yangon Member Female 1 226-31-3081 C Naypyitaw Normal Female 2 631-41-3108 A Yangon Normal Male Product line Unit price Quantity Date Time Payment 0 Health and beauty 74.69 7 1/5/2019 13:08 Ewallet 1 Electronic accessories 15.28 5 3/8/2019 10:29 Cash 2 Home and lifestyle 46.33 7 3/3/2019 13:23 Credit card 1sales.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1000 entries, 0 to 999 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Invoice ID 1000 non-null object 1 Branch 1000 non-null object 2 City 1000 non-null object 3 Customer type 1000 non-null object 4 Gender 1000 non-null object 5 Product line 1000 non-null object 6 Unit price 1000 non-null float64 7 Quantity 1000 non-null int64 8 Date 1000 non-null object 9 Time 1000 non-null object 10 Payment 1000 non-null object dtypes: float64(1), int64(1), object(9) memory usage: 86.1+ KB 데이터 그룹화 groupby() 및 다양한 집계함수 활용 1sales.groupby(by=&quot;Product line&quot;)['Quantity'].count() Product line Electronic accessories 170 Fashion accessories 178 Food and beverages 174 Health and beauty 152 Home and lifestyle 160 Sports and travel 166 Name: Quantity, dtype: int64 1sales.groupby(by=&quot;Product line&quot;)['Quantity'].sum() Product line Electronic accessories 971 Fashion accessories 902 Food and beverages 952 Health and beauty 854 Home and lifestyle 911 Sports and travel 920 Name: Quantity, dtype: int64 12print(sales.groupby(by=[&quot;Branch&quot;,&quot;Customer type&quot;])['Quantity'].sum())print(type(sales.groupby(by=[&quot;Branch&quot;,&quot;Customer type&quot;])['Quantity'].sum())) # Series 객체 Branch Customer type A Member 964 Normal 895 B Member 924 Normal 896 C Member 897 Normal 934 Name: Quantity, dtype: int64 &lt;class 'pandas.core.series.Series'&gt; 12print(sales.groupby(by=[&quot;Branch&quot;,&quot;Payment&quot;], as_index=False)['Quantity'].sum())print(type(sales.groupby(by=[&quot;Branch&quot;,&quot;Payment&quot;], as_index=False)['Quantity'].sum())) # DataFrmae 객체 Branch Payment Quantity 0 A Cash 572 1 A Credit card 580 2 A Ewallet 707 3 B Cash 628 4 B Credit card 599 5 B Ewallet 593 6 C Cash 696 7 C Credit card 543 8 C Ewallet 592 &lt;class 'pandas.core.frame.DataFrame'&gt; 12print(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])['Unit price'].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]))print(type(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])['Unit price'].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]))) # DataFrame 객체 max min mean Branch Payment A Cash 99.78 10.08 56.374636 Credit card 99.56 11.94 53.011635 Ewallet 99.83 10.13 54.849762 B Cash 99.69 11.85 56.758818 Credit card 99.96 10.59 56.838991 Ewallet 99.92 10.75 53.450973 C Cash 99.96 10.17 57.100081 Credit card 99.82 10.18 53.143061 Ewallet 99.79 10.16 59.238962 &lt;class 'pandas.core.frame.DataFrame'&gt; 12print(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])['Unit price'].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]).reset_index())print(type(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])['Unit price'].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]).reset_index())) Branch Payment max min mean 0 A Cash 99.78 10.08 56.374636 1 A Credit card 99.56 11.94 53.011635 2 A Ewallet 99.83 10.13 54.849762 3 B Cash 99.69 11.85 56.758818 4 B Credit card 99.96 10.59 56.838991 5 B Ewallet 99.92 10.75 53.450973 6 C Cash 99.96 10.17 57.100081 7 C Credit card 99.82 10.18 53.143061 8 C Ewallet 99.79 10.16 59.238962 &lt;class 'pandas.core.frame.DataFrame'&gt; 결측치 처리결측치 데이터 생성1234567dict_01 = { 'Score A' : [80, 90, np.nan, 80], 'Score B' : [30, 45, np.nan, np.nan], 'Score C' : [np.nan, 50, 80, 90]}df = pd.DataFrame(dict_01)print(df) Score A Score B Score C 0 80.0 30.0 NaN 1 90.0 45.0 50.0 2 NaN NaN 80.0 3 80.0 NaN 90.0 123print(df.isnull())print(&quot;\\n&quot;)print(df.isnull().sum()) Score A Score B Score C 0 False False True 1 False False False 2 True True False 3 False True False Score A 1 Score B 2 Score C 1 dtype: int64 1234567dict_02 = { &quot;Gender&quot; : [&quot;Male&quot;, &quot;Female&quot;, np.nan, &quot;Male&quot;], &quot;Salary&quot; : [30, 45, 90, 70]}df2 = pd.DataFrame(dict_02)print(df2) Gender Salary 0 Male 30 1 Female 45 2 NaN 90 3 Male 70 123print(df.isnull())print(&quot;\\n&quot;)print(df.isnull().sum()) Score A Score B Score C 0 False False True 1 False False False 2 True True False 3 False True False Score A 1 Score B 2 Score C 1 dtype: int64 결측치 값 대체 문자열 타입과 숫자 타입의 접근 방법 상이 문자열 : 최빈값 등 숫자 : 평균, 최대값, 최소값, 중간값 등 1print(df.fillna(0)) # 0으로 대체 Score A Score B Score C 0 80.0 30.0 0.0 1 90.0 45.0 50.0 2 0.0 0.0 80.0 3 80.0 0.0 90.0 Score A float64 Score B float64 Score C float64 dtype: object 12print(df.fillna(method=&quot;pad&quot;)) # 앞 데이터로 대체# print(df.fillna(method=&quot;ffill&quot;)) : 동일한 결과 Score A Score B Score C 0 80.0 30.0 NaN 1 90.0 45.0 50.0 2 90.0 45.0 80.0 3 80.0 45.0 90.0 Score A Score B Score C 0 80.0 30.0 NaN 1 90.0 45.0 50.0 2 90.0 45.0 80.0 3 80.0 45.0 90.0 12print(df.fillna(method=&quot;backfill&quot;)) # 뒤 데이터로 대체# print(df.fillna(method=&quot;bfill&quot;)) : 동일한 결과 Score A Score B Score C 0 80.0 30.0 50.0 1 90.0 45.0 50.0 2 80.0 NaN 80.0 3 80.0 NaN 90.0 1print(df2['Gender'].fillna(&quot;Genderless&quot;)) # 특정 문자열로 대체 0 Male 1 Female 2 Genderless 3 Male Name: Gender, dtype: object 결측치가 있는 행, 열 제거12345678dict_03 = { 'Score A' : [80, 90, np.nan, 80], 'Score B' : [30, 45, np.nan, np.nan], 'Score C' : [np.nan, 50, 80, 90], 'Score D' : [50, 30, 80, 60]}df3 = pd.DataFrame(dict_03)print(df3) Score A Score B Score C Score D 0 80.0 30.0 NaN 50 1 90.0 45.0 50.0 30 2 NaN NaN 80.0 80 3 80.0 NaN 90.0 60 123print(df3.dropna()) # axis: default 0print(&quot;\\n&quot;)print(df3.dropna(axis=1)) Score A Score B Score C Score D 1 90.0 45.0 50.0 30 Score D 0 50 1 30 2 80 3 60 이상치 탐지 일반적으로 IQR(= Q3 - Q1; 사분위수범위)를 활용하여 탐지 하한 경계값 : Q1 - IQR * 1.5 상한 경계값 : Q3 + IQR * 1.5 Box Plot으로도 확인 가능 실무에서는 각 도메인(비즈니스 영역)별로 기준 상이 1print(sales[['Unit price']].describe()) Unit price count 1000.000000 mean 55.672130 std 26.494628 min 10.080000 25% 32.875000 50% 55.230000 75% 77.935000 max 99.960000 1234567891011q1 = sales['Unit price'].quantile(0.25)q3 = sales['Unit price'].quantile(0.75)iqr = q3 - q1lim_q1 = q1 - 1.5 * IQRlim_q3 = q3 + 1.5 * IQRprint(tuple([lim_q1, lim_q3]))print(&quot;\\n&quot;)out_q1 = (sales['Unit price'] &lt; lim_q1)out_q3 = (sales['Unit price'] &gt; lim_q3)outliers = (sales['Unit price'][out_q1 | out_q3])print(outliers) (-34.715, 145.525) Series([], Name: Unit price, dtype: float64) 12import matplotlib.pyplot as pltplt.boxplot(sales['Unit price']) {'boxes': [&lt;matplotlib.lines.Line2D at 0x7fefce5f93d0&gt;], 'caps': [&lt;matplotlib.lines.Line2D at 0x7fefce5fe3d0&gt;, &lt;matplotlib.lines.Line2D at 0x7fefce5fe910&gt;], 'fliers': [&lt;matplotlib.lines.Line2D at 0x7fefce605410&gt;], 'means': [], 'medians': [&lt;matplotlib.lines.Line2D at 0x7fefce5fee90&gt;], 'whiskers': [&lt;matplotlib.lines.Line2D at 0x7fefce5f9910&gt;, &lt;matplotlib.lines.Line2D at 0x7fefce5f9e50&gt;]}","link":"/2022/03/24/Python/Tutorial/pd_tutorial_02/"},{"title":"Visualization tutorial","text":"데이터 시각화의 기본 조건 목적에 맞는 그래프 선정 선형 그래프, 막대 그래프, 산점도, 박스플롯 등등 환경에 맞는 도구 선택 코드 기반 : R, Python 프로그램 기반 : Excel, PowerBI, Tableau 등등 문맥(도메인)에 맞는 색과 도형 사용 파이썬 시각화 라이브러리Matplotlib 정형 데이터 / 이미지 데이터 Pyplot API : Pyplot 모듈에 있는 함수를 각각 불러와서 구현 사용하기 편리하나, 세부 옵션 조정이 어려움 객체지향 API : Matplotlib에 구현된 객체지향 라이브러리를 직접 활용 라이브러리가 늘어나고, 코드가 복잡함 그래프의 디테일한 세부 옵션 조정이 용이함 일반적으로 두 API를 혼합하여 사용 Seaborn Matplotlib에 종속된 라이브러리 Matplotlib에 비해 코드가 간결함 통계 그래프 구현이 보다 용이 세부적인 옵션은 Matplotlib에서 조정 라이브러리 불러오기1234import matplotlibimport seaborn as snsprint(&quot;matplotlib ver :&quot;, matplotlib.__version__)print(&quot;seaborn ver :&quot;, sns.__version__) matplotlib ver : 3.2.2 seaborn ver : 0.11.2 시각화 테스트12345678910111213141516import matplotlib.pyplot as pltdates = [ '2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-01-09', '2021-01-10']min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0]# 파이썬 시각화 수행 전 기본 설정 (숫자는 변경 가능)fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,6))ax.plot(dates, min_temperature, label = &quot;Min Temp.&quot;)ax.plot(dates, max_temperature, label = &quot;Max Temp.&quot;)ax.legend()plt.show() 주식 데이터 예제1!pip install yfinance --upgrade --no-cache-dir Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.70) Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.5) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5) Requirement already satisfied: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10) Requirement already satisfied: lxml&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.8.0) Requirement already satisfied: requests&gt;=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24.0-&gt;yfinance) (1.15.0) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (2.0.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (2021.10.8) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (1.24.3) 12345import yfinance as yfdata = yf.download(&quot;AAPL&quot;, start=&quot;2019-08-01&quot;, end=&quot;2022-03-23&quot;)ts = data['Open']print(ts.head())print(type(ts)) [*********************100%***********************] 1 of 1 completed Date 2019-08-01 53.474998 2019-08-02 51.382500 2019-08-05 49.497501 2019-08-06 49.077499 2019-08-07 48.852501 Name: Open, dtype: float64 &lt;class 'pandas.core.series.Series'&gt; pyplot 모듈1234567import matplotlib.pyplot as pltplt.plot(ts)plt.title(&quot;Stock Market of AAPL&quot;)plt.xlabel(&quot;Date&quot;)plt.ylabel(&quot;Open Price&quot;)plt.show() 객체지향 라이브러리123456789import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(ts)ax.set_title(&quot;Stock Market of AAPL&quot;)ax.set_xlabel(&quot;Date&quot;)ax.set_ylabel(&quot;Open Price&quot;)plt.show() 막대 그래프Matplotlib12345678910111213141516171819202122import matplotlib.pyplot as pltimport numpy as npimport calendarmonth_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450]fig, ax = plt.subplots(figsize=(10,6))barplots = ax.bar(month_list, sold_list)print(&quot;barplots :&quot;, barplots) # artists 레이어에 12개의 막대가 저장됨for plot in barplots: print(plot) # print(plot.get_x()) # print(plot.get_y()) # print(plot.get_width()) # print(&quot;height:&quot;, plot.get_height()) height = plot.get_height() ax.text(plot.get_x() + plot.get_width()/2, height, height, ha = 'center', va = 'bottom')plt.xticks(month_list, calendar.month_name[1:13], rotation=30) # x축plt.show() barplots : &lt;BarContainer object of 12 artists&gt; Rectangle(xy=(0.6, 0), width=0.8, height=300, angle=0) Rectangle(xy=(1.6, 0), width=0.8, height=400, angle=0) Rectangle(xy=(2.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(3.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(4.6, 0), width=0.8, height=600, angle=0) Rectangle(xy=(5.6, 0), width=0.8, height=960, angle=0) Rectangle(xy=(6.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(7.6, 0), width=0.8, height=910, angle=0) Rectangle(xy=(8.6, 0), width=0.8, height=800, angle=0) Rectangle(xy=(9.6, 0), width=0.8, height=700, angle=0) Rectangle(xy=(10.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(11.6, 0), width=0.8, height=450, angle=0) Seaborn123fig, ax = plt.subplots()sns.countplot(x=&quot;day&quot;, data=tips)plt.show() 1234print(tips['day'].value_counts().index)print(tips['day'].value_counts().values)print()print(tips['day'].value_counts(ascending=True)) CategoricalIndex(['Sat', 'Sun', 'Thur', 'Fri'], categories=['Thur', 'Fri', 'Sat', 'Sun'], ordered=False, dtype='category') [87 76 62 19] Fri 19 Thur 62 Sun 76 Sat 87 Name: day, dtype: int64 12345678910fig, ax = plt.subplots()ax = sns.countplot(x=&quot;day&quot;, data=tips, order=tips['day'].value_counts().index, alpha=0.5)for plot in ax.patches: print(plot) height = plot.get_height() ax.text(plot.get_x() + plot.get_width()/2, height, height, ha = 'center', va = 'bottom')ax.set_ylim(-1, 100)plt.show() Rectangle(xy=(-0.4, 0), width=0.8, height=87, angle=0) Rectangle(xy=(0.6, 0), width=0.8, height=76, angle=0) Rectangle(xy=(1.6, 0), width=0.8, height=62, angle=0) Rectangle(xy=(2.6, 0), width=0.8, height=19, angle=0) 산점도Matplotlib123456789101112131415import seaborn as snstips = sns.load_dataset(&quot;tips&quot;)print(tips.info())x = tips['total_bill']y = tips['tip']fig, ax = plt.subplots(figsize=(10,6))ax.scatter(x,y)ax.set_title('Scatter of tips')ax.set_xlabel('Total Bill')ax.set_ylabel('Tip')plt.show() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 244 entries, 0 to 243 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 total_bill 244 non-null float64 1 tip 244 non-null float64 2 sex 244 non-null category 3 smoker 244 non-null category 4 day 244 non-null category 5 time 244 non-null category 6 size 244 non-null int64 dtypes: category(4), float64(2), int64(1) memory usage: 7.4 KB None 12345678910tips['sex_color'] = tips['sex'].map({'Male':'#4663F5', 'Female':'#FF5F2E'})fig, ax = plt.subplots(figsize=(10,6))for label, data in tips.groupby('sex'): ax.scatter(data['total_bill'], data['tip'], label=label, color=data['sex_color'], alpha=0.5) ax.set_xlabel('Total Bill') ax.set_ylabel('Tip')ax.legend()plt.show() Seaborn12345678910import matplotlib.pyplot as pltimport seaborn as snstips = sns.load_dataset(&quot;tips&quot;)print(tips.info())fig, ax = plt.subplots(figsize=(10,6))sns.scatterplot(x='total_bill', y='tip', hue='sex', data=tips)ax.legend()plt.show() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 244 entries, 0 to 243 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 total_bill 244 non-null float64 1 tip 244 non-null float64 2 sex 244 non-null category 3 smoker 244 non-null category 4 day 244 non-null category 5 time 244 non-null category 6 size 244 non-null int64 dtypes: category(4), float64(2), int64(1) memory usage: 7.4 KB None 두 개의 그래프를 동시에 표현123456fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))sns.regplot(x='total_bill', y='tip', data=tips, ax=ax[0], fit_reg=True)ax[0].set_title(&quot;Scatterplot with Regression Line&quot;)sns.regplot(x='total_bill', y='tip', data=tips, ax=ax[1], fit_reg=False)ax[1].set_title(&quot;Scatterplot without Regression Line&quot;)plt.show() 종합 예제123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import matplotlib.pyplot as pltimport seaborn as snsimport numpy as npfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FuncFormatter)def major_formatter(x, pos): return &quot;$ %.2f&quot; % xformatter = FuncFormatter(major_formatter)tips = sns.load_dataset(&quot;tips&quot;)fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,6))### 왼쪽 막대 그래프ax0 = sns.barplot(x=&quot;day&quot;, y='total_bill', data=tips, ax=ax[0], ci=None, alpha=0.85)# 텍스트 입력for p in ax0.patches: height = np.round(p.get_height(), 2) ax0.text(p.get_x() + p.get_width()/2., height+1, height, ha = 'center', size=12)# 축 범위 및 제목 수정ax0.set_ylim(-3, 30)ax0.set_title(&quot;Basic Bar Graph&quot;)### 왼쪽 막대 그래프ax1 = sns.barplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, data=tips, ax=ax[1], ci=None, color='lightgray', alpha=0.85, zorder=2)# total_bill 평균이 가장 큰 요일group_mean = tips.groupby(['day'])['total_bill'].agg('mean')h_day = group_mean.sort_values(ascending=False).index[0] # sundayh_mean = group_mean.sort_values(ascending=False).values[0] # 21.41# 막대별 옵션 설정for plot in ax1.patches: height = np.round(plot.get_height(), 2) # 기본 설정 fontweight = &quot;normal&quot; color = &quot;k&quot; # 조건 설정 if h_mean == height: fontweight = &quot;bold&quot; color = &quot;darkred&quot; plot.set_facecolor(color) plot.set_edgecolor(&quot;black&quot;) # 텍스트 입력 ax1.text(plot.get_x() + plot.get_width()/2., height+1, height, ha ='center', size=12, fontweight=fontweight, color=color)# spines 제거ax1.spines['top'].set_visible(False)ax1.spines['left'].set_position((&quot;outward&quot;, 20))ax1.spines['left'].set_visible(False)ax1.spines['right'].set_visible(False)ax1.spines['bottom'].set_visible(False)# 축 범위 및 제목 수정ax1.set_ylim(-1, 30)ax1.set_title(&quot;Ideal Bar Graph&quot;, size=16)ax1.yaxis.set_major_locator(MultipleLocator(10))ax1.yaxis.set_major_formatter(formatter)ax1.yaxis.set_minor_locator(MultipleLocator(5))# 축 레이블 수정ax1.set_ylabel(&quot;Avg. Total Bill($)&quot;, fontsize=14)ax1.set_xlabel(&quot;Weekday&quot;, fontsize=14)# 그리드ax1.grid(axis=&quot;y&quot;, which=&quot;major&quot;, color=&quot;lightgray&quot;)ax1.grid(axis=&quot;y&quot;, which=&quot;minor&quot;, ls=&quot;:&quot;)# 축 범주 수정for xtick in ax1.get_xticklabels(): if xtick.get_text() == h_day: xtick.set_color(&quot;darkred&quot;) xtick.set_fontweight(&quot;demibold&quot;)ax1.set_xticklabels(['Thursday', 'Friday', 'Saturday', 'Sunday'], size=12)plt.show()","link":"/2022/03/24/Python/Tutorial/visual_tutorial_01/"},{"title":"Pandas tutorial 1","text":"데이터 전처리 프로세스 중복값 제거 및 결측치 처리 완전 무작위(MCAR) / 무작위(MAR) / 비무작위(NMAR) 제거, 치환, 모델 기반 처리 등 이상치 탐지 및 처리 삭제, 대체, 변환(스케일링) 등 Feature Engineering 정규화, 표준화, 로그변환, 벡터화 등 PCA, EFA 등을 통한 차원 축소 Pandas 라이브러리Pandas의 기본 자료형 Series 객체, DataFrame 객체 Index: 숫자 또는 문자, 중복X Series: Index &amp; Column 1개 DataFrame: Index &amp; Column 2개 이상 각 객체에 따라 사용 가능한 method가 상이함 Pandas 라이브러리 불러오기12import pandas as pdprint(pd.__version__) 1.3.5 테스트12345# DataFrame 객체temp_dic = {'col1' : [1, 2, 3], 'col2' : [4, 5, 6]}df = pd.DataFrame(temp_dic)print(df)print(type(df)) col1 col2 0 1 4 1 2 5 2 3 6 &lt;class 'pandas.core.frame.DataFrame'&gt; 12345# Series 객체temp_dic = {'a':1, 'b':2, 'c':3}ser = pd.Series(temp_dic)print(ser)print(type(ser)) a 1 b 2 c 3 dtype: int64 &lt;class 'pandas.core.series.Series'&gt; 구글 드라이브 연동12from google.colab import drivedrive.mount('/content/drive') Mounted at /content/drive 데이터 불러오기123DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/Data/Lemonade2016.csv'juice = pd.read_csv(DATA_PATH)print(juice) Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 5 7/6/2016 Beach 103 69 82 90.0 0.25 6 7/6/2016 Beach 103 69 82 90.0 0.25 7 7/7/2016 Beach 143 101 81 135.0 0.25 8 NaN Beach 123 86 82 113.0 0.25 9 7/9/2016 Beach 134 95 80 126.0 0.25 10 7/10/2016 Beach 140 98 82 131.0 0.25 11 7/11/2016 Beach 162 120 83 135.0 0.25 12 7/12/2016 Beach 130 95 84 99.0 0.25 13 7/13/2016 Beach 109 75 77 99.0 0.25 14 7/14/2016 Beach 122 85 78 113.0 0.25 15 7/15/2016 Beach 98 62 75 108.0 0.50 16 7/16/2016 Beach 81 50 74 90.0 0.50 17 7/17/2016 Beach 115 76 77 126.0 0.50 18 7/18/2016 Park 131 92 81 122.0 0.50 19 7/19/2016 Park 122 85 78 113.0 0.50 20 7/20/2016 Park 71 42 70 NaN 0.50 21 7/21/2016 Park 83 50 77 90.0 0.50 22 7/22/2016 Park 112 75 80 108.0 0.50 23 7/23/2016 Park 120 82 81 117.0 0.50 24 7/24/2016 Park 121 82 82 117.0 0.50 25 7/25/2016 Park 156 113 84 135.0 0.50 26 7/26/2016 Park 176 129 83 158.0 0.35 27 7/27/2016 Park 104 68 80 99.0 0.35 28 7/28/2016 Park 96 63 82 90.0 0.35 29 7/29/2016 Park 100 66 81 95.0 0.35 30 7/30/2016 Beach 88 57 82 81.0 0.35 31 7/31/2016 Beach 76 47 82 68.0 0.35 12# 전체적인 구조, 결측치 개수, 데이터 타입 파악juice.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 123print(juice.head())print(&quot;-------------------------------------------------------------------&quot;)print(juice.tail()) Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 ------------------------------------------------------------------- Date Location Lemon Orange Temperature Leaflets Price 27 7/27/2016 Park 104 68 80 99.0 0.35 28 7/28/2016 Park 96 63 82 90.0 0.35 29 7/29/2016 Park 100 66 81 95.0 0.35 30 7/30/2016 Beach 88 57 82 81.0 0.35 31 7/31/2016 Beach 76 47 82 68.0 0.35 describe() : 기술통계량 확인 (int형, float형 변수) value_counts() : 범주형 변수 빈도 수 확인 12print(juice.describe())print(&quot;** type(juice.describe()) :&quot;, type(juice.describe())) # DataFrame 객체로 반환 Lemon Orange Temperature Leaflets Price count 32.000000 32.000000 32.000000 31.000000 32.000000 mean 116.156250 80.000000 78.968750 108.548387 0.354687 std 25.823357 21.863211 4.067847 20.117718 0.113137 min 71.000000 42.000000 70.000000 68.000000 0.250000 25% 98.000000 66.750000 77.000000 90.000000 0.250000 50% 113.500000 76.500000 80.500000 108.000000 0.350000 75% 131.750000 95.000000 82.000000 124.000000 0.500000 max 176.000000 129.000000 84.000000 158.000000 0.500000 ** type(juice.describe()) : &lt;class 'pandas.core.frame.DataFrame'&gt; 12print(juice['Location'].value_counts())print(&quot;** type(juice.describe()) :&quot;, type(juice['Location'].value_counts())) # Series 객체로 반환 Beach 17 Park 15 Name: Location, dtype: int64 ** type(juice.describe()) : &lt;class 'pandas.core.series.Series'&gt; 데이터 다루기12juice['Sold'] = 0juice.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 0 1 7/2/2016 Park 98 67 72 90.0 0.25 0 2 7/3/2016 Park 110 77 71 104.0 0.25 0 3 7/4/2016 Beach 134 99 76 98.0 0.25 0 4 7/5/2016 Beach 159 118 78 135.0 0.25 0 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-d7644e91-b826-4359-8387-1580b5266658 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-d7644e91-b826-4359-8387-1580b5266658'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12juice['Sold'] = juice['Lemon'] + juice['Orange']juice.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 2 7/3/2016 Park 110 77 71 104.0 0.25 187 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-2d21cec2-5539-4100-b9f5-ac71828a5d25 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-2d21cec2-5539-4100-b9f5-ac71828a5d25'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 매출액(Revenue) = 가격(Price) * 판매량(Sold) 12juice['Revenue'] = juice['Price'] * juice['Sold']juice.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-506a14b7-9d00-4793-9a3f-5c54252c47b5 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-506a14b7-9d00-4793-9a3f-5c54252c47b5'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 행 또는 열 제거 : drop(axis=0|1) axis=0 : 행 방향(index) 실행 axis=1 : 열 방향(column) 실행 12juice_col_drop = juice.drop('Sold', axis=1)juice_col_drop.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 46.75 3 7/4/2016 Beach 134 99 76 98.0 0.25 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 69.25 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-4ee85edb-efbb-47d3-84c6-8b0cab09eb0f button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4ee85edb-efbb-47d3-84c6-8b0cab09eb0f'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12juice_ind_drop = juice.drop(2, axis=0)juice_ind_drop.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-8329c7e5-dc41-41e9-aa8f-d31cdc115b69 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-8329c7e5-dc41-41e9-aa8f-d31cdc115b69'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 데이터 인덱싱1juice[5:10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 8 NaN Beach 123 86 82 113.0 0.25 209 52.25 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-e5a46c70-33e1-4ae6-868e-0fbcf1b0c121 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-e5a46c70-33e1-4ae6-868e-0fbcf1b0c121'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; boolean 값 활용 (조건식)12# Location 값이 Park인 경우juice[juice['Location'] == 'Park'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 19 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 20 7/20/2016 Park 71 42 70 NaN 0.50 113 56.50 21 7/21/2016 Park 83 50 77 90.0 0.50 133 66.50 22 7/22/2016 Park 112 75 80 108.0 0.50 187 93.50 23 7/23/2016 Park 120 82 81 117.0 0.50 202 101.00 24 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 27 7/27/2016 Park 104 68 80 99.0 0.35 172 60.20 28 7/28/2016 Park 96 63 82 90.0 0.35 159 55.65 29 7/29/2016 Park 100 66 81 95.0 0.35 166 58.10 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-6440b49e-9b42-4a3b-9edd-37f9b7d5b0a6 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-6440b49e-9b42-4a3b-9edd-37f9b7d5b0a6'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12# Leaflets 값이 120 이상인 경우juice[juice['Leaflets'] &gt;= 120] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 59.50 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 70.50 17 7/17/2016 Beach 115 76 77 126.0 0.50 191 95.50 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-92086f96-d263-4c70-85ec-af04ffb445c1 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-92086f96-d263-4c70-85ec-af04ffb445c1'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; iloc vs loc iloc : index 기반, 속도↑, 대용량 데이터에 적합 syntax : df.iloc[row_index, column_index] loc : label or boolean(조건식) 기반, 가독성↑ syntax : df.loc[row_label, column_label] 12%%timejuice.iloc[0:3, 0:2] # 해당 인덱스 미포함 CPU times: user 514 µs, sys: 40 µs, total: 554 µs Wall time: 523 µs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-219659e3-19a4-418f-84d6-7d01a8076e00 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-219659e3-19a4-418f-84d6-7d01a8076e00'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12%%timejuice.loc[0:2, [&quot;Date&quot;,'Location']] # 해당 라벨명 포함 CPU times: user 1.67 ms, sys: 0 ns, total: 1.67 ms Wall time: 5.19 ms .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-5e3530fd-7c31-4a6f-8a4f-dea8d6d42e78 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-5e3530fd-7c31-4a6f-8a4f-dea8d6d42e78'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1234# juice.iloc[juice['Leaflets'] &gt;= 130, ['Date', 'Location', 'Leaflets']]# Error: iLocation based boolean indexing on an integer type is not availablejuice.loc[juice['Leaflets'] &gt;= 130, ['Date', 'Location', 'Leaflets']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Leaflets 4 7/5/2016 Beach 135.0 7 7/7/2016 Beach 135.0 10 7/10/2016 Beach 131.0 11 7/11/2016 Beach 135.0 25 7/25/2016 Park 135.0 26 7/26/2016 Park 158.0 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-80998d16-2836-479c-9ea3-75efcac52299 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-80998d16-2836-479c-9ea3-75efcac52299'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 데이터 정렬 sort_values() 함수 12# Revenue 기준 오름차순juice.sort_values(by=['Revenue']).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 43.05 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-4d6ed323-d5df-47ed-893d-db57c2b39110 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4d6ed323-d5df-47ed-893d-db57c2b39110'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12# Revenue 기준 내림차순juice.sort_values(by=['Revenue'], ascending = False).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 19 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 24 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-ecf0c228-c2c7-4d62-aa08-e194d9fd1e4e button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-ecf0c228-c2c7-4d62-aa08-e194d9fd1e4e'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12# Price 기준 내림차순, Temperature 기준 오름차순juice.sort_values(by=['Price', 'Temperature'], ascending = [False, True]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 20 7/20/2016 Park 71 42 70 NaN 0.50 113 56.50 16 7/16/2016 Beach 81 50 74 90.0 0.50 131 65.50 15 7/15/2016 Beach 98 62 75 108.0 0.50 160 80.00 17 7/17/2016 Beach 115 76 77 126.0 0.50 191 95.50 21 7/21/2016 Park 83 50 77 90.0 0.50 133 66.50 19 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 22 7/22/2016 Park 112 75 80 108.0 0.50 187 93.50 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 23 7/23/2016 Park 120 82 81 117.0 0.50 202 101.00 24 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 27 7/27/2016 Park 104 68 80 99.0 0.35 172 60.20 29 7/29/2016 Park 100 66 81 95.0 0.35 166 58.10 28 7/28/2016 Park 96 63 82 90.0 0.35 159 55.65 30 7/30/2016 Beach 88 57 82 81.0 0.35 145 50.75 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 43.05 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 2 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 13 7/13/2016 Beach 109 75 77 99.0 0.25 184 46.00 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 14 7/14/2016 Beach 122 85 78 113.0 0.25 207 51.75 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 8 NaN Beach 123 86 82 113.0 0.25 209 52.25 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 59.50 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 70.50 12 7/12/2016 Beach 130 95 84 99.0 0.25 225 56.25 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-56042f49-547e-4908-99b7-0e79a9445a3d button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-56042f49-547e-4908-99b7-0e79a9445a3d'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 123# index를 새로 지정해서 새로운 객체로 저장juice2 = juice.sort_values(by=['Price', 'Temperature'], ascending = [False, True]).reset_index(drop=True)juice2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/20/2016 Park 71 42 70 NaN 0.50 113 56.50 1 7/16/2016 Beach 81 50 74 90.0 0.50 131 65.50 2 7/15/2016 Beach 98 62 75 108.0 0.50 160 80.00 3 7/17/2016 Beach 115 76 77 126.0 0.50 191 95.50 4 7/21/2016 Park 83 50 77 90.0 0.50 133 66.50 5 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 6 7/22/2016 Park 112 75 80 108.0 0.50 187 93.50 7 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 8 7/23/2016 Park 120 82 81 117.0 0.50 202 101.00 9 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 10 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 11 7/27/2016 Park 104 68 80 99.0 0.35 172 60.20 12 7/29/2016 Park 100 66 81 95.0 0.35 166 58.10 13 7/28/2016 Park 96 63 82 90.0 0.35 159 55.65 14 7/30/2016 Beach 88 57 82 81.0 0.35 145 50.75 15 7/31/2016 Beach 76 47 82 68.0 0.35 123 43.05 16 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 17 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 18 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 19 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 20 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 21 7/13/2016 Beach 109 75 77 99.0 0.25 184 46.00 22 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 23 7/14/2016 Beach 122 85 78 113.0 0.25 207 51.75 24 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 25 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 26 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 27 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 28 NaN Beach 123 86 82 113.0 0.25 209 52.25 29 7/10/2016 Beach 140 98 82 131.0 0.25 238 59.50 30 7/11/2016 Beach 162 120 83 135.0 0.25 282 70.50 31 7/12/2016 Beach 130 95 84 99.0 0.25 225 56.25 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-95b0ec19-306b-4312-a2fb-6c6bf238bb16 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-95b0ec19-306b-4312-a2fb-6c6bf238bb16'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 데이터 그룹화 groupby() 함수 그룹별 집계함수를 통해 피벗테이블 생성 1juice.groupby(by='Location').count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Lemon Orange Temperature Leaflets Price Sold Revenue Location Beach 16 17 17 17 17 17 17 17 Park 15 15 15 15 14 15 15 15 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-464f987e-371e-4805-8b50-ee52bdc321e8 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-464f987e-371e-4805-8b50-ee52bdc321e8'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1234import numpy as np# Location 그룹별 Lemon, Orange 변수에 대한 집계함수juice.groupby(by='Location')[['Lemon','Orange']].agg([max, min, sum, np.mean]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Lemon Orange max min sum mean max min sum mean Location Beach 162 76 2020 118.823529 120 47 1402 82.470588 Park 176 71 1697 113.133333 129 42 1158 77.200000 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-1d771570-530b-4471-945e-e1db8f8b3457 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-1d771570-530b-4471-945e-e1db8f8b3457'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt;","link":"/2022/03/24/Python/Tutorial/pd_tutorial_01/"},{"title":"Git Installation in Windows11","text":"Git 설치파일 다운로드 git-scm.com 에서 Downloads 클릭 현재 사용 중인 운영체제(Windows) 클릭 현재 사용 중인 시스템 아키텍처(64비트)에 해당하는 링크를 클릭하여 설치 파일 다운 Git Setup 마법사 실행 다운로드받은 Git Setup 파일을 실행 설치하기 위한 경로 지정 후 Next 클릭 설치할 구성요소 선택 후 Next 클릭 일반적으로 기본 상태 그대로 진행해도 무관 Additional icons On the Desktop : 바탕화면에 바로가기 아이콘 추가 Windows Explorer integration Git Bash Here : Git Bash 연결 기능 Git GUI Here : Git GUI 연결 기능 Git LFS ( Large File Support) : 대용량 파일 지원 여부 Associate .git* configuration files with the default text editor : Git 구성 파일을 기본 텍스트 편집기와 연결할지 여부 Associate .sh files to be run with Bash : .sh 확장자 파일을 Bash와 연결할지 선택 Check daily for Git for Windows updates : Git 업데이트를 매일 체크할지 여부 Add a Git Bash Profile to Windows Terminal : 윈도우 터미널에 Git Bash 추가할지 여부 시작 폴더 경로 지정 후 Next 클릭 기본 Git 에디터 선택 후 Next 클릭 기본 옵션은 Vim 편집기이며, Notepad, VSCode, Sublime 등등 선택 가능 Branch 이름 지정 옵션 선택 후 Next 클릭 Let Git decide : 기본적으로 master로 지정, 추후 변경 가능 Override the default branch name for new repositories : 입력한 이름으로 자동 지정 현재 대부분의 경우 main으로 통용되고 있음 이후 옵션들은 별도 지정이나 변경 없이 넘어가고, 마지막 Install 시 설치 진행 모든 설치가 완료된 후 Finish 클릭 Git Bash 사용자 정보 입력 Git Bash 실행 후 사용자 정보 등록 사용자 정보를 등록하면 로컬에서 Git 커밋 시 항상 이 정보가 사용됨 12git config --global user.name &quot;Name&quot;git config --global user.email &quot;Email&quot; .gitconfig에 저장되어 있는 설정 값 확인 : cat ~/.gitconfig Ref.https://iboxcomein.com/windows-git-install/","link":"/2022/10/17/Setting/Git%20Installation%20in%20Windows11/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"setting","slug":"setting","link":"/tags/setting/"},{"name":"data engineering","slug":"data-engineering","link":"/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","link":"/tags/wsl2/"},{"name":"apache","slug":"apache","link":"/tags/apache/"},{"name":"airflow","slug":"airflow","link":"/tags/airflow/"},{"name":"vscode","slug":"vscode","link":"/tags/vscode/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"kibana","slug":"kibana","link":"/tags/kibana/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"development","slug":"development","link":"/tags/development/"},{"name":"r","slug":"r","link":"/tags/r/"},{"name":"statistic","slug":"statistic","link":"/tags/statistic/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"Oracle","slug":"Oracle","link":"/tags/Oracle/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"google colab","slug":"google-colab","link":"/tags/google-colab/"},{"name":"crawling","slug":"crawling","link":"/tags/crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","link":"/tags/BeautifulSoup/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"numpy","slug":"numpy","link":"/tags/numpy/"},{"name":"pipeline","slug":"pipeline","link":"/tags/pipeline/"},{"name":"visualization","slug":"visualization","link":"/tags/visualization/"},{"name":"matplotlib","slug":"matplotlib","link":"/tags/matplotlib/"},{"name":"seaborn","slug":"seaborn","link":"/tags/seaborn/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"windows11","slug":"windows11","link":"/tags/windows11/"}],"categories":[{"name":"hexo","slug":"hexo","link":"/categories/hexo/"},{"name":"setting","slug":"setting","link":"/categories/setting/"},{"name":"r","slug":"r","link":"/categories/r/"},{"name":"sql","slug":"sql","link":"/categories/sql/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"Oracle","slug":"sql/Oracle","link":"/categories/sql/Oracle/"},{"name":"ML","slug":"python/ML","link":"/categories/python/ML/"},{"name":"tutorial","slug":"python/tutorial","link":"/categories/python/tutorial/"},{"name":"crawling","slug":"python/crawling","link":"/categories/python/crawling/"}],"pages":[{"title":"","text":"Sample // Pandoc 2.9 adds attributes on both header and div. We remove the former (to // be compatible with the behavior of Pandoc < 2.8). document.addEventListener('DOMContentLoaded', function(e) { var hs = document.querySelectorAll(\"div.section[class*='level'] > :first-child\"); var i, h, a; for (i = 0; i < hs.length; i++) { h = hs[i]; if (!/^h[1-6]$/i.test(h.tagName)) continue; // it should be a header h1-h6 a = h.attributes; while (a.length > 0) h.removeAttribute(a[0].name); } }); /*! jQuery v3.6.0 | (c) OpenJS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(C,e){\"use strict\";var t=[],r=Object.getPrototypeOf,s=t.slice,g=t.flat?function(e){return t.flat.call(e)}:function(e){return t.concat.apply([],e)},u=t.push,i=t.indexOf,n={},o=n.toString,v=n.hasOwnProperty,a=v.toString,l=a.call(Object),y={},m=function(e){return\"function\"==typeof e&&\"number\"!=typeof e.nodeType&&\"function\"!=typeof e.item},x=function(e){return null!=e&&e===e.window},E=C.document,c={type:!0,src:!0,nonce:!0,noModule:!0};function b(e,t,n){var r,i,o=(n=n||E).createElement(\"script\");if(o.text=e,t)for(r in c)(i=t[r]||t.getAttribute&&t.getAttribute(r))&&o.setAttribute(r,i);n.head.appendChild(o).parentNode.removeChild(o)}function w(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?n[o.call(e)]||\"object\":typeof e}var f=\"3.6.0\",S=function(e,t){return new S.fn.init(e,t)};function p(e){var t=!!e&&\"length\"in e&&e.length,n=w(e);return!m(e)&&!x(e)&&(\"array\"===n||0===t||\"number\"==typeof t&&0","link":"/images/R/R_sample.html"},{"title":"","text":"R_basic_statistics // Pandoc 2.9 adds attributes on both header and div. We remove the former (to // be compatible with the behavior of Pandoc < 2.8). document.addEventListener('DOMContentLoaded', function(e) { var hs = document.querySelectorAll(\"div.section[class*='level'] > :first-child\"); var i, h, a; for (i = 0; i < hs.length; i++) { h = hs[i]; if (!/^h[1-6]$/i.test(h.tagName)) continue; // it should be a header h1-h6 a = h.attributes; while (a.length > 0) h.removeAttribute(a[0].name); } }); /*! jQuery v3.6.0 | (c) OpenJS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(C,e){\"use strict\";var t=[],r=Object.getPrototypeOf,s=t.slice,g=t.flat?function(e){return t.flat.call(e)}:function(e){return t.concat.apply([],e)},u=t.push,i=t.indexOf,n={},o=n.toString,v=n.hasOwnProperty,a=v.toString,l=a.call(Object),y={},m=function(e){return\"function\"==typeof e&&\"number\"!=typeof e.nodeType&&\"function\"!=typeof e.item},x=function(e){return null!=e&&e===e.window},E=C.document,c={type:!0,src:!0,nonce:!0,noModule:!0};function b(e,t,n){var r,i,o=(n=n||E).createElement(\"script\");if(o.text=e,t)for(r in c)(i=t[r]||t.getAttribute&&t.getAttribute(r))&&o.setAttribute(r,i);n.head.appendChild(o).parentNode.removeChild(o)}function w(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?n[o.call(e)]||\"object\":typeof e}var f=\"3.6.0\",S=function(e,t){return new S.fn.init(e,t)};function p(e){var t=!!e&&\"length\"in e&&e.length,n=w(e);return!m(e)&&!x(e)&&(\"array\"===n||0===t||\"number\"==typeof t&&0","link":"/images/R/R_basic_stat.html"}]}