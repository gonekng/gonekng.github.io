{"meta":{"title":"Jiwon's Blog","subtitle":"","description":"","author":"Jiwon Kang","url":"http://gonekng.github.io","root":"/"},"pages":[{"title":"","date":"2022-10-05T05:40:07.922Z","updated":"2022-10-05T05:40:07.922Z","comments":true,"path":"images/R/R_sample.html","permalink":"http://gonekng.github.io/images/R/R_sample.html","excerpt":"","text":"Sample // Pandoc 2.9 adds attributes on both header and div. We remove the former (to // be compatible with the behavior of Pandoc < 2.8). document.addEventListener('DOMContentLoaded', function(e) { var hs = document.querySelectorAll(\"div.section[class*='level'] > :first-child\"); var i, h, a; for (i = 0; i < hs.length; i++) { h = hs[i]; if (!/^h[1-6]$/i.test(h.tagName)) continue; // it should be a header h1-h6 a = h.attributes; while (a.length > 0) h.removeAttribute(a[0].name); } }); /*! jQuery v3.6.0 | (c) OpenJS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(C,e){\"use strict\";var t=[],r=Object.getPrototypeOf,s=t.slice,g=t.flat?function(e){return t.flat.call(e)}:function(e){return t.concat.apply([],e)},u=t.push,i=t.indexOf,n={},o=n.toString,v=n.hasOwnProperty,a=v.toString,l=a.call(Object),y={},m=function(e){return\"function\"==typeof e&&\"number\"!=typeof e.nodeType&&\"function\"!=typeof e.item},x=function(e){return null!=e&&e===e.window},E=C.document,c={type:!0,src:!0,nonce:!0,noModule:!0};function b(e,t,n){var r,i,o=(n=n||E).createElement(\"script\");if(o.text=e,t)for(r in c)(i=t[r]||t.getAttribute&&t.getAttribute(r))&&o.setAttribute(r,i);n.head.appendChild(o).parentNode.removeChild(o)}function w(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?n[o.call(e)]||\"object\":typeof e}var f=\"3.6.0\",S=function(e,t){return new S.fn.init(e,t)};function p(e){var t=!!e&&\"length\"in e&&e.length,n=w(e);return!m(e)&&!x(e)&&(\"array\"===n||0===t||\"number\"==typeof t&&0"},{"title":"","date":"2022-10-05T05:40:07.195Z","updated":"2022-10-05T05:40:07.195Z","comments":true,"path":"images/R/R_basic_stat.html","permalink":"http://gonekng.github.io/images/R/R_basic_stat.html","excerpt":"","text":"R_basic_statistics // Pandoc 2.9 adds attributes on both header and div. We remove the former (to // be compatible with the behavior of Pandoc < 2.8). document.addEventListener('DOMContentLoaded', function(e) { var hs = document.querySelectorAll(\"div.section[class*='level'] > :first-child\"); var i, h, a; for (i = 0; i < hs.length; i++) { h = hs[i]; if (!/^h[1-6]$/i.test(h.tagName)) continue; // it should be a header h1-h6 a = h.attributes; while (a.length > 0) h.removeAttribute(a[0].name); } }); /*! jQuery v3.6.0 | (c) OpenJS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(C,e){\"use strict\";var t=[],r=Object.getPrototypeOf,s=t.slice,g=t.flat?function(e){return t.flat.call(e)}:function(e){return t.concat.apply([],e)},u=t.push,i=t.indexOf,n={},o=n.toString,v=n.hasOwnProperty,a=v.toString,l=a.call(Object),y={},m=function(e){return\"function\"==typeof e&&\"number\"!=typeof e.nodeType&&\"function\"!=typeof e.item},x=function(e){return null!=e&&e===e.window},E=C.document,c={type:!0,src:!0,nonce:!0,noModule:!0};function b(e,t,n){var r,i,o=(n=n||E).createElement(\"script\");if(o.text=e,t)for(r in c)(i=t[r]||t.getAttribute&&t.getAttribute(r))&&o.setAttribute(r,i);n.head.appendChild(o).parentNode.removeChild(o)}function w(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?n[o.call(e)]||\"object\":typeof e}var f=\"3.6.0\",S=function(e,t){return new S.fn.init(e,t)};function p(e){var t=!!e&&\"length\"in e&&e.length,n=w(e);return!m(e)&&!x(e)&&(\"array\"===n||0===t||\"number\"==typeof t&&0"}],"posts":[{"title":"Feature Encoding","slug":"Python/ML/Feature Encoding","date":"2022-12-22T14:21:54.000Z","updated":"2022-12-22T14:36:23.274Z","comments":true,"path":"2022/12/22/Python/ML/Feature Encoding/","link":"","permalink":"http://gonekng.github.io/2022/12/22/Python/ML/Feature%20Encoding/","excerpt":"","text":"데이터 인코딩Scikit-learn 알고리즘은 수치형 변수만 입력값으로 허용하기 때문에,머신러닝을 위해서는 모든 문자열 데이터를 인코딩하여 수치형으로 변환해야 한다. 일반적으로 문자열 데이터는 범주형 데이터와 텍스트 데이터를 의미하는데,범주형 데이터는 각 범주에 대응하는 수치형 변수로 변환하는 것이 효과적이지만텍스트 데이터는 구분자 역할이거나 추가적인 정보를 제공하기 위한 경우가 많다. 이런 경우에는 머신러닝 수행에 있어서 불필요할 가능성이 높으므로형식적인 인코딩보다는 변수의 특성을 잘 살펴본 후 삭제하는 것이 좋다. 머신러닝을 위한 대표적인 인코딩 방식으로는Label Encoding(레이블 인코딩)과 One-Hot Encoding(원-핫 인코딩)이 있다. Label Encoding레이블 인코딩은 간단하게 문자열 값을 각 범주에 해당하는 숫자로 변환하는 방식이다. 하지만 이는 단순히 구분을 위한 숫자이기 때문에일부 알고리즘에서는 각 숫자를 가중치로 해석하여 값을 왜곡하고결과적으로 모델의 예측 성능이 떨어지는 경우도 발생한다. 따라서 레이블 인코딩은 선형 회귀와 같은 알고리즘에는 적용하지 않는 것이 좋다.반면 트리 계열의 비선형 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 큰 무리 없이 사용 가능하다. Scikit-learn의 LabelEncoder를 활용한다. 123456789from sklearn.preprocessing import LabelEncodercities = [&#x27;Seoul&#x27;, &#x27;LA&#x27;, &#x27;Paris&#x27;, &#x27;Tokyo&#x27;, &#x27;LA&#x27;, &#x27;London&#x27;, &#x27;Seoul&#x27;, &#x27;Berlin&#x27;]encoder = LabelEncoder()encoder.fit(cities)labels = encoder.transform(cities)print(labels)# [4 1 3 5 1 2 4 0] classes_ 속성을 통해 각 숫자가 가리키는 범주를 알 수 있다. 123print(encoder.classes_)# [&#x27;Berlin&#x27; &#x27;LA&#x27; &#x27;London&#x27; &#x27;Paris&#x27; &#x27;Seoul&#x27; &#x27;Tokyo&#x27;] inverse_transform 속성을 통해 역변환 할 수 있다. 123print(encoder.inverse_transform([1,4,5,0,2,3]))# [&#x27;LA&#x27; &#x27;Seoul&#x27; &#x27;Tokyo&#x27; &#x27;Berlin&#x27; &#x27;London&#x27; &#x27;Paris&#x27;] One-Hot Encoding원-핫 인코딩은 각 범주에 대응되는 새로운 변수를 추가하여해당 범주에 대응하는 칼럼에만 1을 표시하고 나머지는 0을 표시하는 방식이다. ![](&#x2F;images&#x2F;Python&#x2F;ML&#x2F;Feature Encoding.png) 따라서 인코딩에 앞서 모든 문자열 값이 숫자형으로 변환되어야 하며,Encoder의 입력 값으로 2차원 데이터가 필요하다. 단, 범주가 많을 경우 과도하게 많은 변수가 생성될 수 있기 때문에상황에 맞게 레이블 인코딩과 적절하게 혼용하는 것이 좋다. Scikit-learn의 OneHotEncoder 클래스 123456789101112131415161718192021222324252627282930from sklearn.preprocessing import OneHotEncoderimport numpy as npcities = [&#x27;Seoul&#x27;, &#x27;LA&#x27;, &#x27;Paris&#x27;, &#x27;Tokyo&#x27;, &#x27;LA&#x27;, &#x27;London&#x27;, &#x27;Seoul&#x27;, &#x27;Berlin&#x27;]#Step1: 모든 문자를 숫자형으로 변환합니다.encoder = LabelEncoder()encoder.fit(cities)labels = encoder.transform(cities)#Step2: 2차원 데이터로 변환합니다.labels = labels.reshape(-1, 1)#Step3: One-Hot Encoding 적용합니다.oh_encoder = OneHotEncoder()oh_encoder.fit(labels)oh_labels = oh_encoder.transform(labels)print(oh_labels.toarray())print(oh_labels.shape)# [[0. 0. 0. 0. 1. 0.]# [0. 1. 0. 0. 0. 0.]# [0. 0. 0. 1. 0. 0.]# [0. 0. 0. 0. 0. 1.]# [0. 1. 0. 0. 0. 0.]# [0. 0. 1. 0. 0. 0.]# [0. 0. 0. 0. 1. 0.]# [1. 0. 0. 0. 0. 0.]]# (8, 6) Pandas의 get_dummies 함수 123456import pandas as pdcities = [&#x27;Seoul&#x27;, &#x27;LA&#x27;, &#x27;Paris&#x27;, &#x27;Tokyo&#x27;, &#x27;LA&#x27;, &#x27;London&#x27;, &#x27;Seoul&#x27;, &#x27;Berlin&#x27;]df = pd.DataFrame(&#123;&#x27;item&#x27;:cities&#125;)print(pd.get_dummies(df)) Reference데이터 전처리하기 : 레이블 인코딩 (Label Encoding), 원-핫 인코딩(One-Hot Encoding), get_dummies()를 Pandas에서 사용하기","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"},{"name":"scikit-learn","slug":"scikit-learn","permalink":"http://gonekng.github.io/tags/scikit-learn/"}],"author":"Jiwon Kang"},{"title":"토의면접 TIP","slug":"etc/토의면접 TIP","date":"2022-12-10T14:23:51.000Z","updated":"2022-12-10T14:46:18.076Z","comments":true,"path":"2022/12/10/etc/토의면접 TIP/","link":"","permalink":"http://gonekng.github.io/2022/12/10/etc/%ED%86%A0%EC%9D%98%EB%A9%B4%EC%A0%91%20TIP/","excerpt":"","text":"출처 : Youtube 강민혁 채널 토의면접 평가요소 개방성 설득력 있는 반론 제기 시 본인 주장을 수정하려는 개방적 자세 인정 : 본인이 틀릴 수도 있다는 것을 전제 수정 : 타인 의견 수용 후 더욱 적극적으로 공감함 의견 개진 커뮤니케이션 경청 및 간결하면서도 정확하게 메시지를 전달 경청 : 아이컨택, 메모, 발언자 내용 확인 전달 : 필요한 메시지를 충분히 전달 → 두괄식, 수사 활용, 한 발언에는 한 가지 의미만 전달 분석&#x2F;설득력 분석력 : 제한된 시간 내에 다양한 정보 수집 및 가공 설득력 : 논리적 근거를 제시하여 본인의 주장을 뒷받침 적극성&#x2F;스트레스내성 적극성 : 주제 해결을 위한 의지 → 타인의 주장이 비합리적이거나 논리적이지 않을 경우 공손하게 챌린지 스트레스내성 : 긴장감 적절히 관리, 자신감 있게 말하고 행동 토의 시작 전 주제로 출제 가능한 키워드 공부하기 기출문제, 비즈니스 트렌드, 사업군 이슈 등 아이디어 구상과 함께 상대방 번호 기억하기 준비 시간 조절 및 안배하기 A4 구조화 1분 + 자료 이해 5분 + 아이디어 생각 7분 + 펜 작성 5분 + 생각 정리 2분 &#x3D; 20분 토의 시작 후 기조 발언하기 O번 지원자 기조발언 시작하겠습니다. 저는 ‘OOO’ 주제에 대하여 ~~~를 제시합니다. 이에 대한 근거로는 크게 A, B, C가 있으며 자세한 사항은 본 토의 시간에 말씀드리겠습니다. 상대방 의견 확장하기 O번 지원자님의 A 의견에 공감합니다. 조금 더 내용을 덧붙이자면, ~~ 화제 전환하기 지금까지 A와 관련하여 ~~~ 라는 공통 의견이 나왔습니다. 다만, 한정된 시간을 고려할때 이제 B와 C의 측면 또한 의논하면 좋겠습니다. 다들 동의하시나요? 정리 발언하기 지금까지 ‘OOO’ 주제에 대한 방안으로 크게 A, B, C의 측면에 대해 논의했습니다. 세부적으로는 ~~~ 라는 의견이 나왔는데, 이를 통해 ~~~의 효과를 기대할 수 있습니다. 감사합니다.","categories":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/categories/%EC%B7%A8%EC%A4%80/"}],"tags":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/tags/%EC%B7%A8%EC%A4%80/"},{"name":"면접","slug":"면접","permalink":"http://gonekng.github.io/tags/%EB%A9%B4%EC%A0%91/"}],"author":"Jiwon Kang"},{"title":"면접 TIP","slug":"etc/면접 TIP","date":"2022-12-10T12:56:01.000Z","updated":"2022-12-10T14:23:00.490Z","comments":true,"path":"2022/12/10/etc/면접 TIP/","link":"","permalink":"http://gonekng.github.io/2022/12/10/etc/%EB%A9%B4%EC%A0%91%20TIP/","excerpt":"","text":"출처 :Youtube AND(인싸담당자) 채널Youtube 면접왕이형 채널 기업 분석 필수 요소 4가지 기업 공식 홈페이지 : 일반 현황, 인사말 : 조직 체계, 비전 체계 : 당해년도 전략 목표, 브로슈어, ESG 보고서 기업 공시 자료 → 사기업 - DART, 공기업 - 잡알리오 : 매출액, 영업이익, 사업 흐름 등등 최근 6개월 기업 관련 뉴스 기업 대표의 신년사 - 키워드 기업 분석 필수 준비 질문 5가지 지원 회사가 수행하는 사업은 무엇인가? 지원 회사의 강점과 약점은 무엇인가? 지원 회사의 당해년도 전략 목표는 무엇인가? 지원 회사의 경쟁사는 어디라고 생각하는가? 현 시장 상황에서 지원 회사가 나아가야 할 방향은? 논리적으로 답변하는 방법 💡 두괄식으로 답변 : 주장 → 근거 → 결과 → 포부 Q. ~에 대해 어떻게 생각하나요? 저는 이렇게 생각합니다. 근거는 이겁니다. 그래서 이렇게 하겠습니다. Q. ~와 관련된 경험이 있나요? 저는 이런 경험이 있습니다. 그때의 상황은 어땠습니다. 저는 어떤 액션을 취했습니다. 결과적으로 무슨 일이 벌어졌습니다. 이 경험을 통해 이런 능력을 길렀습니다. 직무적합성 자기소개서부터 면접까지 일관성 있게 답변하기 해당 직무를 좋아하게 된 이유나 계기부터 시작하기 학교 수업, 아르바이트, 유튜브 영상 등 사소한 계기도 충분히 가능 답변에 기승전결을 갖춰서 스토리텔링하기 직무를 좋아하게 된 계기 + 직무를 위한 노력 및 준비 과정 면접 질문 기본 유형 5가지 1분 자기소개 해주세요 질문의도 : 첫인상 체크, 말할 기회 주기, 질문거리 찾기 유사질문 간단하게 자기소개 해주세요 준비된 거 말고 편안하게 자기소개 해주세요 답변방법 가장 어필하고 싶은 필살기를 전달하기 비교적 성과가 명확하고 직무 연관성이 높은 성공경험 비유적 표현, 추상적 개념, 성격적 특징 X 차별화된 본인의 강점을 설명해주세요 질문의도 : 뽑아야 할 이유 찾기 유사질문 성공 경험에 대해서 소개해주세요 분석력을 통해서 성과를 낸 경험이 있다면 얘기해주세요 그 경험을 좀 더 자세하게 설명해주세요 답변방법 가장 의미 있었던 성공경험의 액션과 결과부터 던지기 (두괄식) 그 이후에 상황, 의도, 배운 점을 설명하기 왜 우리 회사에 지원하셨나요? 질문의도 : 회사에 대한 로열티, 관심도, 흥미도 확인 유사질문 다른 회사 어디 또 지원하셨나요? 여기서 떨어지면 어디 갈 거에요? 우리 회사에 대해 아는 대로 얘기해보세요 답변방법 회사의 경쟁력, 기술적 특성을 나의 경험과 연결하여 설명하기 현직자 인터뷰를 통해 회사의 차별화된 특성을 알아보기 성격의 장단점이 있나요? 질문의도 : 자기 객관화가 잘 되어있는지, 회사에 잘 적응할 수 있는지 유사질문 힘들었던 경험이 있나요? 같이 일하기 힘든 사람은 어떤 유형인가요? 주변 사람들은 본인을 어떻게 평가하시나요? 어떤 별명을 가지고 계신가요? 답변방법 MBTI 등을 토대로 객관적인 성격 요소 답변하기 내가 극복했던 단점을 경험과 함께 설명하기 마지막 하고 싶은 말 있으신가요? 질문의도 : 면접 마무리, 딱히 의미 없음 유사질문 마지막으로 궁금하신거 있으신가요? 혹시 하지 못한 말이 있나요? 답변방법 결과가 바뀔 가능성은 거의 없음 필살기를 잘 던졌을 경우 감사 표현과 함께 간단하게 마무리 필살기를 못 던졌을 경우 준비한 내용 던지기 면접관이 답변하기 쉬운, 자랑할 수 있는 질문하기 ex) 회사의 향후 성장 전략 등","categories":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/categories/%EC%B7%A8%EC%A4%80/"}],"tags":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/tags/%EC%B7%A8%EC%A4%80/"},{"name":"면접","slug":"면접","permalink":"http://gonekng.github.io/tags/%EB%A9%B4%EC%A0%91/"}],"author":"Jiwon Kang"},{"title":"자소서 TIP (3)","slug":"etc/자소서 TIP 3","date":"2022-12-08T14:57:25.000Z","updated":"2022-12-08T15:28:30.283Z","comments":true,"path":"2022/12/08/etc/자소서 TIP 3/","link":"","permalink":"http://gonekng.github.io/2022/12/08/etc/%EC%9E%90%EC%86%8C%EC%84%9C%20TIP%203/","excerpt":"","text":"출처 : Youtube AND(인싸담당자) 채널 논리적 사고력📌 논리적 사고력 (문제 해결력) : 분석, 전략, 아이디어, 혁신, 개선, 창의력 → 하나의 답변으로 나머지 역량에 대한 모든 질문에 답변 가능 개념 정리분석 : A에서 B로 갈 수 있는 유의미한 길을 찾아내는 능력 전략 : 가장 빠른 길을 선택하고, 필요한 요소를 활용할 줄 아는 능력→ 분석력과 전략적 사고는 항상 함께 하는 것 (무엇 하나 없으면 맥락이 빠져버림) 창의력 : 기존에 없던, 근거가 없는 길을 만드는 능력→ 질문의 의도는 이와 다르며, 실제로 자소서에 녹여내기 어려움→ 따라서 창의력, 혁신, 새로운 사고 질문에도 분석&amp;전략에 해당하는 답변을 적으면 됨 핵심 포인트단순한 현상의 해결이 아닌 근본적인 문제의 원인을 해결했다. 문제를 해결하는 사고 과정 : 문제 인식 - 원인 분석 - 원인 해결 - 상황 해결 이때 원인은 일차원적으로 생각하는 것이 아니라, 내가 해결할 수 있는 단위까지 연결해야 함 ex) 카페에서 알바하던 중 매출이 떨어진 원인을 찾기 위해 인근 상권을 조사했습니다. 알고 보니 비슷한 컨셉의 카페가 근처에 많이 생긴 것을 알 수 있었습니다. (원인 X) 그래서 저희 단골 고객에게 OO카페를 자주 이용하는 이유를 물어봤더니 다른 카페들에 비해 생과일 주스가 더 맛있어서 자주 찾게 된다는 답변을 들었습니다. 따라서 생과일 주스 라인, 특히 계절에 맞는 수박주스 등을 메뉴에 추가하였고, 커피의 매출은 조금 떨어졌지만 생과일 주스의 매출이 많이 올라서 전체 매출을 유지할 수 있었습니다. 템플릿 역량을 한 줄로 요약(재정의) 문제 문제 인식 및 원인 확인 원인 해결 및 근거 결과 포부 의사소통 능력📌 의사소통 능력은 성과보다 문제 해결에 초점을 맞춰 어필해야 하는 역량 개념 정리플러스(+) 역량 : 성과와 직접적으로 연결 ex) 목표 달성 능력, 도전 정신, 논리적 사고력 등등 마이너스(-) 역량 : 실패와 직접적으로 연결 ex) 의사소통, 커뮤니케이션, 팀워크 등등→ 아무리 뛰어나도 아무 문제가 일어나지 않기 때문에 자소서에 녹여내기 어려움→ 마이너스 역량은 ‘뛰어나다’보다 ‘노력했다’를 서술해야 하는 역량→ 나의 실수 또는 잘못으로 이러한 문제가 발생했다. 하지만 이 역량을 가지고 노력해서 극복했다. | 커뮤니케이션 |정보 전달의 오류를 막기 위해서 내가 한 언어적, 비언어적인 노력과 그로 인한 문제 해결 경험 템플릿 역량을 한 줄로 요약(재정의) 의사소통 능력의 정의는 사람마다 천차만별이기 때문에 재정의가 특히 중요함 정보전달의 오류로 발생한 현상 내가 생각하는 커뮤니케이션의 정의 반드시 문제가 예상되거나 문제가 있어야 함 이해를 돕기 위해 의사소통의 형태를 바꾼 액션 커뮤니케이션을 위한 언어적, 비언어적 노력 보충 설명, 단어 교체, 사례 제시, 시각자료 활용, 재차 확인 결과 포부 | 설득력 |가치관/이해관계로 대립될 때 상대방의 생각을 나의 생각으로 바꾼 경험 (나만의 노하우/방법) ※ 상사는 설득하는 대상이 아니라, 내가 컨펌받아야 하는 대상 → 설득력의 사례 X 템플릿 역량을 한 줄로 요약(재정의) 나만의 설득 방법&#x2F;노하우 상황 액션 설득 과정을 액션 중심으로 서술 결과 포부 조직 경험| 주도성 &#x2F; 적극성 |조직 경험 문항이기 때문에 기존에 주어진 업무 수행은 필수 선행조건 아무도 시키지 않은 일 주어진 업무 외에 추가적으로 업무를 수행한 경험 업무의 범위와 한계를 서술해서 그 범위를 벗어나는 일이라는 것을 어필 주도적인 목표 선정 | 책임감 | 조장으로서 조별과제를 이끌었던 경험은 너무 흔한 사례 → 다른 경험 찾아보기 템플릿 역량을 한 줄로 요약(재정의) 상황 노력(희생) 예상치 못한 장애물을 극복한 경험도 희생으로 볼 수 있음 결과 포부 | 리더십 |리더십 3요소 : ‘조직’이 ‘목표’를 달성할 때 내가 준 ‘영향력’ → 아무리 많은 영향력을 주고 큰 기여를 해도 목표를 달성하지 못하면 리더십 부족으로 간주 템플릿 역량을 한 줄로 요약(재정의) 조직의 목표 또는 발생한 문제 조직 목표 달성을 위해 고민 끝에 내린 나의 의사결정 나의 고민, 결정, 행동의 이유 조직원들에게 준 영향력 결과 포부 | 팀워크 |책임감과 유사한 역량. 나를 희생해서 조직을 위해 헌신할 수 있는가? 선행조건 희생과 시너지 업무 습관 또는 방식의 차이 업무 습관이나 방식은 개인의 성격과 밀접하게 관련 생각의 과정 템플릿 역량을 한 줄로 요약(재정의) 팀원 또는 협업자의 문제 나를 희생해서 조직을 위해 헌신한 부분 나의 고민, 결정, 행동의 이유 조직원들에게 준 영향력 결과 포부 기타 역량| 치밀함 |모든 경우의 수를 예상하여 대비한 경험 (꼼꼼함, 치밀함, 계획력, 기획력) → 문제 해결을 위해 플랜A / 플랜B / 플랜C 를 세웠던 경험 선행조건 문제 상황을 예상 모든 경우의 수에 대한 준비 그 중 한 가지 상황이 발생하여 적용 템플릿 역량을 한 줄로 요약(재정의) 액션 발생 직전의 상황 플랜A, 플랜B, 플랜C를 세우고 그 중 하나가 발생 결과 포부 | 윤리 |과거의 경험으로 미래를 추측할 수 없는 유일한 역량 → 그냥 물어보는 쇼윈도 문항 유혹에 빠졌는데 극복한 케이스 유혹에 빠졌다가 반성한 케이스 타인이 잘못한 케이스 (반면 교사) 템플릿 역량을 한 줄로 요약(재정의) 상황 노력(희생) 예상치 못한 장애물을 극복한 경험도 희생으로 볼 수 있음 결과 포부","categories":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/categories/%EC%B7%A8%EC%A4%80/"}],"tags":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/tags/%EC%B7%A8%EC%A4%80/"},{"name":"자소서","slug":"자소서","permalink":"http://gonekng.github.io/tags/%EC%9E%90%EC%86%8C%EC%84%9C/"}],"author":"Jiwon Kang"},{"title":"자소서 TIP (2)","slug":"etc/자소서 TIP 2","date":"2022-12-08T14:52:11.000Z","updated":"2022-12-08T15:29:30.712Z","comments":true,"path":"2022/12/08/etc/자소서 TIP 2/","link":"","permalink":"http://gonekng.github.io/2022/12/08/etc/%EC%9E%90%EC%86%8C%EC%84%9C%20TIP%202/","excerpt":"","text":"출처 : Youtube AND(인싸담당자) 채널 성장과정📌 성장과정 그 자체보다, 지원자의 역량에 대한 근거를 알고자 하는 질문 핵심 포인트 역량이 성장됐다. 2) 가치관의 성장을 통하여 역량이 성장됐다. → 나의 성장과정과 경험은 ‘OOO 직무에 가장 적합한 사람’으로 만들어주었다. Ver 1. 핵심 역량 중심 직무 핵심 역량 지원 직무의 핵심역량이 경험1, 경험2를 통해서 강점이 되었다. 경험1 and 경험2 나의 강점의 근거로 제시할 수 있는 경험을 상황, 액션, 결과로 제시 미래의 포부 포부로 마무리 Ver 2. 가치관 중심 가치관 문구 설정 단어 또는 문장으로 작성 가치관 형성 계기 실패했던 경험, 가장 힘들었던 경험, 실수했던 경험 등을 통해 얻은 가치관 가치관 → 역량 역량을 펼친 경험 미래의 포부 직무와 연결시켜 포부를 밝힘 ex) 저는 “바람이 불지 않으면 노를 저어라”라는 가치관이 있습니다. 그런 실행력이 있습니다.제가 대학교 때 MT를 가서 누군가한테 이런 이야기를 들었는데요.그때 제 부족한 점에 대해서 깨달았고 그것들이 제 삶에 녹아져서 이런 역량으로 발전되었습니다.이러한 실행력은 이러한 경험들 속에서 채용팀장을 하면서 어떤 성과를 만들 수 있었습니다.이런 실행력을 통해서 이 직무를 하면서 어떻게 더 성과를 내보겠습니다. 성격의 장단점📌 성격의 장단점을 통해 역량을 유추하기 위한 질문 → 비즈니스 상황에서 긍정적이면 강점/장점, 부정적이면 약점/단점이 된다. 핵심 포인트나는 솔직히 이 직무와 정말 잘 맞는다. 장점은 물론 단점마저도 잘 맞는다. 템플릿 성격의 장점 장점이 발휘되었던 사례 성격의 단점 성격의 단점에서 발생하는 문제점 및 보완하는 노력 단점은 절대 완벽하게 개선되지 않기 때문에 그 자체를 개선한다고 하지 말기 단점이 해당 직무에서 가지는 문제점을 장점을 통해 보완한다는 방향이 오히려 낫다 ex) 저의 장점은 추진력입니다.저의 단점은 다소 꼼꼼하지 못합니다. 그래서 계획을 세우면 누락되는 것들이 가끔 발생합니다. 이러한 가끔 발생한 것을 보완하려고 노력했지만 잘 개선되지는 않았습니다.그래서 저는 저의 강점을 활용해서 남들보다 빠르게 실행하고 몇번 더 반복함으로써 완성도를 높여가는 전략으로 일을 처리합니다. ex) 저의 장점은 사교성입니다. 그래서 다양한 사람들과의 팀워크를 통해 성과를 냈습니다.저의 단점은 우유부단합니다. 그러다보니 어떠한 특정 결정이 늦어져 안 좋은 결과로 이어지기도 했습니다. 빠른 결정을 위해 노력하지만 다양한 걱정으로 인해 쉽게 개선되지 못했습니다.저는 이러한 단점을 해결하기 위해 저의 강점인 사교성을 발휘하여 다양한 사람들에게 조언을 들음으로써 빠른 의사결정을 방해하는 여러 문제점들을 해소하고 결정하려고 노력하고 있고, 실제로 의사결정이 더욱더 빨라졌습니다. 성격 단어 오직 - 라운지 - 아카이브 - 직무 찾기 강의자료 - Cross Checking 성격 문장 잡코리아 - 퓨처랩 - 취업 성공 툴 - 자소서 자동완성 - 성격의 장단점 성과지향성📌 목표 달성 능력, 열정, 실행력, 추진력, 도전경험, 실패/극복경험 → 답변 하나로 성과지향성을 판단하는 모든 질문에 대응 가능 | 목표 달성 능력 |목표 달성 능력 = 장애물 + 성공 → 나는 어떠한 장애물도 극복하고 목표한 바를 이루는 사람이다. 선행조건 명확하고 높은 목표 예상하지 못한 장애물 (포기하고 싶은 상황) 목표 달성 템플릿 역량을 한 줄로 요약(재정의) ex) 높은 목표 달성을 위해서는 OOO을 ~해야 한다고 생각했고, 이러한 경험을 통해 이러한 목표를 달성한 경험이 있습니다. 액션 발생 직전의 상황과 목표 목표의 높고 낮은 정도는 상대적인 것 → 높게 쌓아 올리거나, 주변을 깊게 파거나 목표가 더 높게 보이는 방법 주변 사람들의 부정적인 반응 (하지마, 그만해, 안될거야, 어려워 등등) 이전의 상황이나 주변 사람들의 목표와 비교 (작년보다 2배 높게, 남들보다 짧은 기간 등등) 목표 달성에 필요한 세부 목표를 설정하여 장애물과 연결 (OO을 위해서 반드시 OO을 해야함) 장애물 → 극복 결과 포부 | 도전 경험 |도전 경험 : 단순한 시도나 행위 X, 예상되는 리스크를 감내하고 실행했던 경험 → 나는 이러한 리스크가 예상되었음에도 불구하고 ~를 포기하고 이러한 도전을 했다. 템플릿 역량을 한 줄로 요약(재정의) ex) 높은 목표 달성을 위해서는 OOO을 ~해야 한다고 생각했고, 이러한 경험을 통해 이러한 목표를 달성한 경험이 있습니다. 도전 상황과 리스크 장애물 → 극복 결과 배운점 포부 | 실패 경험 |실패 경험 ≠ 실수 경험 - 실패 : 수행 의지가 있는 상태에서 잘못된 상황 - 실수 : 우연히 잘못된 상황 (노력이나 의지와 무관) 선행조건 주어진 목표가 큼 도전 경험 내가 세운 목표도 가능 ※ 실패한 경험이 없을 경우, 목표 달성 경험에서의 목표를 높여서 서술하기 템플릿 역량을 한 줄로 요약(재정의) ex) 높은 목표 달성을 위해서는 OOO을 ~해야 한다고 생각했고, 이러한 경험을 통해 이러한 목표를 달성한 경험이 있습니다. 액션 발생 직전의 상황과 목표 장애물 → 극복 실패 → 원인 분석 극복 포부 입사 후 포부📌 해당 직무에 대해 제대로 알고 있는지 아닌지 판단하기 위한 질문 → 믿기 어렵겠지만 나는 이 직무를 예전부터 꿈꿔왔다는 것을 어필 선행조건 구체성 (상황 가정) ~할 텐데, ~할 일이 생기는데 등등 직무 지식 &#x2F; CDP &#x2F; 핫이슈 현재와 미래의 밸런스 ※ 10년 뒤에 뭐할거에요? &#x3D; 너가 이 업계 10년차(최전성기)가 되면 뭘 하고 싶니? Ver 1. 장기 템플릿 (직무 분석 O) 최종 Goal &#x2F; 하고 싶은 일 해당 직무에서 도달하고 싶은 직책, 그 직책에 도달하면 하고 싶은 일 N년 차의 포부 &#x2F; 단계별 목표 최종 Goal에 도달하기 위한 단계, 그리고 본인이 가고 싶은 CDP 맡게 되는 일 + 하고 싶은 일 맡게 되는 일 + 어려움 + 극복 방법 Ver 2. 단기 템플릿 (직무 분석 X) 최종 Goal &#x2F; 하고 싶은 일 해당 직무에서 도달하고 싶은 직책, 그 직책에 도달하면 하고 싶은 일 문제 해결 현재 직무 &#x2F; 산업군 &#x2F; 회사에서 겪고 있는 어려움을 해결하겠다. ※ 내가 알고 있는 것과 모르고 있는 것의 차이를 명확하게 인지하는 것이 도움이 될 수 있음ex) 학교에서는 A를 배웠는데, 회사 실무에서는 B와 C까지 필요. 따라서 부족한 B와 C는 어떻게 학습하겠다.","categories":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/categories/%EC%B7%A8%EC%A4%80/"}],"tags":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/tags/%EC%B7%A8%EC%A4%80/"},{"name":"자소서","slug":"자소서","permalink":"http://gonekng.github.io/tags/%EC%9E%90%EC%86%8C%EC%84%9C/"}],"author":"Jiwon Kang"},{"title":"자소서 TIP (1)","slug":"etc/자소서 TIP 1","date":"2022-12-08T14:31:51.000Z","updated":"2022-12-08T15:21:54.583Z","comments":true,"path":"2022/12/08/etc/자소서 TIP 1/","link":"","permalink":"http://gonekng.github.io/2022/12/08/etc/%EC%9E%90%EC%86%8C%EC%84%9C%20TIP%201/","excerpt":"","text":"출처 : Youtube AND(인싸담당자) 채널 지원동기📌 우리의 정체성 : 구직자 = 일하는 사람 → 회사와 구직자의 교집합은 돈이 아니라 일! → 나는 일을 하기 위해 지원하고, 회사는 일을 시키기 위해 채용한다. 핵심 포인트이 회사를 좋아해서, 관심이 있어서, 마음에 들어서 지원하는 것이 아니라, 내가 기여할 수 있는 &#x2F; 성공시킬 수 있는 &#x2F; 완성할 수 있는 이 일을 하기 위해서 지원합니다. 선행조건 직무 지원동기 (전문 역량) 산업군 지원동기 (전문 지식) 회사 지원동기 Ver 1. 유사 경험이 있는 경우 ~을 완성&#x2F;성공&#x2F;기여해보고 싶다. (요약) 해당 직무의 과업, 문제, 해결방법에 따라 첫줄에 명시하기 ex) 저는 인싸담당자 PD가 되어서 취준생들에게 영상을 제공할 때 실질적으로 활용 가능한 정보의 콘텐츠를 만들고 싶습니다. 이를 위해서 현직자들을 모셔오고 현직자들과 대화하는 콘텐츠를 만들어서 구독자 100만을 만들겠습니다. 관심을 갖게 된 배경 (산업 &#x2F; 직무) 그 회사를 어떻게 인지하게 되었는가 단순히 제품이나 서비스를 이용해본 경험 X 교수님, 친구, 가족, 선배 등등 평범한 경로라도 상관 없음 이후에 서술할 특정 경험과 연결하기 돈을 벌기 위한 소극적 경험을 회사에 대해 더 알아보기 위한 적극적 경험으로 만들기 ex) 수업 시간에 교수님과 유통 산업에 대한 조사를 하다보니 BGF 리테일의 영업이익률과 발전 가능성에 대해 관심을 갖게 되었습니다. 이러한 관심 때문에 더 알아보고자 GS25 편의점에서 아르바이트를 했습니다. 직무 &#x2F; 산업군 지원동기 관련 전문 지식 회사 지원동기 포부 Ver 2. 유사 경험이 없지만 시간적 여유가 있는 경우 → 산업 분석 및 기업 분석 필수“ 너희가 잘하고 있는 것을 내가 ~으로서 더 잘되게 할 수 있다. ” 이 회사에는 어떠한 강점이 있고, 나의 직무로서 그 강점에 어떤 도움을 주고 싶어서 지원합니다. ~을 완성&#x2F;성공&#x2F;기여해보고 싶다. (요약) 해당 직무의 과업, 문제, 해결방법에 따라 첫줄에 명시하기 회사의 강점 &amp; 회사가 잘하고 있는 점 경쟁사에 비해 가지고 있는 강점이나 차별성 ex) OO 회사는 현재 해당 부분에서 이러한 방법으로 앞서 나가고 있습니다. 내가 잘할 수 있는 이유 (경험1, 경험2) 지원한 직무로서 그 강점에 기여할 수 있는 역량 1~2가지 어필 해당 역량을 발견 혹은 발휘한 경험 1~2가지 어필 산업군에 대한 관심사에 따른 활동 및 경험(자격증, 교육 수료, 박람회 참가 등) ex) OO 회사는 이 제품을 가장 가볍게 만든다는 강점이 있는데 (마케팅) 이러한 특징을 B2C 고객들에게 어떠한 방식을 통해서 더 잘 알리고 싶습니다. (인사) 이러한 특징을 위해서는R&amp;D 역량이 가장 중요한데 그들이 몰입할 수 있는 환경을 만드는 데에 기여하고 싶습니다. 포부 Ver 3. 유사 경험도 없고 시간적 여유도 없는 경우 → 직무 지원동기를 작성한다고 생각하기 ~을 완성&#x2F;성공&#x2F;도전해보고 싶다. (요약) 해당 직무의 과업, 문제, 해결방법에 따라 첫줄에 명시하기 해당 직무의 문제 해당 직무를 수행하는 동안 해결해야 할 문제 ex) OO 직무는 OO 업무를 담당하며, 이때 어떠어떠한 문제를 해결하는 것이 중요하다. 내가 잘할 수 있는 이유 (경험1, 경험2) 해당 문제를 해결하기 위해서 필요한 역량 1~2가지 어필 해당 역량을 발견 혹은 발휘한 경험 1~2가지 어필 Ver 4. 특정 회사를 강조하고 싶은 경우 ~을 완성&#x2F;성공&#x2F;도전해보고 싶다. (요약) 해당 직무의 과업, 문제, 해결방법에 따라 첫줄에 명시하기 회사의 비전 및 인재상 회사의 인재상을 바로 내 역량과 연결시키지 말기 어떠한 인재상이 그 회사의 어떠한 사업에 어떻게 반영되고 있는지 연결 ex) OO 회사는 이러한 인재상을 가지고 있는데, 현재 이루어지고 있는 어떠한 사업 방향에 녹여져 있다는 것을 느꼈습니다. 저 또한 이러한 가치관을 가지고 있기 떄문에 이 회사를 선택하게 되었습니다. 내가 잘할 수 있는 이유 (경험1, 경험2) 해당 문제를 해결하기 위해서 필요한 역량 1~2가지 어필 해당 역량을 발견 혹은 발휘한 경험 1~2가지 어필","categories":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/categories/%EC%B7%A8%EC%A4%80/"}],"tags":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/tags/%EC%B7%A8%EC%A4%80/"},{"name":"자소서","slug":"자소서","permalink":"http://gonekng.github.io/tags/%EC%9E%90%EC%86%8C%EC%84%9C/"}],"author":"Jiwon Kang"},{"title":"1분 자기소개 TIP","slug":"etc/1분 자기소개 TIP","date":"2022-12-07T02:18:25.000Z","updated":"2022-12-08T14:32:31.990Z","comments":true,"path":"2022/12/07/etc/1분 자기소개 TIP/","link":"","permalink":"http://gonekng.github.io/2022/12/07/etc/1%EB%B6%84%20%EC%9E%90%EA%B8%B0%EC%86%8C%EA%B0%9C%20TIP/","excerpt":"","text":"출처 : Youtube AND(인싸담당자) 채널 핵심 포인트진정성 + 특이 경험 + 어그로 → 추가 질문을 끌어내면 성공 4가지 블록 나는 ~~한 사람입니다. 여기서 임팩트가 없으면 뒷부분은 귀에 잘 안 들어옴 예시 저는 국토대장정을 스스로 만든 OOO입니다. 저는 무엇이든 하면 5년 이상 하는 끈기를 가진 OOO입니다. 나의 특징, 성격, 가치관, 주변의 평가, 장단점, 특이 경험, 성공 경험 등등 마지막에는 꼭 직무와 연결시켜야 함 나를 표현할 수 있는 단어를 찾아본다 나의 경험 경험 A &#x2F; 경험 A, 경험 B &#x2F; 경험 A, 지식 공부 B 중요한 포인트를 살짝 빼기 (상황 액션 결과에서 액션을 빼기) 내 경험 자체를 드러내기보다는 관심을 끌고 다음 질문으로 이어가기 위한 목적 꿈보다 해몽 숫자적인 측면을 살짝 넣어주면 조금 더 잘 들림 (뻥튀기 금지) 진정성 강조하는 요약 예시 : “제가 이러한 경험을 할 수 있었던 것은 저의 이런 캐릭터 때문이었습니다.” 나의 캐릭터와 성공 경험을 이어주는 역할 나의 포부 해당 역량이 어떻게 그 직무에서 적용될 것인지 서술 5가지 방법 가치관 첫번째 블록에서 가치관과 핵심 역량을 연결해주기 특색 있는 경험 유사 경험 관련 경험 자체를 서술하는 것이 아니라 텍스트나 영상으로 체득할 수 없는, 몸으로 겪은 부분을 이야기함 공부했다, 느꼈다, 체험해봤다, 다르다는 걸 깨달았다 등등 베이직 역량 직무역량과 다른 베이직 역량 (분석력, 성실함, 책임감 등등) 지식 지식을 어떻게 적용했는지에 대한 경험 지식을 얻기 위한 나의 노력 자체를 어필","categories":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/categories/%EC%B7%A8%EC%A4%80/"}],"tags":[{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/tags/%EC%B7%A8%EC%A4%80/"},{"name":"면접","slug":"면접","permalink":"http://gonekng.github.io/tags/%EB%A9%B4%EC%A0%91/"}],"author":"Jiwon Kang"},{"title":"Feature Scaling","slug":"Python/ML/Feature Scaling","date":"2022-12-02T11:18:35.000Z","updated":"2022-12-02T11:33:27.103Z","comments":true,"path":"2022/12/02/Python/ML/Feature Scaling/","link":"","permalink":"http://gonekng.github.io/2022/12/02/Python/ML/Feature%20Scaling/","excerpt":"","text":"정규화 vs 표준화정규화 : 데이터의 범위의 차이를 왜곡하지 않고 공통 척도로 변경하는 것표준화 : 데이터가 표준정규분포의 속성을 갖도록 재조정되는 것 정규화(Normalization) 표준화(Standardization) Scaling에 최대&#x2F;최소값 사용 Scaling에 평균 및 표준편차 사용 [0,1] 또는 [-1,1] 사이의 값으로 변환 특정 범위로 제한되지 않음 Feature의 크기(범위)가 다를 때 사용 평균을 0, 표준편차를 1로 만들고자 할 때 사용 Feature의 분포에 대해 모를 때 유용 Feature가 정규분포(에 근사)인 경우 유용 MinMaxScaler, MinAbsScaler, Normalizer StandardScaler, RobustScaler Scaler 종류StandardScaler 평균이 0, 분산이 1인 표준정규분포화 이상치의 영향 많이 받음 123456from sklearn.preprocessing import StandardScalerstd = StandardScaler()std.fit(X_train)X_train_scaled = std.transform(X_train)X_test_scaled = std.transform(X_test) RobustScaler 평균과 분산 대신 중간값과 사분위값을 사용 이상치의 영향 최소화 123456from sklearn.preprocessing import StandardScalerstd = StandardScaler()std.fit(X_train)X_train_scaled = std.transform(X_train)X_test_scaled = std.transform(X_test) MinMaxScaler 0과 1 사이의 값으로 변환 이상치의 영향 많이 받음 123456from sklearn.preprocessing import MinMaxScalermms = MinMaxScaler()mms.fit(X_train)X_train_scaled = mms.transform(X_train)X_test_scaled = mms.transform(X_test) MaxAbsScaler -1과 1 사이의 값으로 변환 이상치의 영향 많이 받음 123456from sklearn.preprocessing import MaxAbsScalermas = MaxAbsScaler()mas.fit(X_train)X_train_scaled = mas.transform(X_train)X_test_scaled = mas.transform(X_test) Normalizer 각 열이 아닌 행마다 정규화 수행 한 행의 모든 피처들 사이의 유클리드 거리가 1이 되도록 함 학습이 빠르고, 과대적합 가능성을 낮출 수 있음 12345from sklearn.preprocessing import RobustScalerrbs = RobustScaler()X_train_scaled = rbs.fit_transform(X_train)X_test_scaled = rbs.transform(X_test)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"},{"name":"scikit-learn","slug":"scikit-learn","permalink":"http://gonekng.github.io/tags/scikit-learn/"}],"author":"Jiwon Kang"},{"title":"Jupyter Notebook에서 SQL 실행하기","slug":"SQL/Jupyter Notebook에서 SQL 실행하기","date":"2022-11-18T14:37:04.000Z","updated":"2022-11-18T14:46:58.121Z","comments":true,"path":"2022/11/18/SQL/Jupyter Notebook에서 SQL 실행하기/","link":"","permalink":"http://gonekng.github.io/2022/11/18/SQL/Jupyter%20Notebook%EC%97%90%EC%84%9C%20SQL%20%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0/","excerpt":"","text":"라이브러리 설치 공통적으로 다음 라이브러리를 설치한다 1pip install ipython-sql 접속하고자 하는 DB에 맞게 라이브러리를 설치한다 1234567891011# sql serverpip install pyodbc# PostgreSQL pip install pyscopg2# MySQLpip install PyMySQL# Oraclepip install cx_Oracle Jupyter Notebook에서 설정하기 Jupyter Notebook에서 매직명령어로 익스텐션을 로드한다. 1%load_ext sql 다음과 같은 창이 뜨면 Install을 누른다. 설치하면 정상적으로 실행이 된다 접속하려는 DB에 맞는 코드를 입력 후 실행 1234567891011# SQL Server%sql mssql+pyodbc://user_name:password@host:port_number/db# PostgreSQL%sql postgresql://user_name:password@host:port_number/db # MySQL%sql mysql://user_name:password@host:port_number/db# Oracle%sql oracle://user_name:password@127.0.0.1:port_number/db 연결이 되었으면 코드 앞에 %%sql을 붙이고 쿼리를 실행한다 (세미콜론 제외) Jupyterlab에서 잘 실행되는 것을 확인할 수 있다. Reference 참고1 : https://95pbj.tistory.com/47 참고2 : https://towardsdatascience.com/heres-how-to-run-sql-in-jupyter-notebooks-f26eb90f3259","categories":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/categories/sql/"}],"tags":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/tags/sql/"},{"name":"oracle","slug":"oracle","permalink":"http://gonekng.github.io/tags/oracle/"}],"author":"Jiwon Kang"},{"title":"Disqus로 블로그 댓글 기능 설정","slug":"hexo/disqus_comment","date":"2022-11-16T09:55:51.000Z","updated":"2022-11-16T10:10:34.615Z","comments":true,"path":"2022/11/16/hexo/disqus_comment/","link":"","permalink":"http://gonekng.github.io/2022/11/16/hexo/disqus_comment/","excerpt":"","text":"Hexo 블로그의 Hueman 테마는 기본적으로 Disqus 서비스를 지원하며, 이를 통해 블로그의 댓글 기능을 설정할 수 있다. (Hexo 블로그 Hueman 테마 설정) Disqus 회원가입 Disqus 사이트에 회원가입 후 로그인한다. Disqus 사이트 추가 메인 페이지에서 Get Started 클릭 I want to install Disqus on my site 클릭 Website Name, Category, Language 지정 Basic 요금제 선택 I don’t see my platform listed, install manually with Universal Code 클릭 configure 클릭 Website URL 항목에 블로그 주소 입력 후 Next 클릭 Balanced 옵션 선택 후 Complete Setup 클릭 Dismiss Setup 클릭 오른쪽 상단에 있는 Edit Settings 클릭 Shortname 항목에 있는 나의 Shortname 확인 _config.icarus.yml에 Shortname 설정하기 블로그 테마 폴더의 _config.yml 파일에서 다음 위치에 나의 Disqus Shortname을 입력한다. 123# Commentcomment: disqus: gonekng # enter disqus shortname here Reference https://chinsun9.github.io/2020/09/23/hexo/disqus로-블로그-댓글-사용하기/","categories":[{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/tags/hexo/"},{"name":"hueman","slug":"hueman","permalink":"http://gonekng.github.io/tags/hueman/"},{"name":"disqus","slug":"disqus","permalink":"http://gonekng.github.io/tags/disqus/"}],"author":"Jiwon Kang"},{"title":"Hexo 블로그 Hueman 테마 설정","slug":"hexo/hueman_theme","date":"2022-11-16T08:01:23.000Z","updated":"2022-11-16T10:02:01.966Z","comments":true,"path":"2022/11/16/hexo/hueman_theme/","link":"","permalink":"http://gonekng.github.io/2022/11/16/hexo/hueman_theme/","excerpt":"","text":"Install theme 블로그와 연결된 루트 폴더에서 git 명령어로 Hueman 테마를 다운로드한다. 1$ git clone [https://github.com/ppoffice/hexo-theme-hueman.git](https://github.com/ppoffice/hexo-theme-hueman.git) themes/hueman 블로그의 _config.yml을 수정합니다. 1theme: hueman themes 폴더 안에 있는 _config.yml.example의 이름을 _config.yml로 수정한다. 검색 기능을 위해 hexo-generator-json-content를 설치한다. 1$ npm install -S hexo-generator-json-content Change settings앞에서 이름을 변경했던 _config.yml 파일을 수정하면 각종 설정을 변경할 수 있다. 메뉴123456# Menusmenu: Home: / # Delete this row if you don&#x27;t want categories in your header nav bar Categories: About: https://about.me/gonekng 각 메뉴를 클릭했을 때 이동할 경로를 지정할 수 있다. 이때 카테고리는 따로 지정하지 않아도 각 게시글에서 지정한대로 자동 적용된다. About 메뉴는 블로그 주인에 대한 자기소개 페이지로 이동하기 위한 것으로, 필자는 About.me 라는 사이트를 이용하여 만든 프로필 URL을 연결했다. 커스터마이징1234567891011121314# Customizecustomize: logo: width: 165 height: 60 url: images/logo-header.png theme_color: &#x27;#006bde&#x27; highlight: androidstudio sidebar: right # sidebar position, options: left, right thumbnail: false # enable posts thumbnail, options: true, false favicon: # path to favicon social_links: # for more icons, please see http://fontawesome.io/icons/#brand instagram: https://instagram.com/gone_kng github: https://github.com/gonekng 메뉴 위에 삽입할 로고 파일 url을 지정할 수 있다. hueman/source/css/images 폴더 내부에 저장된 이미지를 사용할 수도 있고, 웹 이미지 url도 가능하다. 테마의 색상을 지정할 수 있다. 게시글에 포함된 코드 블럭에서 적용되는 하이라이트를 지정할 수 있다. 기본값은 androidstudio이며, hueman/source/css/_highlight 폴더에 있는 것들 중 선택할 수 있다. 사이드바의 위치를 조정할 수 있다. 게시글의 썸네일을 표시하거나 숨길 수 있다. 게시글의 썸네일은 게시글에 포함된 첫번째 사진이 기본값이며, 게시글의 front-matter 부분에서 경로를 추가하면 변경 가능하다. 123title: Hello Worlddate: 2022/11/16 16:36:10thumbnail: images/example.jpg 파비콘(URL 앞에 붙는 작은 아이콘)을 지정할 수 있다. 연결하고자 하는 SNS 링크를 추가할 수 있다. 아이콘은 FontAwesome에서 선택하여 이름과 URL을 지정하면 적용된다. 위젯12345678# Widgetswidgets: - recent_posts - category - archive - tagcloud - tag - links 사이드바에 추가되는 다양한 위젯을 지정할 수 있으며, 작성한 순서대로 차례로 보여지게 된다. 링크 위젯에 들어갈 내용은 _config.yml 하단에서 다음의 코드를 통해 추가할 수 있다. 12345# Miscellaneousmiscellaneous: links: Hexo: http://hexo.io Naver blog: https://blog.naver.com/donumm 검색12345# Searchsearch: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false 블로그 내의 검색 기능을 설정할 수 있다. 필자는 테마에서 기본적으로 제공하는 Insight Search를 사용하였다. 앞서 언급했듯이 hexo-generator-json-content를 설치해야 사용 가능하다. 댓글123# Commentcomment: disqus: gonekng # enter disqus shortname here 댓글 기능은 기본적으로 제공하는 Disqus 서비스를 사용하면 된다. Disqus 사이트에 회원가입 및 로그인 후 해당하는 아이디를 입력한다. 자세한 내용은 Disqus로 블로그 댓글 기능 설정 참조 공유12# Shareshare: default # options: jiathis, bdshare, addtoany, default 해당 게시글의 공유 기능에도 몇가지 옵션이 있으나, 필자는 기본값으로 설정하였다. Result Reference https://futurecreator.github.io/2016/06/14/hexo-apply-hueman-theme/","categories":[{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/tags/hexo/"},{"name":"hueman","slug":"hueman","permalink":"http://gonekng.github.io/tags/hueman/"}],"author":"Jiwon Kang"},{"title":"Coding Test Ex.2","slug":"Python/Exercise/coding_test_ex2","date":"2022-11-15T07:45:15.000Z","updated":"2022-11-15T07:43:28.504Z","comments":true,"path":"2022/11/15/Python/Exercise/coding_test_ex2/","link":"","permalink":"http://gonekng.github.io/2022/11/15/Python/Exercise/coding_test_ex2/","excerpt":"","text":"성격유형검사문제나만의 카카오 성격 유형 검사지를 만들려고 합니다. 성격 유형 검사는 다음과 같은 4개 지표로 성격 유형을 구분합니다. 성격은 각 지표에서 두 유형 중 하나로 결정됩니다. 지표 번호 성격 유형 1번 지표 라이언형(R), 튜브형(T) 2번 지표 콘형(C), 프로도형(F) 3번 지표 제이지형(J), 무지형(M) 4번 지표 어피치형(A), 네오형(N) 4개의 지표가 있으므로 성격 유형은 총 16(&#x3D;2 x 2 x 2 x 2)가지가 나올 수 있습니다. 예를 들어, “RFMN”이나 “TCMA”와 같은 성격 유형이 있습니다. 검사지에는 총 n개의 질문이 있고, 각 질문에는 아래와 같은 7개의 선택지가 있습니다. 매우 비동의 비동의 약간 비동의 모르겠음 약간 동의 동의 매우 동의 각 질문은 1가지 지표로 성격 유형 점수를 판단합니다. 예를 들어, 어떤 한 질문에서 4번 지표로 아래 표처럼 점수를 매길 수 있습니다. 선택지 성격 유형 점수 매우 비동의 네오형 3점 비동의 네오형 2점 약간 비동의 네오형 1점 모르겠음 어떤 성격 유형도 점수를 얻지 않습니다 약간 동의 어피치형 1점 동의 어피치형 2점 매우 동의 어피치형 3점 검사자가 질문에서 약간 동의 를 선택할 경우 어피치형(A) 성격 유형 1점을 받게 됩니다. 만약 검사자가 매우 비동의 를 선택할 경우 네오형(N) 성격 유형 3점을 받게 됩니다. 위 예시처럼 네오형이 비동의, 어피치형이 동의인 경우만 주어지지 않고, 질문에 따라 네오형이 동의, 어피치형이 비동의인 경우도 주어질 수 있습니다. 하지만 각 선택지는 고정적인 크기의 점수를 가지고 있습니다. 매우 동의나 매우 비동의 를 선택하면 3점을 얻습니다. 동의나 비동의 를 선택하면 2점을 얻습니다. 약간 동의나 약간 비동의 를 선택하면 1점을 얻습니다. 모르겠음 를 선택하면 점수를 얻지 않습니다. 검사 결과는 모든 질문의 성격 유형 점수를 더하여 각 지표에서 더 높은 점수를 받은 성격 유형이 검사자의 성격 유형이라고 판단합니다. 단, 하나의 지표에서 각 성격 유형 점수가 같으면, 두 성격 유형 중 사전 순으로 빠른 성격 유형을 검사자의 성격 유형이라고 판단합니다. 질문마다 판단하는 지표를 담은 1차원 문자열 배열 survey와 검사자가 각 질문마다 선택한 선택지를 담은 1차원 정수 배열 choices가 매개변수로 주어집니다. 이때, 검사자의 성격 유형 검사 결과를 지표 번호 순서대로 return 하도록 solution 함수를 완성해주세요. 제한사항 1 ≤ survey의 길이 ( &#x3D; n) ≤ 1,000 survey의 원소는 &quot;RT&quot;, &quot;TR&quot;, &quot;FC&quot;, &quot;CF&quot;, &quot;MJ&quot;, &quot;JM&quot;, &quot;AN&quot;, &quot;NA&quot; 중 하나입니다. survey[i]의 첫 번째 캐릭터는 i+1번 질문의 비동의 관련 선택지를 선택하면 받는 성격 유형을 의미합니다. survey[i]의 두 번째 캐릭터는 i+1번 질문의 동의 관련 선택지를 선택하면 받는 성격 유형을 의미합니다. choices의 길이 &#x3D; survey의 길이 choices[i]는 검사자가 선택한 i+1번째 질문의 선택지를 의미합니다. 1 ≤ choices의 원소 ≤ 7 choices 뜻 1 매우 비동의 2 비동의 3 약간 비동의 4 모르겠음 5 약간 동의 6 동의 7 매우 동의 입출력 예시 survey choices result [“AN”, “CF”, “MJ”, “RT”, “NA”] [5, 3, 2, 7, 5] “TCMA” [“TR”, “RT”, “TR”] [7, 1, 3] “RCJA” 입출력 예 #1 1번 질문의 점수 배치는 아래 표와 같습니다. 선택지 성격 유형 점수 매우 비동의 어피치형 3점 비동의 어피치형 2점 약간 비동의 어피치형 1점 모르겠음 어떤 성격 유형도 점수를 얻지 않습니다 약간 동의 네오형 1점 동의 네오형 2점 매우 동의 네오형 3점 1번 질문에서는 지문의 예시와 다르게 비동의 관련 선택지를 선택하면 어피치형(A) 성격 유형의 점수를 얻고, 동의 관련 선택지를 선택하면 네오형(N) 성격 유형의 점수를 얻습니다. 1번 질문에서 검사자는 약간 동의 선택지를 선택했으므로 네오형(N) 성격 유형 점수 1점을 얻게 됩니다. 2번 질문의 점수 배치는 아래 표와 같습니다. 선택지 성격 유형 점수 매우 비동의 콘형 3점 비동의 콘형 2점 약간 비동의 콘형 1점 모르겠음 어떤 성격 유형도 점수를 얻지 않습니다 약간 동의 프로도형 1점 동의 프로도형 2점 매우 동의 프로도형 3점 2번 질문에서 검사자는 약간 비동의 선택지를 선택했으므로 콘형(C) 성격 유형 점수 1점을 얻게 됩니다. 3번 질문의 점수 배치는 아래 표와 같습니다. 선택지 성격 유형 점수 매우 비동의 무지형 3점 비동의 무지형 2점 약간 비동의 무지형 1점 모르겠음 어떤 성격 유형도 점수를 얻지 않습니다 약간 동의 제이지형 1점 동의 제이지형 2점 매우 동의 제이지형 3점 3번 질문에서 검사자는 비동의 선택지를 선택했으므로 무지형(M) 성격 유형 점수 2점을 얻게 됩니다. 4번 질문의 점수 배치는 아래 표와 같습니다. 선택지 성격 유형 점수 매우 비동의 라이언형 3점 비동의 라이언형 2점 약간 비동의 라이언형 1점 모르겠음 어떤 성격 유형도 점수를 얻지 않습니다 약간 동의 튜브형 1점 동의 튜브형 2점 매우 동의 튜브형 3점 4번 질문에서 검사자는 매우 동의 선택지를 선택했으므로 튜브형(T) 성격 유형 점수 3점을 얻게 됩니다. 5번 질문의 점수 배치는 아래 표와 같습니다. 선택지 성격 유형 점수 매우 비동의 네오형 3점 비동의 네오형 2점 약간 비동의 네오형 1점 모르겠음 어떤 성격 유형도 점수를 얻지 않습니다 약간 동의 어피치형 1점 동의 어피치형 2점 매우 동의 어피치형 3점 5번 질문에서 검사자는 약간 동의 선택지를 선택했으므로 어피치형(A) 성격 유형 점수 1점을 얻게 됩니다. 1번부터 5번까지 질문의 성격 유형 점수를 합치면 아래 표와 같습니다. 지표 번호 성격 유형 점수 성격 유형 점수 1번 지표 라이언형(R) 0 튜브형(T) 3 2번 지표 콘형(C) 1 프로도형(F) 0 3번 지표 제이지형(J) 0 무지형(M) 2 4번 지표 어피치형(A) 1 네오형(N) 1 각 지표에서 더 점수가 높은 T,C,M이 성격 유형입니다.하지만, 4번 지표는 1점으로 동일한 점수입니다. 따라서, 4번 지표의 성격 유형은 사전순으로 빠른 A입니다. 따라서 &quot;TCMA&quot;를 return 해야 합니다. 입출력 예 #2 1번부터 3번까지 질문의 성격 유형 점수를 합치면 아래 표와 같습니다. 지표 번호 성격 유형 점수 성격 유형 점수 1번 지표 라이언형(R) 6 튜브형(T) 1 2번 지표 콘형(C) 0 프로도형(F) 0 3번 지표 제이지형(J) 0 무지형(M) 0 4번 지표 어피치형(A) 0 네오형(N) 0 1번 지표는 튜브형(T)보다 라이언형(R)의 점수가 더 높습니다. 따라서 첫 번째 지표의 성격 유형은 R입니다.하지만, 2, 3, 4번 지표는 모두 0점으로 동일한 점수입니다. 따라서 2, 3, 4번 지표의 성격 유형은 사전순으로 빠른 C, J, A입니다. 따라서 &quot;RCJA&quot;를 return 해야 합니다. 풀이 1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145def solution(survey, choices): answer = &#x27;&#x27; score = [[0,0], # R, T [0,0], # C, F [0,0], # J, M [0,0]] # A, N for i, string in enumerate(survey): if string == &quot;RT&quot;: if choices[i] == 1: score[0][0] += 3 elif choices[i] == 2: score[0][0] += 2 elif choices[i] == 3: score[0][0] += 1 elif choices[i] == 5: score[0][1] += 1 elif choices[i] == 6: score[0][1] += 2 elif choices[i] == 7: score[0][1] += 3 else: continue elif string == &quot;TR&quot;: if choices[i] == 1: score[0][1] += 3 elif choices[i] == 2: score[0][1] += 2 elif choices[i] == 3: score[0][1] += 1 elif choices[i] == 5: score[0][0] += 1 elif choices[i] == 6: score[0][0] += 2 elif choices[i] == 7: score[0][0] += 3 else: continue elif string == &quot;CF&quot;: if choices[i] == 1: score[1][0] += 3 elif choices[i] == 2: score[1][0] += 2 elif choices[i] == 3: score[1][0] += 1 elif choices[i] == 5: score[1][1] += 1 elif choices[i] == 6: score[1][1] += 2 elif choices[i] == 7: score[1][1] += 3 else: continue elif string == &quot;FC&quot;: if choices[i] == 1: score[1][1] += 3 elif choices[i] == 2: score[1][1] += 2 elif choices[i] == 3: score[1][1] += 1 elif choices[i] == 5: score[1][0] += 1 elif choices[i] == 6: score[1][0] += 2 elif choices[i] == 7: score[1][0] += 3 else: continue elif string == &quot;JM&quot;: if choices[i] == 1: score[2][0] += 3 elif choices[i] == 2: score[2][0] += 2 elif choices[i] == 3: score[2][0] += 1 elif choices[i] == 5: score[2][1] += 1 elif choices[i] == 6: score[2][1] += 2 elif choices[i] == 7: score[2][1] += 3 else: continue elif string == &quot;MJ&quot;: if choices[i] == 1: score[2][1] += 3 elif choices[i] == 2: score[2][1] += 2 elif choices[i] == 3: score[2][1] += 1 elif choices[i] == 5: score[2][0] += 1 elif choices[i] == 6: score[2][0] += 2 elif choices[i] == 7: score[2][0] += 3 else: continue elif string == &quot;AN&quot;: if choices[i] == 1: score[3][0] += 3 elif choices[i] == 2: score[3][0] += 2 elif choices[i] == 3: score[3][0] += 1 elif choices[i] == 5: score[3][1] += 1 elif choices[i] == 6: score[3][1] += 2 elif choices[i] == 7: score[3][1] += 3 else: continue elif string == &quot;NA&quot;: if choices[i] == 1: score[3][1] += 3 elif choices[i] == 2: score[3][1] += 2 elif choices[i] == 3: score[3][1] += 1 elif choices[i] == 5: score[3][0] += 1 elif choices[i] == 6: score[3][0] += 2 elif choices[i] == 7: score[3][0] += 3 else: continue if score[0][0] &gt;= score[0][1]: answer = answer + &#x27;R&#x27; else: answer = answer + &#x27;T&#x27; if score[1][0] &gt;= score[1][1]: answer = answer + &#x27;C&#x27; else: answer = answer + &#x27;F&#x27; if score[2][0] &gt;= score[2][1]: answer = answer + &#x27;J&#x27; else: answer = answer + &#x27;M&#x27; if score[3][0] &gt;= score[3][1]: answer = answer + &#x27;A&#x27; else: answer = answer + &#x27;N&#x27; return answer 풀이 21234567891011121314151617181920def solution(survey, choices): my_dict = &#123;&quot;RT&quot;:0,&quot;CF&quot;:0,&quot;JM&quot;:0,&quot;AN&quot;:0&#125; for A,B in zip(survey,choices): if A not in my_dict.keys(): A = A[::-1] my_dict[A] -= B-4 else: my_dict[A] += B-4 result = &quot;&quot; for name in my_dict.keys(): if my_dict[name] &gt; 0: result += name[1] elif my_dict[name] &lt; 0: result += name[0] else: result += sorted(name)[0] return result 풀이 3123456789101112131415161718192021222324252627282930313233343536373839404142434445def solution(설문_조사_배열, 선택지_배열): 지표 = &#123;&#125; 지표[&#x27;RT&#x27;] = 지표[&#x27;TR&#x27;] = &#123;&#x27;R&#x27;: 0, &#x27;T&#x27;: 0,&#125; 지표[&#x27;FC&#x27;] = 지표[&#x27;CF&#x27;] = &#123;&#x27;C&#x27;: 0, &#x27;F&#x27;: 0,&#125; 지표[&#x27;MJ&#x27;] = 지표[&#x27;JM&#x27;] = &#123;&#x27;J&#x27;: 0, &#x27;M&#x27;: 0,&#125; 지표[&#x27;AN&#x27;] = 지표[&#x27;NA&#x27;] = &#123;&#x27;A&#x27;: 0, &#x27;N&#x27;: 0,&#125; 점수 = &#123; &#x27;매우 비동의&#x27;: 3, &#x27;비동의&#x27;: 2, &#x27;약간 비동의&#x27;: 1, &#x27;모르겠음&#x27;: 0, &#x27;약간 동의&#x27;: 1, &#x27;동의&#x27;: 2, &#x27;매우 동의&#x27;: 3, &#125; 비동의 = [1, 2, 3] 동의 = [5, 6, 7] 선택지 = &#123; 1: &#x27;매우 비동의&#x27;, 2: &#x27;비동의&#x27;, 3: &#x27;약간 비동의&#x27;, 4: &#x27;모르겠음&#x27;, 5: &#x27;약간 동의&#x27;, 6: &#x27;동의&#x27;, 7: &#x27;매우 동의&#x27;, &#125; answer = &#x27;&#x27; for 인덱스 in range(len(설문_조사_배열)): 비동의_캐릭터, 동의_캐릭터 = 설문_조사_배열[인덱스] if 선택지_배열[인덱스] in 비동의: 지표[설문_조사_배열[인덱스]][비동의_캐릭터] += 점수[선택지[선택지_배열[인덱스]]] continue if 선택지_배열[인덱스] in 동의: 지표[설문_조사_배열[인덱스]][동의_캐릭터] += 점수[선택지[선택지_배열[인덱스]]] 결과_배열 = [지표[&#x27;RT&#x27;].items(), 지표[&#x27;FC&#x27;].items(), 지표[&#x27;MJ&#x27;].items(), 지표[&#x27;AN&#x27;].items()] 정렬된_배열 = [] for 결과 in 결과_배열: 정렬된_배열.append(sorted(결과, key=lambda x: -x[1])) return &#x27;&#x27;.join([캐릭터_점수_튜플[0] for [캐릭터_점수_튜플, _] in 정렬된_배열]) 출처 : Programmers","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"coding test","slug":"python/coding-test","permalink":"http://gonekng.github.io/categories/python/coding-test/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"programmers","slug":"programmers","permalink":"http://gonekng.github.io/tags/programmers/"}],"author":"Jiwon Kang"},{"title":"Coding Test Ex.1","slug":"Python/Exercise/coding_test_ex1","date":"2022-11-15T06:43:00.000Z","updated":"2022-11-15T07:37:30.102Z","comments":true,"path":"2022/11/15/Python/Exercise/coding_test_ex1/","link":"","permalink":"http://gonekng.github.io/2022/11/15/Python/Exercise/coding_test_ex1/","excerpt":"","text":"K번째수문제배열 array의 i번째 숫자부터 j번째 숫자까지 자르고 정렬했을 때, k번째에 있는 수를 구하려 합니다.예를 들어 array가 [1, 5, 2, 6, 3, 7, 4], i &#x3D; 2, j &#x3D; 5, k &#x3D; 3이라면, array의 2번째부터 5번째까지 자르면 [5, 2, 6, 3]입니다. 1에서 나온 배열을 정렬하면 [2, 3, 5, 6]입니다. 2에서 나온 배열의 3번째 숫자는 5입니다. 배열 array, [i, j, k]를 원소로 가진 2차원 배열 commands가 매개변수로 주어질 때, commands의 모든 원소에 대해 앞서 설명한 연산을 적용했을 때 나온 결과를 배열에 담아 return 하도록 solution 함수를 작성해주세요. 제한사항 array의 길이는 1 이상 100 이하입니다. array의 각 원소는 1 이상 100 이하입니다. commands의 길이는 1 이상 50 이하입니다. commands의 각 원소는 길이가 3입니다. 입출력 예시 array commands return [1, 5, 2, 6, 3, 7, 4] [[2, 5, 3], [4, 4, 1], [1, 7, 3]] [5, 6, 3] [1, 5, 2, 6, 3, 7, 4]를 2번째부터 5번째까지 자른 후 정렬합니다. [2, 3, 5, 6]의 세 번째 숫자는 5입니다. [1, 5, 2, 6, 3, 7, 4]를 4번째부터 4번째까지 자른 후 정렬합니다. [6]의 첫 번째 숫자는 6입니다. [1, 5, 2, 6, 3, 7, 4]를 1번째부터 7번째까지 자릅니다. [1, 2, 3, 4, 5, 6, 7]의 세 번째 숫자는 3입니다. 풀이 11234567def solution(array, commands): answer = [] for com in commands: temp = array[com[0]-1:com[1]] temp.sort() answer.append(temp[com[2]-1]) return answer 풀이 212def solution(array, commands): return list(map(lambda x:sorted(array[x[0]-1:x[1]])[x[2]-1], commands)) 풀이 3123456def solution(array, commands): answer = [] for command in commands: i,j,k = command answer.append(list(sorted(array[i-1:j]))[k-1]) return answer 출처 : Programmers","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"coding test","slug":"python/coding-test","permalink":"http://gonekng.github.io/categories/python/coding-test/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"programmers","slug":"programmers","permalink":"http://gonekng.github.io/tags/programmers/"}],"author":"Jiwon Kang"},{"title":"Git Installation in Windows11","slug":"Setting/Git Installation in Windows11","date":"2022-10-17T07:14:08.000Z","updated":"2022-10-17T07:54:02.758Z","comments":true,"path":"2022/10/17/Setting/Git Installation in Windows11/","link":"","permalink":"http://gonekng.github.io/2022/10/17/Setting/Git%20Installation%20in%20Windows11/","excerpt":"","text":"Git 설치파일 다운로드 git-scm.com 에서 Downloads 클릭 현재 사용 중인 운영체제(Windows) 클릭 현재 사용 중인 시스템 아키텍처(64비트)에 해당하는 링크를 클릭하여 설치 파일 다운 Git Setup 마법사 실행 다운로드받은 Git Setup 파일을 실행 설치하기 위한 경로 지정 후 Next 클릭 설치할 구성요소 선택 후 Next 클릭 일반적으로 기본 상태 그대로 진행해도 무관 Additional icons On the Desktop : 바탕화면에 바로가기 아이콘 추가 Windows Explorer integration Git Bash Here : Git Bash 연결 기능 Git GUI Here : Git GUI 연결 기능 Git LFS ( Large File Support) : 대용량 파일 지원 여부 Associate .git* configuration files with the default text editor : Git 구성 파일을 기본 텍스트 편집기와 연결할지 여부 Associate .sh files to be run with Bash : .sh 확장자 파일을 Bash와 연결할지 선택 Check daily for Git for Windows updates : Git 업데이트를 매일 체크할지 여부 Add a Git Bash Profile to Windows Terminal : 윈도우 터미널에 Git Bash 추가할지 여부 시작 폴더 경로 지정 후 Next 클릭 기본 Git 에디터 선택 후 Next 클릭 기본 옵션은 Vim 편집기이며, Notepad, VSCode, Sublime 등등 선택 가능 Branch 이름 지정 옵션 선택 후 Next 클릭 Let Git decide : 기본적으로 master로 지정, 추후 변경 가능 Override the default branch name for new repositories : 입력한 이름으로 자동 지정 현재 대부분의 경우 main으로 통용되고 있음 이후 옵션들은 별도 지정이나 변경 없이 넘어가고, 마지막 Install 시 설치 진행 모든 설치가 완료된 후 Finish 클릭 Git Bash 사용자 정보 입력 Git Bash 실행 후 사용자 정보 등록 사용자 정보를 등록하면 로컬에서 Git 커밋 시 항상 이 정보가 사용됨 12git config --global user.name &quot;Name&quot;git config --global user.email &quot;Email&quot; .gitconfig에 저장되어 있는 설정 값 확인 : cat ~/.gitconfig Ref.https://iboxcomein.com/windows-git-install/","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"git","slug":"git","permalink":"http://gonekng.github.io/tags/git/"},{"name":"windows11","slug":"windows11","permalink":"http://gonekng.github.io/tags/windows11/"}],"author":"Jiwon Kang"},{"title":"웹 개발을 위한 VS code 설정","slug":"dev/Setting VS Code for Web Development","date":"2022-05-03T06:53:01.000Z","updated":"2022-12-10T14:22:39.728Z","comments":true,"path":"2022/05/03/dev/Setting VS Code for Web Development/","link":"","permalink":"http://gonekng.github.io/2022/05/03/dev/Setting%20VS%20Code%20for%20Web%20Development/","excerpt":"","text":"Install Beautify in Extension tap. Functional contribution → Command → Copy HookyQR.beautify Type Ctrl + Shift + P and paste HookyQR.beautify in the search window. Set the key binding of Beauty Selection to Ctrl + Alt + L. Install Live Server in Extension tap. Install Auto Rename Tag in Extension tap.","categories":[{"name":"development","slug":"development","permalink":"http://gonekng.github.io/categories/development/"}],"tags":[{"name":"vscode","slug":"vscode","permalink":"http://gonekng.github.io/tags/vscode/"},{"name":"development","slug":"development","permalink":"http://gonekng.github.io/tags/development/"}],"author":"Jiwon Kang"},{"title":"SQL TEST 6-7","slug":"SQL/SQL TEST 6-7","date":"2022-05-02T03:08:45.000Z","updated":"2022-11-16T15:14:56.484Z","comments":true,"path":"2022/05/02/SQL/SQL TEST 6-7/","link":"","permalink":"http://gonekng.github.io/2022/05/02/SQL/SQL%20TEST%206-7/","excerpt":"","text":"SQL Subquery ‘오라클 SQL과 PL&#x2F;SQL을 다루는 기술 (길벗)’ Q1.populations 테이블에서 2015년 평균 기대수명보다 높은 모든 정보를 조회한다. A1.12345678SELECT * FROM populations WHERE year = 2015 AND life_expectancy &gt; (SELECT AVG(life_expectancy) as AVG FROM populations WHERE year = 2015 GROUP BY year); Q2.subquery_countries 테이블에 있는 capital과 매칭되는 cities 테이블의 정보를 조회한다. A2.123456SELECT a.name, a.country_code, a.urbanarea_pop FROM cities a WHERE a.name IN (SELECT b.capital FROM subquery_countries b) ORDER BY a.urbanarea_pop desc; Q3.economies 테이블에서 code, inflation_rate, unemployment_rate를 조회한다. inflation_rate 오름차순으로 정렬한다. subquery_countries 테이블내 gov_form 컬럼에서 Constitutional Monarchy 또는 Republic이 들어간 국가는 제외한다. A3.123456789SELECT a.code, a.inflation_rate, a.unemployment_rate FROM economies a WHERE a.year = 2015 AND a.code NOT IN (SELECT b.code FROM subquery_countries b WHERE b.gov_form = &#x27;Constitutional Monarchy&#x27; OR b.gov_form LIKE &#x27;%Republic%&#x27;) ORDER BY a.inflation_rate ASC; Q4.2015년 각 대륙별 inflation_rate가 가장 심한 국가와 inflation_rate를 구한다. 힌트) 아래 쿼리 실행 123456SELECT country_name, continent, inflation_rate FROM subquery_countries INNER JOIN economies USING (code) WHERE year = 2015; A4.123456789101112131415With basis AS ( SELECT country_name, continent, inflation_rate FROM subquery_countries INNER JOIN economies USING (code) WHERE year = 2015 ) , max_inf AS ( SELECT continent, MAX(inflation_rate) as inflation_rate FROM basis GROUP BY continent )SELECT a.country_name, b.continent, b.inflation_rate FROM basis a, max_inf b WHERE a.inflation_rate = b.inflation_rate; SQL Window FunctionQ1.각 행에 숫자를 1, 2, 3, … 형태로 추가한다. (row_n으로 표시) row_n 기준으로 오름차순으로 출력 테이블명에 alias를 적용한다. A1.12345678With sub_table AS ( SELECT ROWNUM-95 AS ROW_N , YEAR, CITY, SPORT, DISCIPLINE, ATHLETE FROM summer_medals )SELECT * FROM sub_table WHERE ROW_N &gt; 0; Q2.올림픽 년도를 오름차순 순번대로 작성한다. 힌트) 서브쿼리와 윈도우 함수를 이용한다. A2.12345SELECT year , ROW_NUMBER() OVER (ORDER BY year) as ROW_N FROM (SELECT year FROM summer_medals GROUP BY year); Q3.(1) WITH 절 사용하여 각 운동선수들이 획득한 메달 갯수를 내림차순으로 정렬하도록 한다.(2) (1) 쿼리를 활용하여 그리고 선수들의 랭킹을 추가한다. 상위 5개만 추출 : OFFSET 0 ROWS FETCH NEXT 5 ROWS ONLY A3.123456789With basis AS ( SELECT ATHLETE, COUNT() AS MEDALS FROM summer_medals GROUP BY ATHLETE ORDER BY COUNT() desc )SELECT MEDALS, ATHLETE, ROWNUM FROM basis OFFSET 0 ROWS FETCH NEXT 5 ROWS ONLY; Q4.다음 쿼리는 남자 69KG 역도 경기에서 매년 금메달리스트 조회하는 쿼리이다. 이때 매년 전년도 챔피언도 같이 조회하도록 한다. (LAG &amp; WITH절 사용) 123456SELECT Year, Country AS champion FROM summer_medals WHERE Discipline = &#x27;Weightlifting&#x27; AND Event = &#x27;69KG&#x27; AND Gender = &#x27;Men&#x27; AND Medal = &#x27;Gold&#x27;; A4.1234567891011WITH basis AS ( SELECT Year, Country AS champion FROM summer_medals WHERE Discipline = &#x27;Weightlifting&#x27; AND Event = &#x27;69KG&#x27; AND Gender = &#x27;Men&#x27; AND Medal = &#x27;Gold&#x27; )SELECT year, champion , LAG(champion, 1) OVER(order by champion) AS LAST_CHAMPION FROM basis;","categories":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/categories/sql/"}],"tags":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/tags/sql/"},{"name":"oracle","slug":"oracle","permalink":"http://gonekng.github.io/tags/oracle/"}],"author":"Jiwon Kang"},{"title":"SQL EXERCISE 6-7","slug":"SQL/SQL EXERCISE 6-7","date":"2022-05-02T00:32:11.000Z","updated":"2022-11-16T15:14:53.482Z","comments":true,"path":"2022/05/02/SQL/SQL EXERCISE 6-7/","link":"","permalink":"http://gonekng.github.io/2022/05/02/SQL/SQL%20EXERCISE%206-7/","excerpt":"","text":"CHAPTER 06 ‘오라클 SQL과 PL&#x2F;SQL을 다루는 기술 (길벗)’ Q1.101번 사원에 대해 아래의 결과를 산출하는 쿼리를 작성해 보자. 123---------------------------------------------------------------------------------------사번 사원명 job명칭 job시작일자 job종료일자 job수행부서명--------------------------------------------------------------------------------------- A1.1234567891011121314SELECT a.employee_id 사번 , a.emp_name 사원명 , b.job_title job 명칭 , c.start_date job 시작일자 , c.end_date job 종료일자 , d.department_name FROM employees a , jobs b , job_history c , departments d WHERE a.employee_id = c.employee_id AND b.job_id = c.job_id AND c.department_id = d.department_id AND a.employee_id = 101; 필요한 컬럼 및 테이블 사번(employee_id), 사원명(emp_name) → employees job명칭(job_title) → jobs job시작일자(start_date), job종료일자(end_date) → job_history job수행부서명(department_name) → departments 테이블 조인 조건 employees &amp; job_history → employee_id jobs &amp; job_history → job_id job_history &amp; departments → department_id 기타 조건 101번 사원에 대한 정보 : a.employee_id = 101 Q2.아래의 쿼리를 수행하면 오류가 발생한다. 오류의 원인은 무엇인가? 12345select a.employee_id, a.emp_name, b.job_id, b.department_idfrom employees a,job_history bwhere a.employee_id = b.employee_id(+)and a.department_id(+) = b.department_id; A2.(+) 연산자를 활용한 외부 조인의 경우 한쪽 방향으로만 가능하고, 이때 (+) 연산자는 데이터가 없는 테이블의 컬럼에만 붙여야 한다. 따라서, 위의 쿼리에서는 마지막 줄을 and a.department_id = b.department_id(+)로 수정해야 한다. Q3.외부조인시 (+)연산자를 같이 사용할 수 없는데, IN절에 사용하는 값이 1개인 경우는 사용 가능하다. 그 이유는 무엇일까? A3.IN절에 사용하는 값이 1개인 경우는 등호를 사용하는 것과 같은 의미이므로 사용 가능하다. Q4.다음의 쿼리를 ANSI 문법으로 변경해 보자. 1234567SELECT a.department_id , a.department_name FROM departments a , employees b WHERE a.department_id = b.department_id AND b.salary &gt; 3000 ORDER BY a.department_name; A4.123456SELECT a.department_id, a.department_name FROM departments a INNER JOIN employees b On (a.department_id = b.department_id AND b.salary &gt; 3000) ORDER BY a.department_name; 위의 쿼리는 departments 테이블과 employees 테이블의 내부 조인이다. ANSI 문법에서 내부 조인은 FROM절에서 INNER JOIN 으로 수행하며,조인 조건은 ON 절에 명시한다. Q5.다음은 연관성 있는 서브쿼리이다. 이를 연관성 없는 서브쿼리로 변환해 보자. 123456SELECT a.department_id , a.department_name FROM departments a WHERE EXISTS ( SELECT 1 FROM job_history b WHERE a.department_id = b.department_id ); A5.123456SELECT a.department_id , a.department_name FROM departments a WHERE a.department_id IN (SELECT b.department_id FROM job_history b); 위의 쿼리는 job_history 테이블에 존재하는 department_id에 대해departments 테이블의 department_id와 department_name을 출력한다. 이를 연관성 없는 서브쿼리로 변환하기 위해조인 조건 대신 IN 연산자를 통해 메인 쿼리의 조건으로 활용했다. Q6.연도별 이태리 최대매출액과 사원을 작성하는 쿼리를 학습했다. 이를 기준으로 최대 매출액, 최소매출액, 해당 사원을 조회하는 쿼리를 작성해 보자. A6.1234567891011121314151617181920212223242526272829303132333435363738SELECT emp.sales_year , emp.employee_id , emp2.emp_name , emp.amount_sold FROM (SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = &#x27;Italy&#x27; GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id) emp , (SELECT sales_year , MAX(amount_sold) AS max_sold , MIN(amount_sold) AS min_sold FROM (SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = &#x27;Italy&#x27; GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id) k GROUP BY sales_year) sale , employees emp2 WHERE emp.sales_year = sale.sales_year AND (emp.amount_sold = sale.max_sold OR emp.amount_sold = sale.min_sold) AND emp.employee_id = emp2.employee_id ORDER BY sales_year; 서브쿼리 1 : 연도, 사원별 이탈리아 매출액 (emp) sales, customers, countries를 조인하여 매출액 합계 계산 123456789101112SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = &#x27;Italy&#x27; GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id 서브쿼리 2: 연도별 최대, 최소 매출액 (sale) emp 서브쿼리에서 연도별 최대, 최소값 계산 12345678910111213141516SELECT sales_year , MAX(amount_sold) AS max_sold , MIN(amount_sold) AS min_sold FROM (SELECT SUBSTR(a.sales_month, 1, 4) AS sales_year , a.employee_id , SUM(a.amount_sold) as amount_sold FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id AND c.country_name = &#x27;Italy&#x27; GROUP BY SUBSTR(a.sales_month, 1, 4) , a.employee_id) k GROUP BY sales_year CHAPTER 07Q1.계층형 쿼리 응용편에서 LISTAGG 함수를 사용해 다음과 같이 로우를 컬럼으로 분리했었다. 12345SELECT department_id,LISTAGG(emp_name, &#x27;,&#x27;) WITHIN GROUP (ORDER BY emp_name) as empnamesFROM employeesWHERE department_id IS NOT NULLGROUP BY department_id; LISTAGG 함수 대신 계층형 쿼리, 분석함수를 사용해서 위 쿼리와 동일한 결과를 산출하는 쿼리를 작성해 보자. A1.123456789101112131415SELECT department_id , SUBSTR(SYS_CONNECT_BY_PATH(emp_name, &#x27;,&#x27;),2) empnames FROM ( SELECT emp_name , department_id , COUNT(*) OVER (PARTITION BY department_id) cnt , ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY emp_name) rowseq FROM employees WHERE department_id IS NOT NULL ) WHERE rowseq = cnt START WITH rowseq = 1 CONNECT BY PRIOR rowseq + 1 = rowseq AND PRIOR department_id = department_id; 서브쿼리 : 부서별 사원명, 사원 수, 행 번호 구하기 부서별 파티션 : PARTITION BY department_id ORDER BY emp_name 1234567SELECT emp_name , department_id , COUNT(*) OVER (PARTITION BY department_id) cnt , ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY emp_name) rowseq FROM employees WHERE department_id IS NOT NULL 각 파티션의 마지막 행에 대하여(WHERE rowseq = cnt)파티션의 첫 행부터(START WITH rowseq = 1)부서번호가 같은 직전 행까지(CONNECT BY PRIOR rowseq + 1 = rowseq AND PRIOR department_id = department_id)의 emp_name을연결하여 나타낸다.(SUBSTR(SYS_CONNECT_BY_PATH(emp_name, &#39;,&#39;),2)) Q2.아래의 쿼리는 사원테이블에서 JOB_ID가 ‘SH_CLERK‘인 사원을 조회하는 쿼리이다. 12345678910111213141516SELECT employee_id, emp_name, hire_dateFROM employeesWHERE job_id = &#x27;SH_CLERK&#x27;ORDER By hire_date;EMPLOYEE_ID EMP_NAME HIRE_DATE ----------- -------------------- ------------------- 184 Nandita Sarchand 2004/01/27 00:00:00 192 Sarah Bell 2004/02/04 00:00:00 185 Alexis Bull 2005/02/20 00:00:00 193 Britney Everett 2005/03/03 00:00:00 188 Kelly Chung 2005/06/14 00:00:00.... .... 199 Douglas Grant 2008/01/13 00:00:00 183 Girard Geoni 2008/02/03 00:00:00 사원테이블에서 퇴사일자(retire_date)는 모두 비어있는데,위 결과에서 사원번호가 184인 사원의 퇴사일자는 다음으로 입사일자가 빠른 192번 사원의 입사일자라고 가정해서다음과 같은 형태로 결과를 추출해낼 수 있도록 쿼리를 작성해 보자. (입사일자가 가장 최근인 183번 사원의 퇴사일자는 NULL이다) 1234567891011EMPLOYEE_ID EMP_NAME HIRE_DATE RETIRE_DATE----------- -------------------- ------------------- --------------------------- 184 Nandita Sarchand 2004/01/27 00:00:00 2004/02/04 00:00:00 192 Sarah Bell 2004/02/04 00:00:00 2005/02/20 00:00:00 185 Alexis Bull 2005/02/20 00:00:00 2005/03/03 00:00:00 193 Britney Everett 2005/03/03 00:00:00 2005/06/14 00:00:00 188 Kelly Chung 2005/06/14 00:00:00 2005/08/13 00:00:00.... .... 199 Douglas Grant 2008/01/13 00:00:00 2008/02/03 00:00:00 183 Girard Geoni 2008/02/03 00:00:00 A2.123456789SELECT employee_id , emp_name , hire_date , LEAD(hire_date) OVER (PARTITION BY job_id ORDER BY hire_date) AS retire_date FROM employees WHERE job_id = &#x27;SH_CLERK&#x27; ORDER BY hire_date; 문제에서 요구하는 퇴사일자(retire_date)는입사일자로 정렬했을 때 다음 사원의 입사일자(hire_date)와 같다. 따라서, 다음 행의 데이터를 가져오는 LEAD(hire_date) 함수를 통해각 사원의 퇴사일자(retire_date)를 산출할 수 있다. Q3.sales 테이블에는 판매데이터, customers 테이블에는 고객정보가 있다.2001년 12월 판매데이터 중 현재일자를 기준으로 고객의 나이를 계산해서 다음과 같이 연령대별 매출금액을 보여주는 쿼리를 작성해 보자. 12345678------------------------- 연령대 매출금액-------------------------10대 xxxxxx20대 ....30대 .... 40대 ....------------------------- A3.12345678910111213141516WITH age_amt AS ( SELECT TRUNC((TO_CHAR(SYSDATE, &#x27;yyyy&#x27;) - b.cust_year_of_birth), -1) AS age_seg , SUM(a.amount_sold) AS amount FROM sales a , customers b WHERE a.sales_month = &#x27;200112&#x27; AND a.cust_id = b.cust_id GROUP BY TRUNC((TO_CHAR(SYSDATE, &#x27;yyyy&#x27;) - b.cust_year_of_birth), -1) )SELECT * FROM age_amt ORDER BY age_seg; 서브쿼리 : 현재일자 기준 고객 연령대별 매출액 구하기 (age_amt) 현재일자를 기준으로 고객의 나이를 계산한 다음(TO_CHAR(SYSDATE, &#39;yyyy&#39;) - b.cust_year_of_birth)각 연령대별 amount_sold의 합계를 계산하였음 1234567891011SELECT TRUNC((TO_CHAR(SYSDATE, &#x27;yyyy&#x27;) - b.cust_year_of_birth), -1) AS age_seg , SUM(a.amount_sold) AS amount FROM sales a , customers b WHERE a.sales_month = &#x27;200112&#x27; AND a.cust_id = b.cust_id GROUP BY TRUNC((TO_CHAR(SYSDATE, &#x27;yyyy&#x27;) - b.cust_year_of_birth), -1) Q4.월별로 판매금액이 가장 하위에 속하는 대륙 목록을 뽑아보자.(대륙목록은 countries 테이블의 country_region에 있으며, country_id 컬럼으로 customers 테이블과 조인을 해서 구한다.) 1234567--------------------------------- 매출월 지역(대륙) 매출금액 ---------------------------------199801 Oceania xxxxxx199803 Oceania xxxxxx...--------------------------------- A4.12345678910111213141516171819202122232425WITH basis AS ( SELECT a.sales_month , c.country_region , SUM(a.amount_sold) as amt FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id GROUP BY a.sales_month, c.country_region ) , month_amt AS ( SELECT sales_month AS &quot;매출월&quot; , country_region AS &quot;지역(대륙)&quot; , amt AS &quot;매출금액&quot; , RANK() OVER (PARTITION BY sales_month ORDER BY amt) AS ranks FROM basis )SELECT &quot;매출월&quot;, &quot;지역(대륙)&quot;, &quot;매출금액&quot; FROM month_amt WHERE ranks = 1; 서브쿼리 1 : 월별, 지역별 판매금액 합계 구하기 (basis) sales, customers, countries 조인 월별, 지역별 합계 : SUM(a.amount_sold) as amt 123456789SELECT a.sales_month , c.country_region , SUM(a.amount_sold) as amt FROM sales a , customers b , countries c WHERE a.cust_id = b.cust_id AND b.country_id = c.country_id GROUP BY a.sales_month, c.country_region 서브쿼리 2 : 월별로 각 대륙의 판매금액 합계 순위 구하기 (month_amt) basis 서브쿼리에서 sales_month 파티션별 amt 순위값 계산 1234567SELECT sales_month AS &quot;매출월&quot; , country_region AS &quot;지역(대륙)&quot; , amt AS &quot;매출금액&quot; , RANK() OVER (PARTITION BY sales_month ORDER BY amt) AS ranks FROM basis Q5.5장 연습문제 5번의 정답 결과를 이용해 다음과 같이 지역별, 대출종류별, 월별 대출잔액과 지역별 파티션을 만들어대출종류별 대출잔액의 %를 구하는 쿼리를 작성해보자. 123456789------------------------------------------------------------------------------------------------지역 대출종류 201111 201112 201210 201211 201212 203110 201311------------------------------------------------------------------------------------------------서울 기타대출 73996.9( 36% )서울 주택담보대출 130105.9( 64% ) 부산......------------------------------------------------------------------------------------------------- A5.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354WITH basis AS ( SELECT region, gubun , CASE WHEN period = &#x27;201111&#x27; THEN loan_jan_amt ELSE 0 END amt1 , CASE WHEN period = &#x27;201112&#x27; THEN loan_jan_amt ELSE 0 END amt2 , CASE WHEN period = &#x27;201210&#x27; THEN loan_jan_amt ELSE 0 END amt3 , CASE WHEN period = &#x27;201211&#x27; THEN loan_jan_amt ELSE 0 END amt4 , CASE WHEN period = &#x27;201212&#x27; THEN loan_jan_amt ELSE 0 END amt5 , CASE WHEN period = &#x27;201310&#x27; THEN loan_jan_amt ELSE 0 END amt6 , CASE WHEN period = &#x27;201311&#x27; THEN loan_jan_amt ELSE 0 END amt7 FROM kor_loan_status ) , sum_amt AS ( SELECT region, gubun , SUM(amt1) AS amt1 , SUM(amt2) AS amt2 , SUM(amt3) AS amt3 , SUM(amt4) AS amt4 , SUM(amt5) AS amt5 , SUM(amt6) AS amt6 , SUM(amt7) AS amt7 FROM basis GROUP BY region, gubun )SELECT region AS &quot;지역&quot;, gubun AS &quot;대출종류&quot; , amt1 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt1) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201111&quot; , amt2 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt2) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201112&quot; , amt3 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt3) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201210&quot; , amt4 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt4) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201211&quot; , amt5 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt5) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201212&quot; , amt6 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt6) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201311&quot; , amt7 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt7) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201311&quot; FROM sum_amt ORDER BY region; 서브쿼리 1 : 월별 대출잔액 변수 만들기 (basis) CASE WHEN ~ THEN ~ ELSE 구문으로 월별 대출잔액 변수 생성 12345678910111213141516SELECT region, gubun , CASE WHEN period = &#x27;201111&#x27; THEN loan_jan_amt ELSE 0 END amt1 , CASE WHEN period = &#x27;201112&#x27; THEN loan_jan_amt ELSE 0 END amt2 , CASE WHEN period = &#x27;201210&#x27; THEN loan_jan_amt ELSE 0 END amt3 , CASE WHEN period = &#x27;201211&#x27; THEN loan_jan_amt ELSE 0 END amt4 , CASE WHEN period = &#x27;201212&#x27; THEN loan_jan_amt ELSE 0 END amt5 , CASE WHEN period = &#x27;201310&#x27; THEN loan_jan_amt ELSE 0 END amt6 , CASE WHEN period = &#x27;201311&#x27; THEN loan_jan_amt ELSE 0 END amt7 FROM kor_loan_status 서브쿼리 2 : 지역, 구분으로 그룹화하여 월별 합계 산출 (sum_amt) 1234567SELECT region, gubun , SUM(amt1) AS amt1, SUM(amt2) AS amt2 , SUM(amt3) AS amt3, SUM(amt4) AS amt4 , SUM(amt5) AS amt5, SUM(amt6) AS amt6 , SUM(amt7) AS amt7 FROM basis GROUP BY region, gubun 메인 쿼리 : 지역 내 대출종류별 대출잔액의 비율 산출 1234567891011121314151617SELECT region AS &quot;지역&quot;, gubun AS &quot;대출종류&quot; , amt1 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt1) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201111&quot; , amt2 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt2) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201112&quot; , amt3 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt3) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201210&quot; , amt4 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt4) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201211&quot; , amt5 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt5) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201212&quot; , amt6 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt6) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201311&quot; , amt7 || &#x27;(&#x27; || ROUND(RATIO_TO_REPORT(amt7) OVER (PARTITION BY region), 3) *100 || &#x27;%)&#x27; AS &quot;201311&quot; FROM sum_amt ORDER BY region","categories":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/categories/sql/"}],"tags":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/tags/sql/"},{"name":"oracle","slug":"oracle","permalink":"http://gonekng.github.io/tags/oracle/"}],"author":"Jiwon Kang"},{"title":"SQL Developer 깃허브 연동하기","slug":"SQL/Conneting SQL Developer with Github","date":"2022-04-26T07:02:41.000Z","updated":"2022-11-16T15:25:13.586Z","comments":true,"path":"2022/04/26/SQL/Conneting SQL Developer with Github/","link":"","permalink":"http://gonekng.github.io/2022/04/26/SQL/Conneting%20SQL%20Developer%20with%20Github/","excerpt":"","text":"Step 1. Github 준비 Github에서 SQL Developer와 연동할 새로운 Public Repository를 생성한다. Settings &gt; Developer settings 에서 새로운 Personal access token을 발급받는다. 새로 생성한 Repository의 이름을 입력하고 Select scopes에서 repo를 체크한 후 토큰을 생성한다. 이때 생성된 토큰 번호는 다시 알 수 없기 때문에, 발급 즉시 복사하여 다른 곳에 저장해두어야 함 Step 2. SQL Developer Git 복제 SQL Developer에서 팀 &gt; Git &gt; 복제 를 클릭하여 복제 마법사를 실행한다. 다음 버튼을 클릭한다. 앞서 생성한 Repository의 URL을 입력하고 Github 사용자 이름 및 비밀번호를 입력한다. main 분기를 선택한 후 다음 버튼을 클릭한다. 로컬 Git 저장소 경로 및 이름을 지정한다. 입력한 정보를 확인한 후 완료 버튼을 클릭한다. Step 3. Git Push 테스트 SQL Developer에서 파일 &gt; 새로 만들기 &gt; 모든 항목 을 클릭하여 SQL 파일을 생성한다. 새로운 파일의 이름을 입력한 후 디렉토리에는 Github와 연결한 로컬 폴더를 지정한다. 간단한 SQL 쿼리를 작성, 실행, 저장한 다음 팀 &gt; Git &gt; 모두 커밋 을 클릭한다. 작성자와 커밋한 사람에 이름을 입력한 다음 확인 버튼을 클릭한다. 팀 &gt; Git &gt; 푸시 를 클릭하여 푸시 마법사를 실행한다. 푸시 마법사의 안내에 따라 진행한다. 다음과 같은 에러가 발생할 경우 Step 2의 Git 복제 마법사를 다시 실행하고, Github 비밀번호에 앞서 발급받은 토큰 번호를 입력한다. Perform the push operation again and check that it is uploaded normally.","categories":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/categories/sql/"}],"tags":[{"name":"github","slug":"github","permalink":"http://gonekng.github.io/tags/github/"},{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/tags/sql/"},{"name":"oracle","slug":"oracle","permalink":"http://gonekng.github.io/tags/oracle/"}],"author":"Jiwon Kang"},{"title":"Oracle 19c Installation in Windows11","slug":"SQL/Oracle 19c Installation in Windows11","date":"2022-04-25T08:27:10.000Z","updated":"2022-11-16T15:15:02.904Z","comments":true,"path":"2022/04/25/SQL/Oracle 19c Installation in Windows11/","link":"","permalink":"http://gonekng.github.io/2022/04/25/SQL/Oracle%2019c%20Installation%20in%20Windows11/","excerpt":"","text":"Step 1. Install Oracle Database Run the setup file as administrator and follow the procedure below. If the following error occurs, go back to the beginning and change to ‘Software Only Settings’. Creating and configuring a single instance database : Installing myoracle and database Software Only Settings : Installing myoracle only Run a cmd as administrator and enter the code below. (if you changed to ‘Software Only Settings’.) C:\\sql_lecture\\WINDOWS.X64_193000_db_home&gt;**dbca** If the error above occurs again, proceed as follows. Step 2. Create Tablespace by SQL Plus Run SQL Plus as administrator and enter the username and password. Enter the SQL code below. 12# Create a new tablespaceCREATE TABLESPACE myts DATAFILE &#x27;C:\\sql_lecture\\oradata\\MYORACLE\\myts.dbf&#x27; SIZE 100M AUTOEXTEND ON NEXT 5M; 12# Create a new userCREATE USER ora_user IDENTIFIED BY jiwon DEFAULT TABLESPACE MYTS TEMPORARY TABLESPACE TEMP; 1234# Grant a DBA role to the userGRANT DBA TO ora_user;권한이 부여되었습니다. 1234# Connect to the database as the userconnect ora_user/jiwon;연결되었습니다. 123456# Print out the currently logged-in usernameselect user from dual;USER--------------------------------------------------------------------------------ORA_USER Step 3. Install SQL Developer Run the setup file. Click No if the warning below occurs. Run a CMD as administrator and enter the code below. C:\\WINDOWS\\system32&gt;lsnrctl status If an Unknown error occurs as described above, run the Net Configuration Assistant. If you input the code at the CMD again, the listener information is printed normally. Create a new Database Access. Tool &gt; Setting &gt; Database &gt; NLS Enter YYYY/MM/DD HH24:MI:SS in ‘Time Record Format’. Write the query below and check the result. 1SELECT user from DUAL; Create a backup folder under the C drive and download expall.dmp and expcust.dmp URL : https://github.com/gilbutITbook/006696/tree/master/01장 환경설정 Run a CMD as administrator and enter the code below at the backup folder. 1imp ora_user/evan file=expall.dmp log=empall.log ignore=y grants=y rows=y indexes=y full=y 1imp ora_user/evan file=expcust.dmp log=expcust.log ignore=y grants=y rows=y indexes=y full=y Write the query below and check the result. 1SELECT table_name FROM user_tables; In SQL Plus, make sure that the user is created correctly.","categories":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/categories/sql/"}],"tags":[{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/tags/sql/"},{"name":"oracle","slug":"oracle","permalink":"http://gonekng.github.io/tags/oracle/"}],"author":"Jiwon Kang"},{"title":"Crawling Music Chart Top100","slug":"Python/Crawling/Crawling Music Chart Top100","date":"2022-04-22T08:10:19.000Z","updated":"2022-10-05T05:39:51.855Z","comments":true,"path":"2022/04/22/Python/Crawling/Crawling Music Chart Top100/","link":"","permalink":"http://gonekng.github.io/2022/04/22/Python/Crawling/Crawling%20Music%20Chart%20Top100/","excerpt":"","text":"Website Info Request URL : https://music.bugs.co.kr/chart Request Method : GET User-Agent: Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;100.0.4896.127 Safari&#x2F;537.36 Crawling Code step03_bugsTop100.py 12345678910111213141516171819202122232425262728293031323334353637383940import requestsimport warningsfrom bs4 import BeautifulSoupwarnings.filterwarnings(&#x27;ignore&#x27;)import pandas as pddef crawling(soup): chart = soup.find(&quot;table&quot;, class_=&quot;list trackList byChart&quot;) titles = [] artists = [] for p in chart.find_all(&quot;p&quot;, class_=&quot;title&quot;): titles.append(p.get_text()[1:-1]) for p in chart.find_all(&quot;p&quot;, class_=&quot;artist&quot;): artists.append(p.get_text()[1:-1]) return(titles, artists)def df_csv(tp): df = pd.DataFrame(&#123;&quot;title&quot; : tp[0], &quot;artist&quot; : tp[1]&#125;) print(df) df.to_csv(&quot;top100.csv&quot;, index=False) print(&quot;Crawling is done!&quot;)def main(): CUSTOM_HEADER = &#123; &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://music.bugs.co.kr/chart&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) df_csv(crawling(soup))if __name__ == &quot;__main__&quot;: main() top100.csv","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"crawling","slug":"python/crawling","permalink":"http://gonekng.github.io/categories/python/crawling/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"crawling","slug":"crawling","permalink":"http://gonekng.github.io/tags/crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"http://gonekng.github.io/tags/BeautifulSoup/"}],"author":"Jiwon Kang"},{"title":"Crawling Headline News","slug":"Python/Crawling/Crawling Headline News","date":"2022-04-22T06:22:10.000Z","updated":"2022-10-05T05:39:51.695Z","comments":true,"path":"2022/04/22/Python/Crawling/Crawling Headline News/","link":"","permalink":"http://gonekng.github.io/2022/04/22/Python/Crawling/Crawling%20Headline%20News/","excerpt":"","text":"Check the Website Info Access Developer Tools of the website and enter the Nework tab. Type ctrl + R and enter the Doc tap. Enter a site and check the Headers tap with the site. Copy the value of referer and user-agent. Crawling Code step01_headlinenews.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445import warningsimport requestsfrom bs4 import BeautifulSoupwarnings.filterwarnings(&#x27;ignore&#x27;)import pandas as pddef crawling(soup): div = soup.find(&quot;div&quot;, class_=&quot;list_issue&quot;) print(type(div)) titles = [] urls = [] for a in div.find_all(&quot;a&quot;): titles.append(a.get_text()) urls.append(a[&#x27;href&#x27;]) results = (titles, urls) return(results)def df_csv(tp): df = pd.DataFrame(&#123;&quot;newstitle&quot; : tp[0], &quot;url&quot; : tp[1]&#125;) print(df) df.to_csv(&quot;headlinecrawling.csv&quot;, index=False) print(&quot;Crawling is done!&quot;)def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://www.naver.com/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://www.naver.com/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) # 200 : Good # 404 : URL Error # 503 : Server Down soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;, from_encoding=&#x27;utf-8&#x27;) df_csv(crawling(soup))if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"crawling","slug":"python/crawling","permalink":"http://gonekng.github.io/categories/python/crawling/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"crawling","slug":"crawling","permalink":"http://gonekng.github.io/tags/crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"http://gonekng.github.io/tags/BeautifulSoup/"}],"author":"Jiwon Kang"},{"title":"Crawling Data from Web","slug":"Python/Crawling/Crawling Data from Web","date":"2022-04-22T03:39:12.000Z","updated":"2022-10-05T05:39:51.545Z","comments":true,"path":"2022/04/22/Python/Crawling/Crawling Data from Web/","link":"","permalink":"http://gonekng.github.io/2022/04/22/Python/Crawling/Crawling%20Data%20from%20Web/","excerpt":"","text":"Step 1. Set virtual environment Create a new directory under the C drive and virtual environment. 123$ mkdir crawling &amp;&amp; cd crawling$ virtualenv venv$ sourve venv/Scipts/activate Install some required packages. 123$ pip install beautifulsoup4$ pip install numpy pandas matplotlib seaborn$ pip install requests Step 2. Crawling Practice 1 Create a HTML file index.html 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;titl&gt;test&lt;/titl&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;aaaaaaaa&lt;/h1&gt; &lt;h2&gt;dddd&lt;/h2&gt; &lt;div class=&quot;chapter01&quot;&gt; &lt;p&gt;Don&#x27;t Crawl here &lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;chapter02&quot;&gt; &lt;p&gt;Just Crawling here&lt;/p&gt; &lt;/div&gt; &lt;div id=&quot;main&quot;&gt; &lt;p&gt; Crawling .................. &lt;/p&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; Create a python file main.py crawling text from index.html 12345678910111213141516from bs4 import BeautifulSoupdef main(): # Convert index.html to BeautifulSoup Object soup = BeautifulSoup(open(&quot;index.html&quot;, encoding=&#x27;UTF-8&#x27;), &quot;html.parser&quot;) print(type(soup)) print(soup.find(&quot;p&quot;)) print(&quot;----------------&quot;) print(soup.find_all(&quot;p&quot;)) print(&quot;----------------&quot;) print(soup.find(&quot;div&quot;, class_ = &quot;chapter02&quot;)) print(&quot;----------------&quot;) print(soup.find(&quot;div&quot;, id = &quot;main&quot;).find(&quot;p&quot;).get_text())if __name__ == &quot;__main__&quot;: main() Run the main.py and check the result printed. 1234567891011$ python main.py&lt;class &#x27;bs4.BeautifulSoup&#x27;&gt;&lt;p&gt;Don&#x27;t crawl here!&lt;/p&gt;----------------[&lt;p&gt;Don&#x27;t crawl here!&lt;/p&gt;, &lt;p&gt;Just Crawl here!&lt;/p&gt;, &lt;p&gt; Crawling .................. &lt;/p&gt;]----------------&lt;div class=&quot;chapter02&quot;&gt;&lt;p&gt;Just Crawl here!&lt;/p&gt;&lt;/div&gt;---------------- Step 3. Quick Start BeautifulSoup4 URL : https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start index2.html 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; temp1.py 1234from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index2.html&quot;), &#x27;html.parser&#x27;)print(soup.prettify()) 123456789101112131415161718192021222324252627282930313233343536$ python temp1.py&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse&#x27;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt; &lt;b&gt; The Dormouse&#x27;s story &lt;/b&gt; &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; Elsie &lt;/a&gt; , &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt; Lacie &lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt; Tillie &lt;/a&gt; ; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; ... &lt;/p&gt; &lt;/body&gt;&lt;/html&gt; temp2.py 1234567891011121314151617181920from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index2.html&quot;), &#x27;html.parser&#x27;)print(soup.title)print(&quot;----------------&quot;)print(soup.title.name)print(&quot;----------------&quot;)print(soup.title.string)print(&quot;----------------&quot;)print(soup.title.parent.name)print(&quot;----------------&quot;)print(soup.p)print(&quot;----------------&quot;)print(soup.p[&#x27;class&#x27;])print(&quot;----------------&quot;)print(soup.a)print(&quot;----------------&quot;)print(soup.find_all(&#x27;a&#x27;))print(&quot;----------------&quot;)print(soup.find(id=&quot;link3&quot;)) 12345678910111213141516171819$ python temp2.py&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;----------------title----------------The Dormouse&#x27;s story----------------head----------------&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;----------------[&#x27;title&#x27;]----------------&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;----------------[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]----------------&lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; temp3.py 1234567from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index2.html&quot;), &#x27;html.parser&#x27;)for link in soup.find_all(&#x27;a&#x27;): print(link.get(&#x27;href&#x27;))print(soup.get_text()) 1234567891011121314151617$ python temp3.pyhttp://example.com/elsiehttp://example.com/laciehttp://example.com/tillieThe Dormouse&#x27;s storyThe Dormouse&#x27;s story Once upon a time there were three little sisters; and their names were Elsie, Lacieand Tillie; and they lived at the bottom of a well....","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"crawling","slug":"python/crawling","permalink":"http://gonekng.github.io/categories/python/crawling/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"crawling","slug":"crawling","permalink":"http://gonekng.github.io/tags/crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"http://gonekng.github.io/tags/BeautifulSoup/"}],"author":"Jiwon Kang"},{"title":"Spark Installation in WSL2","slug":"Setting/Spark Installation in WSL2","date":"2022-04-20T07:39:12.000Z","updated":"2022-10-17T07:24:35.040Z","comments":true,"path":"2022/04/20/Setting/Spark Installation in WSL2/","link":"","permalink":"http://gonekng.github.io/2022/04/20/Setting/Spark%20Installation%20in%20WSL2/","excerpt":"","text":"Step 1. Install required files Install java and spark file. (Skip if already installed.) 123$ sudo apt-get install openjdk-8-jdk$ sudo wget https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz$ sudo tar -xvzf spark-3.2.0-bin-hadoop3.2.tgz Step 2. Set environment variables Open .bashrc file and add the code below. 12345export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export SPARK_HOME=/mnt/c/hadoop/spark-3.2.0-bin-hadoop3.2export PATH=$JAVA_HOME/bin:$PATHexport PATH=$SPARK_HOME/bin:$PATHexport PYSPARK_PYTHON=/usr/bin/python3 Update the code and make sure it’s actually reflected. 1234source ~/.bashrcecho SPARK_HOME/mnt/c/hadoop/spark-3.2.0-bin-hadoop3.2 Step 3. Run Pyspark Run pyspark in the path. Run the code below in the CMD and check the result printed. 1234&gt;&gt;&gt; rd = sc.textFile(&quot;README.md&quot;)&gt;&gt;&gt; rd.count()109 Step 4. Deploy in Web browser. Create a new directory temp, and virtual environment. 12$ mkdir temp &amp;&amp; cd temp$ virtualenv venv Connect to virtual environment and install pyspark. 12$ source venv/bin/activate$ pip install pyspark Create a new directory and README.md file. 12$ mkdir data &amp;&amp; cd data$ vi README.md *This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a text file. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is installed. As with the Scala and Java examples, we use a SparkSession to create Datasets. For applications that use custom classes or third-party libraries, we can also add code dependencies to spark-submit through its –py-files argument by packaging them into a .zip file (see spark-submit –help for details). SimpleApp is simple enough that we do not need to specify any code dependencies. We can run this application using the bin&#x2F;spark-submit script:* Back to temp and create SampleApp.py. 12$ cd ..$ vi SampleApp.py 123456789101112131415***# SampleApp.py***from pyspark.sql import SparkSessionlogFile = &quot;data/README.md&quot; # Should be some file on your systemspark = SparkSession.builder.appName(&quot;SimpleApp&quot;).getOrCreate()logData = spark.read.text(logFile).cache()numAs = logData.filter(logData.value.contains(&#x27;a&#x27;)).count()numBs = logData.filter(logData.value.contains(&#x27;b&#x27;)).count()print(&quot;Lines with a: %i, lines with b: %i&quot; % (numAs, numBs))input(&quot;Typing....&quot;)spark.stop() Run the SimpleApp.py 1$SPARK_HOME/bin/spark-submit --master local[4] SimpleApp.py Check the address below and copy it. Enter the corresponding address in the web browser and check the web UI.","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","permalink":"http://gonekng.github.io/tags/wsl2/"},{"name":"spark","slug":"spark","permalink":"http://gonekng.github.io/tags/spark/"}],"author":"Jiwon Kang"},{"title":"Spark Installation on Windows11","slug":"Setting/Spark Installation on Windows11","date":"2022-04-19T02:50:39.000Z","updated":"2022-10-17T07:24:41.519Z","comments":true,"path":"2022/04/19/Setting/Spark Installation on Windows11/","link":"","permalink":"http://gonekng.github.io/2022/04/19/Setting/Spark%20Installation%20on%20Windows11/","excerpt":"","text":"Step 1. Install Java DK Download Windows Installer. URL : https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html Run the download file as an administrator. Modify the path as shown in the picture below. (Be careful not to include spaces in the path name.) Step 2. Install Spark Download Spark .tgz file. (Click the link in the images below.) URL : https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz Download WinRAR to unzip the .tgz compressed file, and run as an administrator. URL : https://www.rarlab.com/download.htm Open the Spark file with WinRAR and extract to the folder. Rename the folder to spark, and copy and paste under the C drive. Open spark\\conf\\log4j.properties file with memo pad, and change the log4j.rootCategory value from INFO to ERROR. Step 3. Install Winutils Download winutils.exe. (Check the version of Spark.) URL : ‣ Create a foler winutils\\bin, and copy and paste winutils.exe. Run a CMD as an administrator, and write the code below. 1234&gt; cd c:\\winutils\\bin&gt; winutils.exe chmod 777 \\tmp\\hive****ChangeFileModeByMask error (2): ??? ??? ?? ? ????. If the above error occurs, create the tmp\\hive folder under the C drive and run it again. Step 4. Setting environment variables Create a new user variable SPARK_HOME, and set the value as the path of spark folder. Create a new user variable JAVA_HOME, and set the value as the path of jdk folder. Create a new user variable HADOOP_HOME, and set the value as the path of winutils folder. Edit the Path variable Insert %SPARK_HOME%\\bin and %JAVA_HOME%\\bin. Create a new user variable PYSPARK_PYTHON, and set the value as PYTHON. Run a CMD as an administrator, and run pyspark in the c:\\spark path. Run the code below in the CMD and check the result printed. 1234&gt; rd = sc.textFile(&quot;README.md&quot;)&gt; rd.count()109 Create new user variables and set the value. PYSPARK_DRIVER_PYTHON ; jupyter PYSPARK_DRIVER_PYTHON_OPTS ; notebook Reference https://dschloe.github.io/python/python_edu&#x2F;00_settings&#x2F;spark_installation_windows_10&#x2F;","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"spark","slug":"spark","permalink":"http://gonekng.github.io/tags/spark/"}],"author":"Jiwon Kang"},{"title":"ElasticSearch and Kibana Setting in WSL 2","slug":"Setting/ElasticSearch and Kibana Setting in WSL 2","date":"2022-04-15T08:43:10.000Z","updated":"2022-10-17T07:24:05.101Z","comments":true,"path":"2022/04/15/Setting/ElasticSearch and Kibana Setting in WSL 2/","link":"","permalink":"http://gonekng.github.io/2022/04/15/Setting/ElasticSearch%20and%20Kibana%20Setting%20in%20WSL%202/","excerpt":"","text":"Step 1. Install Package Update the system package and install a package related to HTTPS. 12$ sudo apt update$ sudo apt install apt-transport-https Install Java and check the version of Java. 12345$ sudo apt install openjdk-11-jdk$ java -versionopenjdk 11.0.14.1 2022-02-08OpenJDK Runtime Environment (build 11.0.14.1+1-Ubuntu-0ubuntu1.20.04)OpenJDK 64-Bit Server VM (build 11.0.14.1+1-Ubuntu-0ubuntu1.20.04, mixed mode, sharing) Open the vi editor to set the java environment variable. 1$ sudo vi /etc/environment Insert the following sentence in vi editor. JAVA_HOME=&quot;/usr/lib/jvm/java-11-openjdk-amd64&quot; Update the environment variables and check the contents. 1234$ source /etc/environment$ echo $JAVA_HOME/usr/lib/jvm/java-11-openjdk-amd64 Step 2. Install ElasticSearch Check the GPG keys. 123$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key addOK Add a library and install ElasticSearch. 1234$ sudo sh -c &#x27;echo &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; &gt; /etc/apt/sources.list.d/elastic-7.x.list&#x27;$ sudo apt-get update$ sudo apt-get install elasticsearch Step 3. Start ElasticSearch Start EleasticSearch 1234$ sudo systemctl start elasticsearchSystem has not been booted with systemd as init system (PID 1). Can&#x27;t operate.Failed to connect to bus: Host is down If the above error is printed, add the following command. 12$ sudo -b unshare --pid --fork --mount-proc /lib/systemd/systemd --system-unit=basic.target$ sudo -E nsenter --all -t $(pgrep -xo systemd) runuser -P -l $USER -c &quot;exec $SHELL&quot; Enable the ElasticSearch and start the service. 123456$ sudo systemctl enable elasticsearchSynchronizing state of elasticsearch.service with SysV service script with /lib/systemd/systemd-sysv-install.Executing: /lib/systemd/systemd-sysv-install enable elasticsearch$ sudo systemctl start elasticsearch Ensure that the service is actually operational. 12345678910111213141516171819$ curl -X GET &quot;localhost:9200/&quot;&#123; &quot;name&quot; : &quot;DESKTOP-JM1I3QF&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;ma7ulQQ_RL-Y3ZNsjz0ZVw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;7.17.2&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;deb&quot;, &quot;build_hash&quot; : &quot;de7261de50d90919ae53b0eff9413fd7e5307301&quot;, &quot;build_date&quot; : &quot;2022-03-28T15:12:21.446567561Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.11.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; Check whether it is printed well on the window screen. Step 4. Install and Start Kibana Install and enable Kibana service 12345$ sudo apt-get install kibana$ sudo systemctl enable kibanaSynchronizing state of kibana.service with SysV service script with /lib/systemd/systemd-sysv-install.Executing: /lib/systemd/systemd-sysv-install enable kibana Start Kibana service and check the status 1234567891011121314$ sudo systemctl start kibana$ sudo systemctl status kibana● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2022-04-14 11:53:07 KST; 21min ago Docs: https://www.elastic.co Main PID: 303 (node) Tasks: 11 (limit: 4646) Memory: 599.0M CGroup: /system.slice/kibana.service └─303 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli/dis&gt;Apr 14 11:53:07 DESKTOP-JM1I3QF systemd[1]: Started Kibana. Step 5. Check Kibana WebUI Make sure it connects to ElasticSearch well URL : http://localhost:5601/ Reference https://dschloe.github.io/settings/elasticsearch_kibana_wsl2&#x2F; https://www.how2shout.com/how-to/install-uninstall-elasticsearch-ubuntu-19-04-18-04-16-04.html","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","permalink":"http://gonekng.github.io/tags/wsl2/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://gonekng.github.io/tags/elasticsearch/"},{"name":"kibana","slug":"kibana","permalink":"http://gonekng.github.io/tags/kibana/"}],"author":"Jiwon Kang"},{"title":"Link VSCode with Remote WSL","slug":"Setting/Link VSCode with Remote WSL","date":"2022-04-15T08:33:16.000Z","updated":"2022-10-17T07:24:54.604Z","comments":true,"path":"2022/04/15/Setting/Link VSCode with Remote WSL/","link":"","permalink":"http://gonekng.github.io/2022/04/15/Setting/Link%20VSCode%20with%20Remote%20WSL/","excerpt":"","text":"Step 1. Install VSCode URL : https://code.visualstudio.com/download Download the System Installer for each OS. Check ‘Add to PATH’ and reboot after installation. Step 2. Link Remote WSL Install Remote WSL in Extension tab of VSCode. (File tab → Open Folder) Select the airflow-test folder that WSL installed. (Terminal → New Terminal) Open a new terminal and add a WSL terminal. Activate the virtual environment in WSL terminal. Run a python code and check if it is printed well. ex) main.py Reference https://dschloe.github.io/settings/vscode_wsl2&#x2F;","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","permalink":"http://gonekng.github.io/tags/wsl2/"},{"name":"vscode","slug":"vscode","permalink":"http://gonekng.github.io/tags/vscode/"}],"author":"Jiwon Kang"},{"title":"Establishing an Airflow Data Pipeline","slug":"Setting/Establishing an Airflow Data Pipeline","date":"2022-04-15T06:38:19.000Z","updated":"2022-10-17T07:24:14.706Z","comments":true,"path":"2022/04/15/Setting/Establishing an Airflow Data Pipeline/","link":"","permalink":"http://gonekng.github.io/2022/04/15/Setting/Establishing%20an%20Airflow%20Data%20Pipeline/","excerpt":"","text":"Step 01. Create a Virtual Data Create dags foler below (venv) airflow-test folder. 1234$ mkdir dags$ lsairflow-webserver.pid airflow.cfg airflow.db dags logs venv webserver_config.py Install the necessary libraries. 1$ pip3 install faker pandas Create data folder and write python file in the folder to create a virtual data. filename : step01_writecsv.py 123$ mkdir data$ cd data$ vi step01_writecsv.py 123456789101112131415161718192021***# step01_writecsv.py***from faker import Fakerimport csvoutput = open(&#x27;data.csv&#x27;,&#x27;w&#x27;)fake = Faker()header = [&#x27;name&#x27;,&#x27;age&#x27;,&#x27;street&#x27;,&#x27;city&#x27;,&#x27;state&#x27;,&#x27;zip&#x27;,&#x27;lng&#x27;,&#x27;lat&#x27;]mywriter = csv.writer(output)mywriter.writerow(header)for r in range(1000): mywriter.writerow([[fake.name](http://fake.name/)(), fake.random_int(min=18, max=80, step=1), fake.street_address(), fake.city(), fake.state(), fake.zipcode(), fake.longitude(), fake.latitude()])output.close() Run the file above and make sure that the data is well generated. 1234$ python3 step01_writecsv.py$ lsdata.csv step01_writecsv.py Step 2. Establish csv2join file Write code to build CSV and JSON transform files in dags folder. filename : csv2join.py 1$ vi csv2json.py 123456789101112131415161718192021222324252627282930313233343536***# csv2join.py***import datetime as dtfrom datetime import timedeltafrom airflow import DAGfrom airflow.operators.bash import BashOperatorfrom airflow.operators.python import PythonOperatorimport pandas as pddef csvToJson(): df=pd.read_csv(&#x27;data/data.csv&#x27;) for i,r in df.iterrows(): print(r[&#x27;name&#x27;]) df.to_json(&#x27;fromAirflow.json&#x27;,orient=&#x27;records&#x27;)default_args = &#123; &#x27;owner&#x27;: &#x27;evan&#x27;, &#x27;start_date&#x27;: dt.datetime(2020, 3, 18), &#x27;retries&#x27;: 1, &#x27;retry_delay&#x27;: dt.timedelta(minutes=5),&#125;with DAG(&#x27;MyCSVDAG&#x27;, default_args=default_args, schedule_interval=timedelta(minutes=5), # &#x27;0 * * * *&#x27;, ) as dag: print_starting = BashOperator(task_id=&#x27;starting&#x27;, bash_command=&#x27;echo &quot;I am reading the CSV now.....&quot;&#x27;) csvJson = PythonOperator(task_id=&#x27;convertCSVtoJson&#x27;, python_callable=csvToJson)print_starting &gt;&gt; csvJson Run the csv2json.py above. 1$ python3 csv2json.py Step 04. Run Webserver and Scheduler Simultaneously Open a separate terminal and run the webserver and scheduler. 12$ airflow webserver -p 8080$ airflow scheduler Check if it works normally in the Web UI. Reference https://dschloe.github.io/python/data_engineering&#x2F;ch03_reading_writing_file&#x2F;airflow_csv2json_sample&#x2F;","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","permalink":"http://gonekng.github.io/tags/wsl2/"},{"name":"apache","slug":"apache","permalink":"http://gonekng.github.io/tags/apache/"},{"name":"airflow","slug":"airflow","permalink":"http://gonekng.github.io/tags/airflow/"}],"author":"Jiwon Kang"},{"title":"Apache-Airflow Setting in Windows11 (WSL 2)","slug":"Setting/Apache-Airflow Setting in Windows11 (WSL 2)","date":"2022-04-15T03:10:08.000Z","updated":"2022-10-17T07:23:56.143Z","comments":true,"path":"2022/04/15/Setting/Apache-Airflow Setting in Windows11 (WSL 2)/","link":"","permalink":"http://gonekng.github.io/2022/04/15/Setting/Apache-Airflow%20Setting%20in%20Windows11%20(WSL%202)/","excerpt":"","text":"Step 1. Create a virtual environment Install pip and virtualenv package 12$ sudo apt install python3-pip$ sudo pip3 install virtualenv Create a virtual environment in c:\\airflow-test folder 1234567$ virtualenv venvcreated virtual environment CPython3.8.10.final.0-64 in 29086mscreator CPython3Posix(dest=/mnt/c/airflow-test/venv, clear=False, no_vcs_ignore=False, global=False)seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/donumm/.local/share/virtualenv)added seed packages: pip==22.0.4, setuptools==62.1.0, wheel==0.37.1activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator Open .bashrc file and add the following code. 123$ vi ~/.bashrc*export AIRFLOW_HOME=/mnt/c/airflow-test* Update the code and make sure it’s actually reflected. 1234$ source ~/.bashrc$ echo $AIRFLOW_HOME/mnt/c/airflow-test Connect to virtual environment. 1$ source venv/bin/activate ※ Airflow must be installed in the virtual environment and executed in the virtual environment. Step 4. Install Apache Airflow Install PostgreSQL, Slack, and Celery packages 1pip3 install &#x27;apache-airflow[postgres, slack, celery]&#x27; Initialize the DB to run the airflow. 1$ airflow db init Register an username and password of the airflow 12***# Create a new user***$ airflow users create --username airflow --password airflow --firstname Jiwon --lastname Kang --role Admin --email donumm64@gmail.co 123456***# Check the user list***$ airflow users listid | username | email | first_name | last_name | roles===+==========+====================+============+===========+======1 | donumm | donumm64@gmail.com | Jiwon | Kang | Admin Open airflow.cfg file, and change the value of load_examples from True to False. Reset the db in terminal. 123$ airflow db reset...Proceed? (y/n) Y Run the airflow webserver and scheduler. 12$ airflow webserver -p 8080$ airflow scheduler Connect the airflow webserver. URL : http://localhost:8080/login/ Reference https://dschloe.github.io/settings/apache_airflow_using_wsl2&#x2F;","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","permalink":"http://gonekng.github.io/tags/wsl2/"},{"name":"apache","slug":"apache","permalink":"http://gonekng.github.io/tags/apache/"},{"name":"airflow","slug":"airflow","permalink":"http://gonekng.github.io/tags/airflow/"}],"author":"Jiwon Kang"},{"title":"WSL 2 Installation in Windows11","slug":"Setting/WSL 2 Installation in Windows11","date":"2022-04-15T02:43:19.000Z","updated":"2022-10-17T07:24:51.898Z","comments":true,"path":"2022/04/15/Setting/WSL 2 Installation in Windows11/","link":"","permalink":"http://gonekng.github.io/2022/04/15/Setting/WSL%202%20Installation%20in%20Windows11/","excerpt":"","text":"Step 1. Enable WSL-related features by DISM Run Windows Terminal as administrator Enable Microsoft-Windows-Subsystem-Linux Features 1$ dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart Enable the VirtualMachinePlatform feature Reboot if the operation is completed successfully. 1$ dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Step 2. WSL2 Kernel Update Install the update file from the link below. URL : https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi Open Windows terminal and change WSL version to 2. 1$ wsl --set-default-version 2 Install Ubuntu, the most popular Linux distribution, and run as administrator. ![](&#x2F;images&#x2F;Setting&#x2F;wsl2&#x2F;Untitled 1.png) In Ubuntu, Set the username and password. Check the currently installed version with wsl -l -v 123$ wsl -l -v NAME STATE VERSION* Ubuntu Running 2 If it says version 1, execute the following command. 12345$ wsl --set-version Ubuntu 2변환이 진행 중입니다. 몇 분 정도 걸릴 수 있습니다...WSL 2와의 주요 차이점에 대한 자세한 내용은 [https://aka.ms/wsl2를](https://aka.ms/wsl2%EB%A5%BC) 참조하세요변환이 완료되었습니다. Make sure that it says version 2. 123$ wsl -l -v NAME STATE VERSION* Ubuntu Running 2 Reference https://www.lainyzine.com/ko/article/how-to-install-wsl2-and-use-linux-on-windows-10/#google_vignette","categories":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"}],"tags":[{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","permalink":"http://gonekng.github.io/tags/wsl2/"}],"author":"Jiwon Kang"},{"title":"ML Practice 9_3","slug":"Python/ML/ML_ch_9_3","date":"2022-04-08T03:27:01.000Z","updated":"2022-10-05T05:39:54.030Z","comments":true,"path":"2022/04/08/Python/ML/ML_ch_9_3/","link":"","permalink":"http://gonekng.github.io/2022/04/08/Python/ML/ML_ch_9_3/","excerpt":"","text":"LSTM(Long Short-Term Memory) When the sentence is long, the learning ability of RNN is poor. LSTM is designed to keep short-term memory long. 1234567891011from tensorflow.keras.datasets import imdbfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = imdb.load_data( num_words=500)train_input, val_input, train_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42)train_input.shape, val_input.shape, train_target.shape, val_target.shape Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz 17465344/17464789 [==============================] - 0s 0us/step 17473536/17464789 [==============================] - 0s 0us/step ((20000,), (5000,), (20000,), (5000,)) 12345from tensorflow.keras.preprocessing.sequence import pad_sequencestrain_seq = pad_sequences(train_input, maxlen=100)val_seq = pad_sequences(val_input, maxlen=100)train_seq.shape, val_seq.shape ((20000, 100), (5000, 100)) 123456from tensorflow import kerasmodel = keras.Sequential()model.add(keras.layers.Embedding(500, 16, input_length=100))model.add(keras.layers.LSTM(8))model.add(keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;))model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 16) 8000 lstm (LSTM) (None, 8) 800 dense (Dense) (None, 1) 9 ================================================================= Total params: 8,809 Trainable params: 8,809 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model.compile(optimizer=rmsprop, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-lstm-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 16s 42ms/step - loss: 0.6925 - accuracy: 0.5433 - val_loss: 0.6916 - val_accuracy: 0.5986 Epoch 2/100 313/313 [==============================] - 20s 65ms/step - loss: 0.6902 - accuracy: 0.6076 - val_loss: 0.6883 - val_accuracy: 0.6470 Epoch 3/100 313/313 [==============================] - 16s 50ms/step - loss: 0.6842 - accuracy: 0.6512 - val_loss: 0.6783 - val_accuracy: 0.6680 Epoch 4/100 313/313 [==============================] - 14s 46ms/step - loss: 0.6591 - accuracy: 0.6861 - val_loss: 0.6237 - val_accuracy: 0.7128 Epoch 5/100 313/313 [==============================] - 14s 45ms/step - loss: 0.5975 - accuracy: 0.7211 - val_loss: 0.5842 - val_accuracy: 0.7220 Epoch 6/100 313/313 [==============================] - 16s 51ms/step - loss: 0.5716 - accuracy: 0.7355 - val_loss: 0.5643 - val_accuracy: 0.7408 Epoch 7/100 313/313 [==============================] - 14s 45ms/step - loss: 0.5510 - accuracy: 0.7534 - val_loss: 0.5470 - val_accuracy: 0.7458 Epoch 8/100 313/313 [==============================] - 17s 54ms/step - loss: 0.5328 - accuracy: 0.7619 - val_loss: 0.5319 - val_accuracy: 0.7564 Epoch 9/100 313/313 [==============================] - 14s 44ms/step - loss: 0.5147 - accuracy: 0.7739 - val_loss: 0.5130 - val_accuracy: 0.7724 Epoch 10/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4982 - accuracy: 0.7824 - val_loss: 0.5000 - val_accuracy: 0.7746 Epoch 11/100 313/313 [==============================] - 17s 53ms/step - loss: 0.4837 - accuracy: 0.7909 - val_loss: 0.4874 - val_accuracy: 0.7794 Epoch 12/100 313/313 [==============================] - 15s 47ms/step - loss: 0.4717 - accuracy: 0.7957 - val_loss: 0.4767 - val_accuracy: 0.7868 Epoch 13/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4620 - accuracy: 0.7990 - val_loss: 0.4696 - val_accuracy: 0.7892 Epoch 14/100 313/313 [==============================] - 15s 48ms/step - loss: 0.4534 - accuracy: 0.8033 - val_loss: 0.4662 - val_accuracy: 0.7908 Epoch 15/100 313/313 [==============================] - 15s 48ms/step - loss: 0.4470 - accuracy: 0.8067 - val_loss: 0.4606 - val_accuracy: 0.7946 Epoch 16/100 313/313 [==============================] - 15s 47ms/step - loss: 0.4414 - accuracy: 0.8067 - val_loss: 0.4558 - val_accuracy: 0.7924 Epoch 17/100 313/313 [==============================] - 14s 45ms/step - loss: 0.4366 - accuracy: 0.8087 - val_loss: 0.4516 - val_accuracy: 0.7972 Epoch 18/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4329 - accuracy: 0.8098 - val_loss: 0.4485 - val_accuracy: 0.7968 Epoch 19/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4301 - accuracy: 0.8088 - val_loss: 0.4461 - val_accuracy: 0.7962 Epoch 20/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4274 - accuracy: 0.8093 - val_loss: 0.4456 - val_accuracy: 0.7988 Epoch 21/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4248 - accuracy: 0.8102 - val_loss: 0.4429 - val_accuracy: 0.7994 Epoch 22/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4225 - accuracy: 0.8106 - val_loss: 0.4481 - val_accuracy: 0.7960 Epoch 23/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4211 - accuracy: 0.8117 - val_loss: 0.4417 - val_accuracy: 0.7974 Epoch 24/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4198 - accuracy: 0.8116 - val_loss: 0.4393 - val_accuracy: 0.8010 Epoch 25/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4180 - accuracy: 0.8127 - val_loss: 0.4464 - val_accuracy: 0.7890 Epoch 26/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4173 - accuracy: 0.8130 - val_loss: 0.4400 - val_accuracy: 0.8002 Epoch 27/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4161 - accuracy: 0.8123 - val_loss: 0.4370 - val_accuracy: 0.8018 Epoch 28/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4154 - accuracy: 0.8131 - val_loss: 0.4375 - val_accuracy: 0.8010 Epoch 29/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4143 - accuracy: 0.8127 - val_loss: 0.4361 - val_accuracy: 0.8020 Epoch 30/100 313/313 [==============================] - 14s 45ms/step - loss: 0.4131 - accuracy: 0.8136 - val_loss: 0.4370 - val_accuracy: 0.8022 Epoch 31/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4129 - accuracy: 0.8134 - val_loss: 0.4360 - val_accuracy: 0.8022 Epoch 32/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4126 - accuracy: 0.8141 - val_loss: 0.4355 - val_accuracy: 0.8014 Epoch 33/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4117 - accuracy: 0.8134 - val_loss: 0.4358 - val_accuracy: 0.8040 Epoch 34/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4112 - accuracy: 0.8134 - val_loss: 0.4346 - val_accuracy: 0.7986 Epoch 35/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4106 - accuracy: 0.8134 - val_loss: 0.4353 - val_accuracy: 0.7978 Epoch 36/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4100 - accuracy: 0.8144 - val_loss: 0.4346 - val_accuracy: 0.7976 Epoch 37/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4100 - accuracy: 0.8128 - val_loss: 0.4342 - val_accuracy: 0.8052 Epoch 38/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4092 - accuracy: 0.8141 - val_loss: 0.4350 - val_accuracy: 0.8052 Epoch 39/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4090 - accuracy: 0.8138 - val_loss: 0.4334 - val_accuracy: 0.8028 Epoch 40/100 313/313 [==============================] - 13s 40ms/step - loss: 0.4083 - accuracy: 0.8152 - val_loss: 0.4325 - val_accuracy: 0.7992 Epoch 41/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4080 - accuracy: 0.8156 - val_loss: 0.4358 - val_accuracy: 0.7960 Epoch 42/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4075 - accuracy: 0.8149 - val_loss: 0.4322 - val_accuracy: 0.8016 Epoch 43/100 313/313 [==============================] - 12s 39ms/step - loss: 0.4069 - accuracy: 0.8134 - val_loss: 0.4327 - val_accuracy: 0.8014 Epoch 44/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4067 - accuracy: 0.8155 - val_loss: 0.4333 - val_accuracy: 0.8042 Epoch 45/100 313/313 [==============================] - 12s 40ms/step - loss: 0.4063 - accuracy: 0.8140 - val_loss: 0.4331 - val_accuracy: 0.8022 12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(history.history[&#x27;loss&#x27;])ax.plot(history.history[&#x27;val_loss&#x27;])ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;loss&#x27;)ax.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() dropout &#x3D; 0.3 12345model2 = keras.Sequential()model2.add(keras.layers.Embedding(500, 16, input_length=100))model2.add(keras.layers.LSTM(8, dropout=0.3))model2.add(keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;))model2.summary() Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 100, 16) 8000 lstm_1 (LSTM) (None, 8) 800 dense_1 (Dense) (None, 1) 9 ================================================================= Total params: 8,809 Trainable params: 8,809 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model2.compile(optimizer=rmsprop, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-dropout-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 18s 48ms/step - loss: 0.6925 - accuracy: 0.5239 - val_loss: 0.6913 - val_accuracy: 0.5654 Epoch 2/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6895 - accuracy: 0.5954 - val_loss: 0.6872 - val_accuracy: 0.6314 Epoch 3/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6816 - accuracy: 0.6561 - val_loss: 0.6736 - val_accuracy: 0.6828 Epoch 4/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6444 - accuracy: 0.7018 - val_loss: 0.6027 - val_accuracy: 0.7116 Epoch 5/100 313/313 [==============================] - 13s 43ms/step - loss: 0.5789 - accuracy: 0.7199 - val_loss: 0.5620 - val_accuracy: 0.7352 Epoch 6/100 313/313 [==============================] - 14s 44ms/step - loss: 0.5517 - accuracy: 0.7405 - val_loss: 0.5403 - val_accuracy: 0.7526 Epoch 7/100 313/313 [==============================] - 14s 44ms/step - loss: 0.5317 - accuracy: 0.7545 - val_loss: 0.5219 - val_accuracy: 0.7598 Epoch 8/100 313/313 [==============================] - 13s 43ms/step - loss: 0.5117 - accuracy: 0.7681 - val_loss: 0.5049 - val_accuracy: 0.7702 Epoch 9/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4968 - accuracy: 0.7747 - val_loss: 0.4917 - val_accuracy: 0.7760 Epoch 10/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4844 - accuracy: 0.7829 - val_loss: 0.4814 - val_accuracy: 0.7864 Epoch 11/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4741 - accuracy: 0.7871 - val_loss: 0.4728 - val_accuracy: 0.7892 Epoch 12/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4667 - accuracy: 0.7912 - val_loss: 0.4679 - val_accuracy: 0.7870 Epoch 13/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4584 - accuracy: 0.7942 - val_loss: 0.4614 - val_accuracy: 0.7932 Epoch 14/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4530 - accuracy: 0.7968 - val_loss: 0.4590 - val_accuracy: 0.7942 Epoch 15/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4499 - accuracy: 0.7979 - val_loss: 0.4546 - val_accuracy: 0.7956 Epoch 16/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4458 - accuracy: 0.7995 - val_loss: 0.4517 - val_accuracy: 0.7988 Epoch 17/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4415 - accuracy: 0.7990 - val_loss: 0.4481 - val_accuracy: 0.7984 Epoch 18/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4374 - accuracy: 0.8018 - val_loss: 0.4468 - val_accuracy: 0.7994 Epoch 19/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4342 - accuracy: 0.8030 - val_loss: 0.4516 - val_accuracy: 0.7964 Epoch 20/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4325 - accuracy: 0.8054 - val_loss: 0.4431 - val_accuracy: 0.8024 Epoch 21/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4305 - accuracy: 0.8057 - val_loss: 0.4400 - val_accuracy: 0.7996 Epoch 22/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4279 - accuracy: 0.8037 - val_loss: 0.4388 - val_accuracy: 0.7964 Epoch 23/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4250 - accuracy: 0.8075 - val_loss: 0.4392 - val_accuracy: 0.8014 Epoch 24/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4253 - accuracy: 0.8062 - val_loss: 0.4361 - val_accuracy: 0.7966 Epoch 25/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4241 - accuracy: 0.8077 - val_loss: 0.4357 - val_accuracy: 0.8008 Epoch 26/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4219 - accuracy: 0.8077 - val_loss: 0.4342 - val_accuracy: 0.8008 Epoch 27/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4205 - accuracy: 0.8097 - val_loss: 0.4331 - val_accuracy: 0.8002 Epoch 28/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4195 - accuracy: 0.8098 - val_loss: 0.4327 - val_accuracy: 0.7988 Epoch 29/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4197 - accuracy: 0.8058 - val_loss: 0.4326 - val_accuracy: 0.8006 Epoch 30/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4172 - accuracy: 0.8076 - val_loss: 0.4335 - val_accuracy: 0.7954 Epoch 31/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4160 - accuracy: 0.8116 - val_loss: 0.4308 - val_accuracy: 0.8012 Epoch 32/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4161 - accuracy: 0.8108 - val_loss: 0.4319 - val_accuracy: 0.7986 Epoch 33/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4140 - accuracy: 0.8119 - val_loss: 0.4304 - val_accuracy: 0.8006 Epoch 34/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4146 - accuracy: 0.8110 - val_loss: 0.4299 - val_accuracy: 0.7984 Epoch 35/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4140 - accuracy: 0.8101 - val_loss: 0.4293 - val_accuracy: 0.8008 Epoch 36/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4132 - accuracy: 0.8122 - val_loss: 0.4293 - val_accuracy: 0.7994 Epoch 37/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4127 - accuracy: 0.8110 - val_loss: 0.4307 - val_accuracy: 0.7980 Epoch 38/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4137 - accuracy: 0.8112 - val_loss: 0.4286 - val_accuracy: 0.8046 Epoch 39/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4127 - accuracy: 0.8109 - val_loss: 0.4277 - val_accuracy: 0.8040 Epoch 40/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4103 - accuracy: 0.8106 - val_loss: 0.4284 - val_accuracy: 0.8052 Epoch 41/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4105 - accuracy: 0.8106 - val_loss: 0.4281 - val_accuracy: 0.8002 Epoch 42/100 313/313 [==============================] - 13s 41ms/step - loss: 0.4111 - accuracy: 0.8137 - val_loss: 0.4284 - val_accuracy: 0.7982 1234567fig, ax = plt.subplots()ax.plot(history.history[&#x27;loss&#x27;])ax.plot(history.history[&#x27;val_loss&#x27;])ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;loss&#x27;)ax.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() return_squences &#x3D; True 123456model3 = keras.Sequential()model3.add(keras.layers.Embedding(500, 16, input_length=100))model3.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True))model3.add(keras.layers.LSTM(8, dropout=0.3))model3.add(keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;))model3.summary() Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_2 (Embedding) (None, 100, 16) 8000 lstm_2 (LSTM) (None, 100, 8) 800 lstm_3 (LSTM) (None, 8) 544 dense_2 (Dense) (None, 1) 9 ================================================================= Total params: 9,353 Trainable params: 9,353 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model3.compile(optimizer=rmsprop, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-2rnn-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model3.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 29s 81ms/step - loss: 0.6908 - accuracy: 0.5653 - val_loss: 0.6870 - val_accuracy: 0.6378 Epoch 2/100 313/313 [==============================] - 25s 79ms/step - loss: 0.6632 - accuracy: 0.6561 - val_loss: 0.6162 - val_accuracy: 0.6984 Epoch 3/100 313/313 [==============================] - 24s 78ms/step - loss: 0.5830 - accuracy: 0.7079 - val_loss: 0.5565 - val_accuracy: 0.7352 Epoch 4/100 313/313 [==============================] - 24s 78ms/step - loss: 0.5463 - accuracy: 0.7387 - val_loss: 0.5279 - val_accuracy: 0.7536 Epoch 5/100 313/313 [==============================] - 25s 79ms/step - loss: 0.5207 - accuracy: 0.7556 - val_loss: 0.5072 - val_accuracy: 0.7636 Epoch 6/100 313/313 [==============================] - 25s 79ms/step - loss: 0.5026 - accuracy: 0.7671 - val_loss: 0.4941 - val_accuracy: 0.7730 Epoch 7/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4910 - accuracy: 0.7728 - val_loss: 0.4812 - val_accuracy: 0.7818 Epoch 8/100 313/313 [==============================] - 31s 100ms/step - loss: 0.4813 - accuracy: 0.7782 - val_loss: 0.4747 - val_accuracy: 0.7810 Epoch 9/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4730 - accuracy: 0.7832 - val_loss: 0.4661 - val_accuracy: 0.7878 Epoch 10/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4657 - accuracy: 0.7869 - val_loss: 0.4612 - val_accuracy: 0.7896 Epoch 11/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4614 - accuracy: 0.7872 - val_loss: 0.4589 - val_accuracy: 0.7898 Epoch 12/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4568 - accuracy: 0.7886 - val_loss: 0.4547 - val_accuracy: 0.7934 Epoch 13/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4519 - accuracy: 0.7911 - val_loss: 0.4577 - val_accuracy: 0.7874 Epoch 14/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4484 - accuracy: 0.7945 - val_loss: 0.4498 - val_accuracy: 0.7930 Epoch 15/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4472 - accuracy: 0.7955 - val_loss: 0.4498 - val_accuracy: 0.7914 Epoch 16/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4447 - accuracy: 0.7983 - val_loss: 0.4468 - val_accuracy: 0.7968 Epoch 17/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4441 - accuracy: 0.7944 - val_loss: 0.4444 - val_accuracy: 0.7968 Epoch 18/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4418 - accuracy: 0.7990 - val_loss: 0.4442 - val_accuracy: 0.7950 Epoch 19/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4375 - accuracy: 0.8007 - val_loss: 0.4477 - val_accuracy: 0.7936 Epoch 20/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4365 - accuracy: 0.8027 - val_loss: 0.4416 - val_accuracy: 0.7964 Epoch 21/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4354 - accuracy: 0.8012 - val_loss: 0.4415 - val_accuracy: 0.7944 Epoch 22/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4341 - accuracy: 0.8014 - val_loss: 0.4408 - val_accuracy: 0.7972 Epoch 23/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4345 - accuracy: 0.8008 - val_loss: 0.4390 - val_accuracy: 0.7960 Epoch 24/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4315 - accuracy: 0.8037 - val_loss: 0.4412 - val_accuracy: 0.7912 Epoch 25/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4305 - accuracy: 0.8049 - val_loss: 0.4397 - val_accuracy: 0.7928 Epoch 26/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4294 - accuracy: 0.8051 - val_loss: 0.4382 - val_accuracy: 0.7990 Epoch 27/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4281 - accuracy: 0.8025 - val_loss: 0.4378 - val_accuracy: 0.7954 Epoch 28/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4277 - accuracy: 0.8035 - val_loss: 0.4375 - val_accuracy: 0.7952 Epoch 29/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4255 - accuracy: 0.8067 - val_loss: 0.4365 - val_accuracy: 0.7984 Epoch 30/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4254 - accuracy: 0.8055 - val_loss: 0.4360 - val_accuracy: 0.7990 Epoch 31/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4248 - accuracy: 0.8065 - val_loss: 0.4350 - val_accuracy: 0.8000 Epoch 32/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4247 - accuracy: 0.8051 - val_loss: 0.4352 - val_accuracy: 0.8006 Epoch 33/100 313/313 [==============================] - 25s 80ms/step - loss: 0.4240 - accuracy: 0.8061 - val_loss: 0.4358 - val_accuracy: 0.7958 Epoch 34/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4239 - accuracy: 0.8051 - val_loss: 0.4348 - val_accuracy: 0.8020 Epoch 35/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4240 - accuracy: 0.8066 - val_loss: 0.4340 - val_accuracy: 0.7998 Epoch 36/100 313/313 [==============================] - 24s 78ms/step - loss: 0.4230 - accuracy: 0.8044 - val_loss: 0.4358 - val_accuracy: 0.8024 Epoch 37/100 313/313 [==============================] - 25s 78ms/step - loss: 0.4227 - accuracy: 0.8061 - val_loss: 0.4334 - val_accuracy: 0.7994 Epoch 38/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4207 - accuracy: 0.8063 - val_loss: 0.4337 - val_accuracy: 0.8000 Epoch 39/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4194 - accuracy: 0.8075 - val_loss: 0.4349 - val_accuracy: 0.8012 Epoch 40/100 313/313 [==============================] - 25s 80ms/step - loss: 0.4203 - accuracy: 0.8073 - val_loss: 0.4326 - val_accuracy: 0.8018 Epoch 41/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4208 - accuracy: 0.8064 - val_loss: 0.4347 - val_accuracy: 0.8036 Epoch 42/100 313/313 [==============================] - 25s 80ms/step - loss: 0.4196 - accuracy: 0.8066 - val_loss: 0.4371 - val_accuracy: 0.8006 Epoch 43/100 313/313 [==============================] - 25s 79ms/step - loss: 0.4196 - accuracy: 0.8066 - val_loss: 0.4358 - val_accuracy: 0.8018 1234567fig, ax = plt.subplots()ax.plot(history.history[&#x27;loss&#x27;])ax.plot(history.history[&#x27;val_loss&#x27;])ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;loss&#x27;)ax.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() GRU(Gated Recurrent Unit) 12345model4 = keras.Sequential()model4.add(keras.layers.Embedding(500, 16, input_length=100))model4.add(keras.layers.GRU(8))model4.add(keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;))model4.summary() Model: &quot;sequential_3&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_3 (Embedding) (None, 100, 16) 8000 gru (GRU) (None, 8) 624 dense_3 (Dense) (None, 1) 9 ================================================================= Total params: 8,633 Trainable params: 8,633 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model4.compile(optimizer=rmsprop, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-gru-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model4.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 16s 45ms/step - loss: 0.6925 - accuracy: 0.5303 - val_loss: 0.6916 - val_accuracy: 0.5616 Epoch 2/100 313/313 [==============================] - 13s 43ms/step - loss: 0.6905 - accuracy: 0.5642 - val_loss: 0.6895 - val_accuracy: 0.5804 Epoch 3/100 313/313 [==============================] - 13s 43ms/step - loss: 0.6875 - accuracy: 0.5913 - val_loss: 0.6861 - val_accuracy: 0.5956 Epoch 4/100 313/313 [==============================] - 14s 43ms/step - loss: 0.6826 - accuracy: 0.6100 - val_loss: 0.6803 - val_accuracy: 0.6116 Epoch 5/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6747 - accuracy: 0.6278 - val_loss: 0.6712 - val_accuracy: 0.6314 Epoch 6/100 313/313 [==============================] - 13s 43ms/step - loss: 0.6611 - accuracy: 0.6477 - val_loss: 0.6548 - val_accuracy: 0.6538 Epoch 7/100 313/313 [==============================] - 13s 42ms/step - loss: 0.6369 - accuracy: 0.6689 - val_loss: 0.6239 - val_accuracy: 0.6790 Epoch 8/100 313/313 [==============================] - 13s 42ms/step - loss: 0.5868 - accuracy: 0.7036 - val_loss: 0.5557 - val_accuracy: 0.7230 Epoch 9/100 313/313 [==============================] - 13s 42ms/step - loss: 0.5165 - accuracy: 0.7487 - val_loss: 0.5120 - val_accuracy: 0.7520 Epoch 10/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4907 - accuracy: 0.7675 - val_loss: 0.4945 - val_accuracy: 0.7624 Epoch 11/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4746 - accuracy: 0.7786 - val_loss: 0.4819 - val_accuracy: 0.7712 Epoch 12/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4622 - accuracy: 0.7870 - val_loss: 0.4729 - val_accuracy: 0.7780 Epoch 13/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4530 - accuracy: 0.7927 - val_loss: 0.4647 - val_accuracy: 0.7824 Epoch 14/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4451 - accuracy: 0.7976 - val_loss: 0.4613 - val_accuracy: 0.7862 Epoch 15/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4388 - accuracy: 0.8009 - val_loss: 0.4538 - val_accuracy: 0.7864 Epoch 16/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4342 - accuracy: 0.8044 - val_loss: 0.4503 - val_accuracy: 0.7912 Epoch 17/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4302 - accuracy: 0.8057 - val_loss: 0.4499 - val_accuracy: 0.7906 Epoch 18/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4265 - accuracy: 0.8094 - val_loss: 0.4460 - val_accuracy: 0.7964 Epoch 19/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4239 - accuracy: 0.8113 - val_loss: 0.4442 - val_accuracy: 0.7972 Epoch 20/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4214 - accuracy: 0.8130 - val_loss: 0.4434 - val_accuracy: 0.7978 Epoch 21/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4195 - accuracy: 0.8127 - val_loss: 0.4436 - val_accuracy: 0.7994 Epoch 22/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4180 - accuracy: 0.8145 - val_loss: 0.4415 - val_accuracy: 0.7968 Epoch 23/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4164 - accuracy: 0.8141 - val_loss: 0.4401 - val_accuracy: 0.8006 Epoch 24/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4151 - accuracy: 0.8149 - val_loss: 0.4401 - val_accuracy: 0.7978 Epoch 25/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4136 - accuracy: 0.8159 - val_loss: 0.4398 - val_accuracy: 0.7988 Epoch 26/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4128 - accuracy: 0.8171 - val_loss: 0.4394 - val_accuracy: 0.7962 Epoch 27/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4116 - accuracy: 0.8181 - val_loss: 0.4384 - val_accuracy: 0.7980 Epoch 28/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4111 - accuracy: 0.8163 - val_loss: 0.4378 - val_accuracy: 0.8012 Epoch 29/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4102 - accuracy: 0.8189 - val_loss: 0.4378 - val_accuracy: 0.7974 Epoch 30/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4096 - accuracy: 0.8183 - val_loss: 0.4380 - val_accuracy: 0.7978 Epoch 31/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4087 - accuracy: 0.8187 - val_loss: 0.4376 - val_accuracy: 0.7978 Epoch 32/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4080 - accuracy: 0.8183 - val_loss: 0.4387 - val_accuracy: 0.7976 Epoch 33/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4076 - accuracy: 0.8184 - val_loss: 0.4352 - val_accuracy: 0.8018 Epoch 34/100 313/313 [==============================] - 13s 42ms/step - loss: 0.4065 - accuracy: 0.8183 - val_loss: 0.4359 - val_accuracy: 0.7948 Epoch 35/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4060 - accuracy: 0.8185 - val_loss: 0.4350 - val_accuracy: 0.7974 Epoch 36/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4059 - accuracy: 0.8194 - val_loss: 0.4426 - val_accuracy: 0.7946 Epoch 37/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4048 - accuracy: 0.8208 - val_loss: 0.4329 - val_accuracy: 0.8018 Epoch 38/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4044 - accuracy: 0.8197 - val_loss: 0.4335 - val_accuracy: 0.7998 Epoch 39/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4040 - accuracy: 0.8202 - val_loss: 0.4327 - val_accuracy: 0.8010 Epoch 40/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4039 - accuracy: 0.8199 - val_loss: 0.4325 - val_accuracy: 0.8042 Epoch 41/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4031 - accuracy: 0.8197 - val_loss: 0.4316 - val_accuracy: 0.8024 Epoch 42/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4025 - accuracy: 0.8213 - val_loss: 0.4316 - val_accuracy: 0.8004 Epoch 43/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4020 - accuracy: 0.8198 - val_loss: 0.4320 - val_accuracy: 0.7986 Epoch 44/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4016 - accuracy: 0.8214 - val_loss: 0.4311 - val_accuracy: 0.8020 Epoch 45/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4011 - accuracy: 0.8214 - val_loss: 0.4301 - val_accuracy: 0.8040 Epoch 46/100 313/313 [==============================] - 13s 43ms/step - loss: 0.4001 - accuracy: 0.8210 - val_loss: 0.4306 - val_accuracy: 0.8008 Epoch 47/100 313/313 [==============================] - 14s 43ms/step - loss: 0.4001 - accuracy: 0.8224 - val_loss: 0.4375 - val_accuracy: 0.7950 Epoch 48/100 313/313 [==============================] - 14s 43ms/step - loss: 0.3998 - accuracy: 0.8222 - val_loss: 0.4306 - val_accuracy: 0.8018 1234567fig, ax = plt.subplots()ax.plot(history.history[&#x27;loss&#x27;])ax.plot(history.history[&#x27;val_loss&#x27;])ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;loss&#x27;)ax.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 9_2","slug":"Python/ML/ML_ch_9_2","date":"2022-04-08T02:57:20.000Z","updated":"2022-10-05T05:39:53.919Z","comments":true,"path":"2022/04/08/Python/ML/ML_ch_9_2/","link":"","permalink":"http://gonekng.github.io/2022/04/08/Python/ML/ML_ch_9_2/","excerpt":"","text":"Text Normalization: Pre-processing text for use as input data Cleansing 텍스트 분석에 방해되는 불필요한 문자 및 기호를 사전에 제거 ex) HTML, XML 태그 제거 Tokenization Sentence Tokenization- 문장, 마침표, 개행문자 등 문장 마지막을 뜻하는 기호를 따라 분리- 문장이 가지는 의미가 시맨틱적으로 중요한 요소일 때 사용 Word Tokenization- 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리 Stop word elimination 필수 문법 요소이나 문맥적으로 큰 의미 없는 단어(ex. is, the, a, will)가 텍스트에 빈번하게 나타나면 중요한 단어로 인지될 수 있어서 사전 제거가 필요함 Stemming Lemmatization IMDB Review Classification with RNN a dataset that categorizes IMDB reviews as positive and negative based on comments 123from tensorflow.keras.datasets import imdb(train_input, train_target), (test_input, test_target) = imdb.load_data( num_words=500) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz 17465344/17464789 [==============================] - 0s 0us/step 17473536/17464789 [==============================] - 0s 0us/step Datasets are made of a one-dimensional array, because the length of the text is different 1print(train_input.shape, test_input.shape) (25000,) (25000,) 123print(len(train_input[0]))print(len(train_input[1]))print(len(train_input[2])) 218 189 141 1print(train_input[0]) [1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32] Target 0: negative review Target 1: positive review 1print(train_target[:20]) [1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1] Split data12345from sklearn.model_selection import train_test_splittrain_input, val_input, train_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42)train_input.shape, val_input.shape, train_target.shape, val_target.shape ((20000,), (5000,), (20000,), (5000,)) Visualize data mean and median of the number of words in each review 123import numpy as nplengths = np.array([len(x) for x in train_input])print(np.mean(lengths), np.median(lengths)) 239.00925 178.0 123456import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.hist(lengths)ax.set_xlabel(&quot;length&quot;)ax.set_ylabel(&quot;frequency&quot;)plt.show() Use only 100 words that are much shorter than the median Use padding to match the length of each review to 100 1234from tensorflow.keras.preprocessing.sequence import pad_sequencestrain_seq = pad_sequences(train_input, maxlen=100) # cut the front part of sequencesprint(train_seq.shape) # the number of data = 2000, length of each data = 100 (20000, 100) 1print(train_seq[0]) [ 10 4 20 9 2 364 352 5 45 6 2 2 33 269 8 2 142 2 5 2 17 73 17 204 5 2 19 55 2 2 92 66 104 14 20 93 76 2 151 33 4 58 12 188 2 151 12 215 69 224 142 73 237 6 2 7 2 2 188 2 103 14 31 10 10 451 7 2 5 2 80 91 2 30 2 34 14 20 151 50 26 131 49 2 84 46 50 37 80 79 6 2 46 7 14 20 10 10 470 158] 1print(train_input[0][-10:]) [6, 2, 46, 7, 14, 20, 10, 10, 470, 158] 1print(train_seq[5]) [ 0 0 0 0 1 2 195 19 49 2 2 190 4 2 352 2 183 10 10 13 82 79 4 2 36 71 269 8 2 25 19 49 7 4 2 2 2 2 2 10 10 48 25 40 2 11 2 2 40 2 2 5 4 2 2 95 14 238 56 129 2 10 10 21 2 94 364 352 2 2 11 190 24 484 2 7 94 205 405 10 10 87 2 34 49 2 7 2 2 2 2 2 290 2 46 48 64 18 4 2] 1val_seq = pad_sequences(val_input, maxlen=100) RNN Model 100 : Length of each text data 500 : Numer of words 1234from tensorflow import kerasmodel = keras.Sequential()model.add(keras.layers.SimpleRNN(8, input_shape=(100,500)))model.add(keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;)) one-hot encoding 12train_oh = keras.utils.to_categorical(train_seq)print(train_oh.shape) (20000, 100, 500) 1print(train_oh[0][0][:12]) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] 1print(np.sum(train_oh[0][0])) 1.0 12val_oh = keras.utils.to_categorical(val_seq)print(val_oh.shape) (5000, 100, 500) model structure 1model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn (SimpleRNN) (None, 8) 4072 dense (Dense) (None, 1) 9 ================================================================= Total params: 4,081 Trainable params: 4,081 Non-trainable params: 0 _________________________________________________________________ model fitting 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model.compile(optimizer=rmsprop, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-simplernn-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model.fit(train_oh, train_target, epochs=100, batch_size=64, validation_data=(val_oh, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 47s 135ms/step - loss: 0.6977 - accuracy: 0.5094 - val_loss: 0.6916 - val_accuracy: 0.5338 Epoch 2/100 313/313 [==============================] - 41s 131ms/step - loss: 0.6828 - accuracy: 0.5623 - val_loss: 0.6780 - val_accuracy: 0.5792 Epoch 3/100 313/313 [==============================] - 72s 230ms/step - loss: 0.6685 - accuracy: 0.6031 - val_loss: 0.6649 - val_accuracy: 0.6114 Epoch 4/100 313/313 [==============================] - 57s 183ms/step - loss: 0.6525 - accuracy: 0.6366 - val_loss: 0.6478 - val_accuracy: 0.6438 Epoch 5/100 313/313 [==============================] - 41s 131ms/step - loss: 0.6280 - accuracy: 0.6789 - val_loss: 0.6241 - val_accuracy: 0.6850 Epoch 6/100 313/313 [==============================] - 41s 131ms/step - loss: 0.6086 - accuracy: 0.7050 - val_loss: 0.6063 - val_accuracy: 0.7042 Epoch 7/100 313/313 [==============================] - 42s 134ms/step - loss: 0.5897 - accuracy: 0.7224 - val_loss: 0.5904 - val_accuracy: 0.7158 Epoch 8/100 313/313 [==============================] - 41s 131ms/step - loss: 0.5732 - accuracy: 0.7354 - val_loss: 0.5774 - val_accuracy: 0.7188 Epoch 9/100 313/313 [==============================] - 54s 174ms/step - loss: 0.5576 - accuracy: 0.7492 - val_loss: 0.5626 - val_accuracy: 0.7364 Epoch 10/100 313/313 [==============================] - 41s 132ms/step - loss: 0.5432 - accuracy: 0.7583 - val_loss: 0.5500 - val_accuracy: 0.7444 Epoch 11/100 313/313 [==============================] - 43s 137ms/step - loss: 0.5301 - accuracy: 0.7650 - val_loss: 0.5391 - val_accuracy: 0.7480 Epoch 12/100 313/313 [==============================] - 51s 164ms/step - loss: 0.5188 - accuracy: 0.7713 - val_loss: 0.5304 - val_accuracy: 0.7608 Epoch 13/100 313/313 [==============================] - 40s 129ms/step - loss: 0.5078 - accuracy: 0.7771 - val_loss: 0.5276 - val_accuracy: 0.7526 Epoch 14/100 313/313 [==============================] - 42s 133ms/step - loss: 0.4980 - accuracy: 0.7824 - val_loss: 0.5108 - val_accuracy: 0.7698 Epoch 15/100 313/313 [==============================] - 45s 143ms/step - loss: 0.4889 - accuracy: 0.7865 - val_loss: 0.5043 - val_accuracy: 0.7708 Epoch 16/100 313/313 [==============================] - 42s 135ms/step - loss: 0.4807 - accuracy: 0.7901 - val_loss: 0.4944 - val_accuracy: 0.7752 Epoch 17/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4730 - accuracy: 0.7957 - val_loss: 0.4903 - val_accuracy: 0.7758 Epoch 18/100 313/313 [==============================] - 42s 135ms/step - loss: 0.4661 - accuracy: 0.7979 - val_loss: 0.4878 - val_accuracy: 0.7744 Epoch 19/100 313/313 [==============================] - 40s 129ms/step - loss: 0.4602 - accuracy: 0.7994 - val_loss: 0.4813 - val_accuracy: 0.7808 Epoch 20/100 313/313 [==============================] - 40s 127ms/step - loss: 0.4543 - accuracy: 0.8031 - val_loss: 0.4756 - val_accuracy: 0.7804 Epoch 21/100 313/313 [==============================] - 40s 128ms/step - loss: 0.4492 - accuracy: 0.8030 - val_loss: 0.4719 - val_accuracy: 0.7816 Epoch 22/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4448 - accuracy: 0.8059 - val_loss: 0.4724 - val_accuracy: 0.7800 Epoch 23/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4412 - accuracy: 0.8077 - val_loss: 0.4671 - val_accuracy: 0.7852 Epoch 24/100 313/313 [==============================] - 42s 134ms/step - loss: 0.4374 - accuracy: 0.8097 - val_loss: 0.4642 - val_accuracy: 0.7894 Epoch 25/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4338 - accuracy: 0.8113 - val_loss: 0.4635 - val_accuracy: 0.7862 Epoch 26/100 313/313 [==============================] - 40s 127ms/step - loss: 0.4304 - accuracy: 0.8121 - val_loss: 0.4596 - val_accuracy: 0.7900 Epoch 27/100 313/313 [==============================] - 40s 127ms/step - loss: 0.4278 - accuracy: 0.8130 - val_loss: 0.4588 - val_accuracy: 0.7928 Epoch 28/100 313/313 [==============================] - 40s 128ms/step - loss: 0.4255 - accuracy: 0.8135 - val_loss: 0.4585 - val_accuracy: 0.7930 Epoch 29/100 313/313 [==============================] - 41s 132ms/step - loss: 0.4229 - accuracy: 0.8149 - val_loss: 0.4619 - val_accuracy: 0.7888 Epoch 30/100 313/313 [==============================] - 41s 130ms/step - loss: 0.4205 - accuracy: 0.8156 - val_loss: 0.4560 - val_accuracy: 0.7950 Epoch 31/100 313/313 [==============================] - 41s 130ms/step - loss: 0.4182 - accuracy: 0.8163 - val_loss: 0.4575 - val_accuracy: 0.7922 Epoch 32/100 313/313 [==============================] - 40s 128ms/step - loss: 0.4168 - accuracy: 0.8195 - val_loss: 0.4564 - val_accuracy: 0.7910 Epoch 33/100 313/313 [==============================] - 42s 133ms/step - loss: 0.4143 - accuracy: 0.8180 - val_loss: 0.4573 - val_accuracy: 0.7872 1234567fig, ax = plt.subplots()ax.plot(history.history[&#x27;loss&#x27;])ax.plot(history.history[&#x27;val_loss&#x27;])ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;loss&#x27;)ax.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() 1print(train_seq.nbytes, train_oh.nbytes) 8000000 4000000000 Word Embedding: Replace each word with a real number vector of fixed size. It solves memory inefficiencies in one-hot encoding. Since it receives an integer data as an input, train_seq can be used. 1234model2 = keras.Sequential()model2.add(keras.layers.Embedding(500, 16, input_length=100))model2.add(keras.layers.SimpleRNN(8, input_shape=(100,500)))model2.add(keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;)) 1model2.summary() Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 16) 8000 simple_rnn_1 (SimpleRNN) (None, 8) 200 dense_1 (Dense) (None, 1) 9 ================================================================= Total params: 8,209 Trainable params: 8,209 Non-trainable params: 0 _________________________________________________________________ 123456789101112rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)model2.compile(optimizer=rmsprop, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-simplernn-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/100 313/313 [==============================] - 52s 162ms/step - loss: 0.6938 - accuracy: 0.5105 - val_loss: 0.6909 - val_accuracy: 0.5236 Epoch 2/100 313/313 [==============================] - 51s 162ms/step - loss: 0.6747 - accuracy: 0.5947 - val_loss: 0.6529 - val_accuracy: 0.6548 Epoch 3/100 313/313 [==============================] - 53s 169ms/step - loss: 0.6280 - accuracy: 0.6870 - val_loss: 0.6142 - val_accuracy: 0.6996 Epoch 4/100 313/313 [==============================] - 52s 166ms/step - loss: 0.5937 - accuracy: 0.7228 - val_loss: 0.5887 - val_accuracy: 0.7214 Epoch 5/100 313/313 [==============================] - 52s 165ms/step - loss: 0.5699 - accuracy: 0.7440 - val_loss: 0.5712 - val_accuracy: 0.7298 Epoch 6/100 313/313 [==============================] - 51s 163ms/step - loss: 0.5495 - accuracy: 0.7571 - val_loss: 0.5569 - val_accuracy: 0.7476 Epoch 7/100 313/313 [==============================] - 51s 164ms/step - loss: 0.5361 - accuracy: 0.7618 - val_loss: 0.5451 - val_accuracy: 0.7444 Epoch 8/100 313/313 [==============================] - 52s 166ms/step - loss: 0.5230 - accuracy: 0.7689 - val_loss: 0.5360 - val_accuracy: 0.7514 Epoch 9/100 313/313 [==============================] - 53s 168ms/step - loss: 0.5127 - accuracy: 0.7724 - val_loss: 0.5302 - val_accuracy: 0.7510 Epoch 10/100 313/313 [==============================] - 52s 165ms/step - loss: 0.5043 - accuracy: 0.7758 - val_loss: 0.5213 - val_accuracy: 0.7554 Epoch 11/100 313/313 [==============================] - 52s 167ms/step - loss: 0.4956 - accuracy: 0.7806 - val_loss: 0.5188 - val_accuracy: 0.7544 Epoch 12/100 313/313 [==============================] - 53s 170ms/step - loss: 0.4899 - accuracy: 0.7823 - val_loss: 0.5170 - val_accuracy: 0.7604 Epoch 13/100 313/313 [==============================] - 53s 170ms/step - loss: 0.4858 - accuracy: 0.7839 - val_loss: 0.5110 - val_accuracy: 0.7604 Epoch 14/100 313/313 [==============================] - 52s 166ms/step - loss: 0.4809 - accuracy: 0.7873 - val_loss: 0.5086 - val_accuracy: 0.7626 Epoch 15/100 313/313 [==============================] - 53s 168ms/step - loss: 0.4763 - accuracy: 0.7897 - val_loss: 0.5061 - val_accuracy: 0.7658 Epoch 16/100 313/313 [==============================] - 53s 169ms/step - loss: 0.4733 - accuracy: 0.7912 - val_loss: 0.5111 - val_accuracy: 0.7576 Epoch 17/100 313/313 [==============================] - 54s 172ms/step - loss: 0.4692 - accuracy: 0.7901 - val_loss: 0.5097 - val_accuracy: 0.7556 Epoch 18/100 313/313 [==============================] - 54s 173ms/step - loss: 0.4676 - accuracy: 0.7922 - val_loss: 0.4955 - val_accuracy: 0.7692 Epoch 19/100 313/313 [==============================] - 54s 171ms/step - loss: 0.4653 - accuracy: 0.7943 - val_loss: 0.4997 - val_accuracy: 0.7694 Epoch 20/100 313/313 [==============================] - 54s 172ms/step - loss: 0.4656 - accuracy: 0.7929 - val_loss: 0.4959 - val_accuracy: 0.7700 Epoch 21/100 313/313 [==============================] - 54s 172ms/step - loss: 0.4618 - accuracy: 0.7964 - val_loss: 0.4962 - val_accuracy: 0.7676 1234567fig, ax = plt.subplots()ax.plot(history.history[&#x27;loss&#x27;])ax.plot(history.history[&#x27;val_loss&#x27;])ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;loss&#x27;)ax.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 9_1","slug":"Python/ML/ML_ch_9_1","date":"2022-04-08T01:23:11.000Z","updated":"2022-10-05T05:39:53.837Z","comments":true,"path":"2022/04/08/Python/ML/ML_ch_9_1/","link":"","permalink":"http://gonekng.github.io/2022/04/08/Python/ML/ML_ch_9_1/","excerpt":"","text":"Sequential data meaningful in order such as text data, time series data Requires the function to remember previously entered data Text data text mining (representatively, sentimental analysis) natural language processing (using chatbot) basic deep learning algorithm : RNN, LSTM tensorflow: https://wikidocs.net/book/2155 pytorch: https://wikidocs.net/32471 RNN (Recurrent Neural Network) the contrary of feedfoward neural network Fully connected layer + Loop that circulates the processing flow of previous data timestep : a step in processing a sample activation : default tanh (S-shape, -1 to 1) 1 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 8_2","slug":"Python/ML/ML_ch_8_2","date":"2022-04-07T02:47:21.000Z","updated":"2022-10-05T05:39:53.765Z","comments":true,"path":"2022/04/07/Python/ML/ML_ch_8_2/","link":"","permalink":"http://gonekng.github.io/2022/04/07/Python/ML/ML_ch_8_2/","excerpt":"","text":"Prepare Fashion Mnist Data12345678from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) =\\ keras.datasets.fashion_mnist.load_data()train_scaled = train_input.reshape(-1, 28, 28, 1)/255.0 # standardization (0~255 -&gt; 0~1)train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step 26435584/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step Create CNN Model1234567891011121314model = keras.Sequential()model.add(keras.layers.Conv2D(32, kernel_size=3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;, input_shape=(28,28,1))) # convolution layer (32 kernels of 3*3 size)model.add(keras.layers.MaxPooling2D(2)) # pooling layer (max pooling of 2*2 size)model.add(keras.layers.Conv2D(64, kernel_size=3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)) # convolution layer (64 kernels of 3*3 size)model.add(keras.layers.MaxPooling2D(2)) # pooling layer (max pooling of 2*2 size)model.add(keras.layers.Flatten()) # the two-dimensional input array into one dimensionmodel.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;)) # hidden layermodel.add(keras.layers.Dropout(0.4)) # drop out 40%model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) # output layermodel.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 32) 320 max_pooling2d (MaxPooling2D (None, 14, 14, 32) 0 ) conv2d_1 (Conv2D) (None, 14, 14, 64) 18496 max_pooling2d_1 (MaxPooling (None, 7, 7, 64) 0 2D) flatten (Flatten) (None, 3136) 0 dense (Dense) (None, 100) 313700 dropout (Dropout) (None, 100) 0 dense_1 (Dense) (None, 10) 1010 ================================================================= Total params: 333,526 Trainable params: 333,526 Non-trainable params: 0 _________________________________________________________________ 1keras.utils.plot_model(model) 1keras.utils.plot_model(model, show_shapes=True) Compile and Fit123456789101112# optimizer: adammodel.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)# callback and early stoppingcheckpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-cnn-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) Epoch 1/20 1500/1500 [==============================] - 65s 43ms/step - loss: 0.5331 - accuracy: 0.8090 - val_loss: 0.3290 - val_accuracy: 0.8807 Epoch 2/20 1500/1500 [==============================] - 69s 46ms/step - loss: 0.3533 - accuracy: 0.8729 - val_loss: 0.2771 - val_accuracy: 0.8973 Epoch 3/20 1500/1500 [==============================] - 64s 43ms/step - loss: 0.3032 - accuracy: 0.8901 - val_loss: 0.2525 - val_accuracy: 0.9049 Epoch 4/20 1500/1500 [==============================] - 64s 43ms/step - loss: 0.2691 - accuracy: 0.9030 - val_loss: 0.2521 - val_accuracy: 0.9081 Epoch 5/20 1500/1500 [==============================] - 63s 42ms/step - loss: 0.2445 - accuracy: 0.9096 - val_loss: 0.2351 - val_accuracy: 0.9107 Epoch 6/20 1500/1500 [==============================] - 66s 44ms/step - loss: 0.2262 - accuracy: 0.9150 - val_loss: 0.2259 - val_accuracy: 0.9162 Epoch 7/20 1500/1500 [==============================] - 65s 43ms/step - loss: 0.2075 - accuracy: 0.9229 - val_loss: 0.2284 - val_accuracy: 0.9153 Epoch 8/20 1500/1500 [==============================] - 63s 42ms/step - loss: 0.1921 - accuracy: 0.9276 - val_loss: 0.2376 - val_accuracy: 0.9173 Validation and Predict loss function (visualize train loss and validation loss) 12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(history.history[&#x27;loss&#x27;])ax.plot(history.history[&#x27;val_loss&#x27;])ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;loss&#x27;)ax.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() The epoch 6 appears to be optimal. 1model.evaluate(val_scaled, val_target) # same as output of epoch 6 375/375 [==============================] - 5s 12ms/step - loss: 0.2259 - accuracy: 0.9162 [0.22585801780223846, 0.9161666631698608] Confirm prediction of the first sample 123fig, ax = plt.subplots()ax.imshow(val_scaled[0].reshape(28,28), cmap=&#x27;gray_r&#x27;)plt.show() 1234# The fit(), predict(), and evaluate() all expect the first dimension of the input to be the batch dimension.# Slicing, unlike indexing, maintains the entire dimension even if there is one element.preds = model.predict(val_scaled[0:1])print(preds) [[1.88540562e-13 1.07813886e-17 3.65283819e-17 1.44768129e-14 7.86518068e-17 2.58884016e-15 2.82545543e-15 2.49630367e-13 1.00000000e+00 1.07822686e-14]] 12345fig, ax = plt.subplots()ax.bar(range(1,11),preds[0])ax.set_xlabel(&#x27;class&#x27;)ax.set_ylabel(&#x27;prob.&#x27;)plt.show() 12345classes = [&#x27;t-shirt&#x27;, &#x27;pants&#x27;, &#x27;sweater&#x27;, &#x27;dress&#x27;, &#x27;coat&#x27;, &#x27;sandal&#x27;, &#x27;shirt&#x27;, &#x27;sneakers&#x27;, &#x27;bag&#x27;, &#x27;boots&#x27;]import numpy as npprint(classes[np.argmax(preds)]) bag Model Test12test_scaled = test_input.reshape(-1,28,28,1)/255.0model.evaluate(test_scaled, test_target) 313/313 [==============================] - 4s 13ms/step - loss: 0.2519 - accuracy: 0.9093 [0.25190481543540955, 0.9093000292778015] GPU check in Google Colab12345678&#x27;&#x27;&#x27;import tensorflow as tfdevice_name = tf.test.gpu_device_name()if device_name != &#x27;/device:GPU:0&#x27;: raise SystemError(&#x27;GPU device not found&#x27;)print(&#x27;Found GPU at: &#123;&#125;&#x27;.format(device_name))&#x27;&#x27;&#x27; 123456789101112131415&#x27;&#x27;&#x27;import tensorflow as tfmodel.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-cnn-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)with tf.device(&#x27;/device:GPU:0&#x27;): history = model.fit(train_scaled, train_target, epochs=10, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb])&#x27;&#x27;&#x27; Visualize CNN12model = keras.models.load_model(&#x27;best-cnn-model.h5&#x27;)model.layers [&lt;keras.layers.convolutional.Conv2D at 0x7f1fc8314c10&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f1fc82d34d0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7f1fc82d3490&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f1fc8393f10&gt;, &lt;keras.layers.core.flatten.Flatten at 0x7f1fc8332050&gt;, &lt;keras.layers.core.dense.Dense at 0x7f1fc83954d0&gt;, &lt;keras.layers.core.dropout.Dropout at 0x7f1fc8395110&gt;, &lt;keras.layers.core.dense.Dense at 0x7f1fc83f0710&gt;] check weights of the first convolution layer 12conv = model.layers[0]print(conv.weights[0].shape, conv.weights[1].shape) (3, 3, 1, 32) (32,) 12conv_weights = conv.weights[0].numpy()print(conv_weights.mean(), conv_weights.std()) -0.010425919 0.21259554 12345fig, ax = plt.subplots()ax.hist(conv_weights.reshape(-1,1))ax.set_xlabel(&#x27;weight&#x27;)ax.set_ylabel(&#x27;count&#x27;)plt.show() 1234567# 32 kernels of 3*3 sizefig, ax = plt.subplots(2, 16, figsize=(15,2))for i in range(2): for j in range(16): ax[i,j].imshow(conv_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) ax[i,j].axis(&#x27;off&#x27;)plt.show() Compare to empty CNN which is untrained 12345no_training_model = keras.Sequential()no_training_model.add(keras.layers.Conv2D(32, kernel_size=3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;, input_shape=(28,28,1)))no_training_conv = no_training_model.layers[0]print(no_training_conv.weights[0].shape) (3, 3, 1, 32) 12no_training_weights = no_training_conv.weights[0].numpy()print(no_training_weights.mean(), no_training_weights.std()) -0.008161874 0.08076286 12345fig, ax = plt.subplots()ax.hist(no_training_weights.reshape(-1,1))ax.set_xlabel(&#x27;weight&#x27;)ax.set_ylabel(&#x27;count&#x27;)plt.show() It shows a relatively even distributionbecause tensorflow randomly select a value from an equal distribution at first. 123456fig, ax = plt.subplots(2, 16, figsize=(15,2))for i in range(2): for j in range(16): ax[i,j].imshow(no_training_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) ax[i,j].axis(&#x27;off&#x27;)plt.show() Visualize Feature Map feature map of the first convolution layer 12345print(model.input)conv_acti = keras.Model(model.input, model.layers[0].output) # functional API(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()plt.imshow(train_input[0], cmap=&#x27;gray_r&#x27;)plt.show() KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;conv2d_input&#39;), name=&#39;conv2d_input&#39;, description=&quot;created by layer &#39;conv2d_input&#39;&quot;) 123inputs = train_input[0:1].reshape(-1, 28, 28, 1)/255.0feature_maps = conv_acti.predict(inputs)print(feature_maps.shape) (1, 28, 28, 32) 123456fig, ax = plt.subplots(4, 8, figsize=(15,8))for i in range(4): for j in range(8): ax[i,j].imshow(feature_maps[0,:,:,i*8+j]) ax[i,j].axis(&#x27;off&#x27;)plt.show() feature map of the second convolution layer 123456789101112conv2_acti = keras.Model(model.input, model.layers[2].output)feature_maps = conv2_acti.predict(train_input[0:1].reshape(-1, 28, 28, 1)/255.0)print(feature_maps.shape)fig, axs = plt.subplots(8, 8, figsize=(12,12))for i in range(8): for j in range(8): axs[i, j].imshow(feature_maps[0,:,:,i*8 + j]) axs[i, j].axis(&#x27;off&#x27;)plt.show() (1, 14, 14, 64) Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 8_1","slug":"Python/ML/ML_ch_8_1","date":"2022-04-06T02:26:18.000Z","updated":"2022-10-05T05:39:53.685Z","comments":true,"path":"2022/04/06/Python/ML/ML_ch_8_1/","link":"","permalink":"http://gonekng.github.io/2022/04/06/Python/ML/ML_ch_8_1/","excerpt":"","text":"CNN(Convolution Neural Network) Neural network operations can also be applied to two-dimensional arrays by CNN. Neuron in CNN is called filter or kernel. 12from tensorflow import keraskeras.layers.Conv2D(10, kernel_size=(3,3), activation=&quot;relu&quot;) &lt;keras.layers.convolutional.Conv2D at 0x7effd27dea10&gt; Padding &amp; StridePadding : Filling the border of the input array with virtual elements To prevent the loss of the original features of the image even if you resize the array, Same padding : Padding to zero around the input to make the input and feature map the same size Valid padding : Convolution only in a pure input array without padding Stride : Size of the filter moving over the input layer (default 1)1keras.layers.Conv2D(10, kernel_size=(3,3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;, strides=1) &lt;keras.layers.convolutional.Conv2D at 0x7effceb4fb10&gt; Pooling Reducing the size of the feature map while maintaining the original features of the image Max pooling, Average pooling, etc 1keras.layers.MaxPooling2D(2, strides=2, padding=&quot;valid&quot;) &lt;keras.layers.pooling.MaxPooling2D at 0x7effce850fd0&gt; 1keras.layers.AveragePooling2D(2, strides=2, padding=&quot;valid&quot;) &lt;keras.layers.pooling.AveragePooling2D at 0x7effcea305d0&gt; Overall process in CNN Input Image Data CNN Layer kernel_size, padding, stride activation function Calculate each feature map Pooling Layer Maxpooling &#x2F; Averagepooling final feature map Repeat the above process Fully Connected Layer Calculate classification predictions (Softmax) Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"Pipeline Tutorial","slug":"Python/Tutorial/Pipeline_tutorial","date":"2022-04-06T01:05:01.000Z","updated":"2022-10-05T05:39:54.302Z","comments":true,"path":"2022/04/06/Python/Tutorial/Pipeline_tutorial/","link":"","permalink":"http://gonekng.github.io/2022/04/06/Python/Tutorial/Pipeline_tutorial/","excerpt":"","text":"Pipeline : 데이터 누수(Data Leakge) 방지를 위한 모델링 기법 Pycaret, MLOps (Pipeline 형태로 구축) 머신러닝 코드의 자동화 및 운영 가능 기존 방식 데이터 불러오기 -&gt; 데이터 전처리 -&gt; 특성 공학 -&gt; 데이터셋 분리 -&gt; 모델링 -&gt; 평가 파이프라인 방식 데이터 불러오기 -&gt; 데이터 전처리 -&gt; 데이터셋 분리 -&gt; 파이프라인 구축(피처공학, 모델링) -&gt; 평가 데이터 불러오기1234import pandas as pdimport numpy as npdata = pd.read_csv(&#x27;https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv&#x27;)data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 731 entries, 0 to 730 Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 instant 731 non-null int64 1 dteday 731 non-null object 2 season 731 non-null int64 3 yr 731 non-null int64 4 mnth 731 non-null int64 5 holiday 731 non-null int64 6 weekday 731 non-null int64 7 workingday 731 non-null int64 8 weathersit 731 non-null int64 9 temp 731 non-null float64 10 atemp 731 non-null float64 11 hum 731 non-null float64 12 windspeed 731 non-null float64 13 rentals 731 non-null int64 dtypes: float64(4), int64(9), object(1) memory usage: 80.1+ KB 12345from sklearn.model_selection import train_test_splitX = data.drop(&#x27;rentals&#x27;,axis=1)y = data[&#x27;rentals&#x27;]X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123) Pipeline 구축데이터 전처리 파이프라인12345678910111213141516171819202122232425262728293031323334353637from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoderfrom sklearn.impute import SimpleImputerfrom sklearn.compose import ColumnTransformerfrom sklearn.pipeline import Pipeline# 수치형 데이터numeric_transformer = Pipeline(steps=[ (&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;mean&#x27;)) ,(&#x27;scaler&#x27;, StandardScaler())])# 서열형 데이터ordinal_transformer = Pipeline(steps=[ (&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;constant&#x27;)) ,(&#x27;ordEncoder&#x27;, OrdinalEncoder())])# 명목형 데이터onehot_transformer = Pipeline(steps=[ (&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;constant&#x27;)) ,(&#x27;oheEncoder&#x27;, OneHotEncoder()) ])# 수치형 데이터 및 Categorical 데이터 컬럼 분리numeric_features = [&#x27;temp&#x27;, &#x27;atemp&#x27;, &#x27;hum&#x27;, &#x27;windspeed&#x27;]ordinal_features = [&#x27;holiday&#x27;, &#x27;weekday&#x27;, &#x27;workingday&#x27;, &#x27;weathersit&#x27;]onehot_features = [&#x27;season&#x27;, &#x27;mnth&#x27;]# numeric_features = data.select_dtypes(include=[&#x27;int64&#x27;, &#x27;float64&#x27;]).columns# categorical_features = data.select_dtypes(include=[&#x27;object&#x27;]).drop([&#x27;Loan_Status&#x27;], axis=1).columnspreprocessor = ColumnTransformer( transformers=[ (&#x27;numeric&#x27;, numeric_transformer, numeric_features) , (&#x27;ord_categorical&#x27;, ordinal_transformer, ordinal_features) , (&#x27;ohe_categorical&#x27;, onehot_transformer, onehot_features)]) 모델 적용 파이프라인123456789from sklearn.ensemble import RandomForestRegressorpipeline = Pipeline(steps = [ (&#x27;preprocessor&#x27;, preprocessor) # 전처리 파이프라인 ,(&#x27;regressor&#x27;, RandomForestRegressor()) # 모델 연결 ])rf_model = pipeline.fit(X_train, y_train)print(rf_model) Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(transformers=[(&#39;numeric&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler())]), [&#39;temp&#39;, &#39;atemp&#39;, &#39;hum&#39;, &#39;windspeed&#39;]), (&#39;ord_categorical&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)), (&#39;ordEncoder&#39;, OrdinalEncoder())]), [&#39;holiday&#39;, &#39;weekday&#39;, &#39;workingday&#39;, &#39;weathersit&#39;]), (&#39;ohe_categorical&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)), (&#39;oheEncoder&#39;, OneHotEncoder())]), [&#39;season&#39;, &#39;mnth&#39;])])), (&#39;regressor&#39;, RandomForestRegressor())]) 모델 평가123from sklearn.metrics import r2_scorepredictions = rf_model.predict(X_val)print (r2_score(y_val, predictions)) 0.7654903256614782 다중 모형 개발1234567891011121314151617181920from sklearn.ensemble import RandomForestRegressorfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.linear_model import LinearRegressionregressors = [ RandomForestRegressor(), DecisionTreeRegressor(), LinearRegression()]# regressors = [pipe_rf, pipe_dt]for regressor in regressors: pipeline = Pipeline(steps = [ (&#x27;preprocessor&#x27;, preprocessor) ,(&#x27;regressor&#x27;,regressor) ]) model = pipeline.fit(X_train, y_train) predictions = model.predict(X_val) print(regressor) print(f&#x27;Model r2 score:&#123;r2_score(predictions, y_val)&#125;&#x27;) RandomForestRegressor() Model r2 score:0.7447806201844671 DecisionTreeRegressor() Model r2 score:0.5885371412997458 LinearRegression() Model r2 score:0.5703227526319388","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"},{"name":"pipeline","slug":"pipeline","permalink":"http://gonekng.github.io/tags/pipeline/"}],"author":"Jiwon Kang"},{"title":"ML Practice 7_3","slug":"Python/ML/ML_ch_7_3","date":"2022-04-05T03:14:17.000Z","updated":"2022-10-05T05:39:53.612Z","comments":true,"path":"2022/04/05/Python/ML/ML_ch_7_3/","link":"","permalink":"http://gonekng.github.io/2022/04/05/Python/ML/ML_ch_7_3/","excerpt":"","text":"Create DNN Model12345678910from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = \\ keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 0s 0us/step 26435584/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step Define function of create model 1234567891011def model_fn(a_layer=None): model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28,28))) model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;)) if a_layer: model.add(a_layer) model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) return model model = model_fn()model.summary &lt;bound method Model.summary of &lt;keras.engine.sequential.Sequential object at 0x7f9181716750&gt;&gt; Loss Curve12model.compile(loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)history = model.fit(train_scaled, train_target, epochs = 5, verbose = 0) verbose default 1: print the indicator along with the progress bar per epoch. verbose 2: print the indicator without the progress bar per epoch. verbose 0: print none 123print(history) # classprint(history.history) # dictionaryprint(history.history.keys()) &lt;keras.callbacks.History object at 0x7f917bab27d0&gt; &#123;&#39;loss&#39;: [0.5360119342803955, 0.3935061991214752, 0.3552784025669098, 0.33411645889282227, 0.31946054100990295], &#39;accuracy&#39;: [0.8113541603088379, 0.8598541617393494, 0.8732083439826965, 0.8820000290870667, 0.8862708210945129]&#125; dict_keys([&#39;loss&#39;, &#39;accuracy&#39;]) loss curve (epoch 5) 123456import matplotlib.pyplot as pltplt.plot(history.history[&#x27;loss&#x27;])plt.xlabel(&#x27;epoch&#x27;)plt.ylabel(&#x27;loss&#x27;)plt.show() accuracy curve (epoch 5) 1234plt.plot(history.history[&#x27;accuracy&#x27;])plt.xlabel(&#x27;epoch&#x27;)plt.ylabel(&#x27;accuracy&#x27;)plt.show() loss curve (epoch 20) 12345678model = model_fn()model.compile(loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)history = model.fit(train_scaled, train_target, epochs=20, verbose=0)plt.plot(history.history[&#x27;loss&#x27;])plt.xlabel(&#x27;epoch&#x27;)plt.ylabel(&#x27;loss&#x27;)plt.show() Validation Loss123456789101112model = model_fn()model.compile(loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)history = model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target))plt.plot(history.history[&#x27;loss&#x27;])plt.plot(history.history[&#x27;val_loss&#x27;])plt.xlabel(&#x27;epoch&#x27;)plt.ylabel(&#x27;loss&#x27;)plt.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() Epoch 1/20 1500/1500 [==============================] - 6s 4ms/step - loss: 0.5344 - accuracy: 0.8118 - val_loss: 0.4414 - val_accuracy: 0.8471 Epoch 2/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3950 - accuracy: 0.8577 - val_loss: 0.3638 - val_accuracy: 0.8668 Epoch 3/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3573 - accuracy: 0.8702 - val_loss: 0.3754 - val_accuracy: 0.8682 Epoch 4/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3365 - accuracy: 0.8791 - val_loss: 0.3783 - val_accuracy: 0.8701 Epoch 5/20 1500/1500 [==============================] - 5s 4ms/step - loss: 0.3191 - accuracy: 0.8865 - val_loss: 0.3576 - val_accuracy: 0.8772 Epoch 6/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3085 - accuracy: 0.8898 - val_loss: 0.3556 - val_accuracy: 0.8806 Epoch 7/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2982 - accuracy: 0.8948 - val_loss: 0.3736 - val_accuracy: 0.8807 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2910 - accuracy: 0.8976 - val_loss: 0.3443 - val_accuracy: 0.8869 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2841 - accuracy: 0.8998 - val_loss: 0.3757 - val_accuracy: 0.8832 Epoch 10/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2755 - accuracy: 0.9031 - val_loss: 0.4034 - val_accuracy: 0.8766 Epoch 11/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2700 - accuracy: 0.9059 - val_loss: 0.4085 - val_accuracy: 0.8792 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2655 - accuracy: 0.9075 - val_loss: 0.3936 - val_accuracy: 0.8835 Epoch 13/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2589 - accuracy: 0.9105 - val_loss: 0.4122 - val_accuracy: 0.8812 Epoch 14/20 1500/1500 [==============================] - 5s 4ms/step - loss: 0.2545 - accuracy: 0.9116 - val_loss: 0.4056 - val_accuracy: 0.8842 Epoch 15/20 1500/1500 [==============================] - 6s 4ms/step - loss: 0.2506 - accuracy: 0.9137 - val_loss: 0.4048 - val_accuracy: 0.8815 Epoch 16/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2454 - accuracy: 0.9159 - val_loss: 0.4132 - val_accuracy: 0.8808 Epoch 17/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2410 - accuracy: 0.9177 - val_loss: 0.4343 - val_accuracy: 0.8831 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2356 - accuracy: 0.9190 - val_loss: 0.4574 - val_accuracy: 0.8767 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2326 - accuracy: 0.9201 - val_loss: 0.4499 - val_accuracy: 0.8817 Epoch 20/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.2284 - accuracy: 0.9204 - val_loss: 0.4834 - val_accuracy: 0.8751 There is a large difference in loss between training data and verification data. This is a typical overfitting model. 12345678910111213model = model_fn()model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)history = model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target))plt.plot(history.history[&#x27;loss&#x27;])plt.plot(history.history[&#x27;val_loss&#x27;])plt.xlabel(&#x27;epoch&#x27;)plt.ylabel(&#x27;loss&#x27;)plt.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() Epoch 1/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5231 - accuracy: 0.8183 - val_loss: 0.4741 - val_accuracy: 0.8357 Epoch 2/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3940 - accuracy: 0.8576 - val_loss: 0.3736 - val_accuracy: 0.8671 Epoch 3/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3557 - accuracy: 0.8703 - val_loss: 0.3567 - val_accuracy: 0.8712 Epoch 4/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3273 - accuracy: 0.8796 - val_loss: 0.3398 - val_accuracy: 0.8790 Epoch 5/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3091 - accuracy: 0.8872 - val_loss: 0.3324 - val_accuracy: 0.8803 Epoch 6/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2904 - accuracy: 0.8925 - val_loss: 0.3194 - val_accuracy: 0.8842 Epoch 7/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2802 - accuracy: 0.8967 - val_loss: 0.3333 - val_accuracy: 0.8796 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2679 - accuracy: 0.9010 - val_loss: 0.3265 - val_accuracy: 0.8830 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2588 - accuracy: 0.9040 - val_loss: 0.3298 - val_accuracy: 0.8858 Epoch 10/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2482 - accuracy: 0.9068 - val_loss: 0.3282 - val_accuracy: 0.8840 Epoch 11/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2413 - accuracy: 0.9094 - val_loss: 0.3098 - val_accuracy: 0.8889 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2315 - accuracy: 0.9131 - val_loss: 0.3250 - val_accuracy: 0.8867 Epoch 13/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2260 - accuracy: 0.9141 - val_loss: 0.3164 - val_accuracy: 0.8911 Epoch 14/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2181 - accuracy: 0.9185 - val_loss: 0.3511 - val_accuracy: 0.8774 Epoch 15/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2128 - accuracy: 0.9200 - val_loss: 0.3397 - val_accuracy: 0.8817 Epoch 16/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2059 - accuracy: 0.9222 - val_loss: 0.3219 - val_accuracy: 0.8903 Epoch 17/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2021 - accuracy: 0.9240 - val_loss: 0.3423 - val_accuracy: 0.8859 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.1952 - accuracy: 0.9277 - val_loss: 0.3313 - val_accuracy: 0.8916 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.1918 - accuracy: 0.9272 - val_loss: 0.3396 - val_accuracy: 0.8871 Epoch 20/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.1879 - accuracy: 0.9295 - val_loss: 0.3354 - val_accuracy: 0.8904 Overfitting has decreased a little, but it is still necessary to improve. Dropout Basically, it is a principle to calculate all parameters. Neurons without some output are excluded from the calculation. 12model = model_fn(keras.layers.Dropout(0.3)) # drop out 30%model.summary() Model: &quot;sequential_5&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_5 (Flatten) (None, 784) 0 dense_10 (Dense) (None, 100) 78500 dropout (Dropout) (None, 100) 0 dense_11 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 123456789101112model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)history = model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target))plt.plot(history.history[&#x27;loss&#x27;])plt.plot(history.history[&#x27;val_loss&#x27;])plt.xlabel(&#x27;epoch&#x27;)plt.ylabel(&#x27;loss&#x27;)plt.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() Epoch 1/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5967 - accuracy: 0.7907 - val_loss: 0.4495 - val_accuracy: 0.8294 Epoch 2/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4413 - accuracy: 0.8409 - val_loss: 0.4071 - val_accuracy: 0.8472 Epoch 3/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4044 - accuracy: 0.8547 - val_loss: 0.3616 - val_accuracy: 0.8674 Epoch 4/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3833 - accuracy: 0.8603 - val_loss: 0.3605 - val_accuracy: 0.8651 Epoch 5/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3688 - accuracy: 0.8646 - val_loss: 0.3423 - val_accuracy: 0.8750 Epoch 6/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3542 - accuracy: 0.8696 - val_loss: 0.3479 - val_accuracy: 0.8744 Epoch 7/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3439 - accuracy: 0.8725 - val_loss: 0.3449 - val_accuracy: 0.8752 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3356 - accuracy: 0.8763 - val_loss: 0.3356 - val_accuracy: 0.8802 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3280 - accuracy: 0.8796 - val_loss: 0.3361 - val_accuracy: 0.8801 Epoch 10/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3212 - accuracy: 0.8781 - val_loss: 0.3394 - val_accuracy: 0.8734 Epoch 11/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3200 - accuracy: 0.8813 - val_loss: 0.3327 - val_accuracy: 0.8763 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3115 - accuracy: 0.8852 - val_loss: 0.3325 - val_accuracy: 0.8776 Epoch 13/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3061 - accuracy: 0.8860 - val_loss: 0.3216 - val_accuracy: 0.8860 Epoch 14/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3034 - accuracy: 0.8860 - val_loss: 0.3193 - val_accuracy: 0.8864 Epoch 15/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2961 - accuracy: 0.8880 - val_loss: 0.3198 - val_accuracy: 0.8846 Epoch 16/20 1500/1500 [==============================] - 4s 2ms/step - loss: 0.2913 - accuracy: 0.8900 - val_loss: 0.3310 - val_accuracy: 0.8823 Epoch 17/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2870 - accuracy: 0.8933 - val_loss: 0.3162 - val_accuracy: 0.8848 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2838 - accuracy: 0.8931 - val_loss: 0.3321 - val_accuracy: 0.8838 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2829 - accuracy: 0.8935 - val_loss: 0.3320 - val_accuracy: 0.8840 Epoch 20/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2814 - accuracy: 0.8942 - val_loss: 0.3218 - val_accuracy: 0.8882 Overfitting has improved a lot. Save and Load Model123456model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)history = model.fit(train_scaled, train_target, epochs=10, verbose=0, validation_data=(val_scaled, val_target)) save_weights() : method of saving the parameters of a model save() : method of saving both the parameters and structure of a model ‘.h5’ : HDF5 format 12model.save_weights(&#x27;model-weights.h5&#x27;)model.save(&#x27;model-whole.h5&#x27;) 1!ls -al *.h5 -rw-r--r-- 1 root root 982664 Apr 5 02:37 best-model.h5 -rw-r--r-- 1 root root 333448 Apr 5 02:42 model-weights.h5 -rw-r--r-- 1 root root 982664 Apr 5 02:42 model-whole.h5 load previously saved parameters 12model = model_fn(keras.layers.Dropout(0.3))model.load_weights(&#x27;model-weights.h5&#x27;) 1234# Returns the largest value in the predict method resultimport numpy as npval_labels = np.argmax(model.predict(val_scaled), axis=-1)print(np.mean(val_labels == val_target)) 0.8825833333333334 load previously saved model 12model = keras.models.load_model(&#x27;model-whole.h5&#x27;)model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.8826 [0.3247545063495636, 0.8825833201408386] Callback12345678910model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-model.h5&#x27;, save_best_only=True)model.fit(train_scaled, train_target, epochs=20, verbose=1, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb]) Epoch 1/20 1500/1500 [==============================] - 5s 3ms/step - loss: 0.5955 - accuracy: 0.7909 - val_loss: 0.4305 - val_accuracy: 0.8437 Epoch 2/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4369 - accuracy: 0.8436 - val_loss: 0.3847 - val_accuracy: 0.8572 Epoch 3/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.4027 - accuracy: 0.8533 - val_loss: 0.3737 - val_accuracy: 0.8633 Epoch 4/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3833 - accuracy: 0.8607 - val_loss: 0.3648 - val_accuracy: 0.8628 Epoch 5/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3675 - accuracy: 0.8662 - val_loss: 0.3481 - val_accuracy: 0.8703 Epoch 6/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3544 - accuracy: 0.8710 - val_loss: 0.3434 - val_accuracy: 0.8758 Epoch 7/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3435 - accuracy: 0.8736 - val_loss: 0.3388 - val_accuracy: 0.8781 Epoch 8/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3360 - accuracy: 0.8759 - val_loss: 0.3333 - val_accuracy: 0.8760 Epoch 9/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3261 - accuracy: 0.8777 - val_loss: 0.3333 - val_accuracy: 0.8755 Epoch 10/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3203 - accuracy: 0.8808 - val_loss: 0.3319 - val_accuracy: 0.8807 Epoch 11/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3154 - accuracy: 0.8822 - val_loss: 0.3275 - val_accuracy: 0.8794 Epoch 12/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3063 - accuracy: 0.8849 - val_loss: 0.3206 - val_accuracy: 0.8842 Epoch 13/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3024 - accuracy: 0.8871 - val_loss: 0.3239 - val_accuracy: 0.8815 Epoch 14/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3002 - accuracy: 0.8882 - val_loss: 0.3249 - val_accuracy: 0.8838 Epoch 15/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2928 - accuracy: 0.8911 - val_loss: 0.3237 - val_accuracy: 0.8827 Epoch 16/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2891 - accuracy: 0.8911 - val_loss: 0.3216 - val_accuracy: 0.8839 Epoch 17/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2854 - accuracy: 0.8918 - val_loss: 0.3301 - val_accuracy: 0.8844 Epoch 18/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2834 - accuracy: 0.8942 - val_loss: 0.3315 - val_accuracy: 0.8833 Epoch 19/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2776 - accuracy: 0.8959 - val_loss: 0.3381 - val_accuracy: 0.8790 Epoch 20/20 1500/1500 [==============================] - 4s 3ms/step - loss: 0.2758 - accuracy: 0.8965 - val_loss: 0.3273 - val_accuracy: 0.8830 &lt;keras.callbacks.History at 0x7f916df86590&gt; 12model = keras.models.load_model(&#x27;best-model.h5&#x27;)model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3206 - accuracy: 0.8842 [0.32058343291282654, 0.8842499852180481] early stopping : to stop training before overfitting begins 12345678910model = model_fn(keras.layers.Dropout(0.3))model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-model.h5&#x27;, save_best_only=True)early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) 1print(early_stopping_cb.stopped_epoch) 5 123456plt.plot(history.history[&#x27;loss&#x27;])plt.plot(history.history[&#x27;val_loss&#x27;])plt.xlabel(&#x27;epoch&#x27;)plt.ylabel(&#x27;loss&#x27;)plt.legend([&#x27;train&#x27;, &#x27;val&#x27;])plt.show() It stopped early in 5 epoch, and the issue of overfitting was solved. Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 7_2","slug":"Python/ML/ML_ch_7_2","date":"2022-04-05T01:20:01.000Z","updated":"2022-10-05T05:39:53.531Z","comments":true,"path":"2022/04/05/Python/ML/ML_ch_7_2/","link":"","permalink":"http://gonekng.github.io/2022/04/05/Python/ML/ML_ch_7_2/","excerpt":"","text":"Prepare Dataset123from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() 1234567from sklearn.model_selection import train_test_splittrain_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) DNN Layer12345# hidden layer dense1 = keras.layers.Dense(100, activation=&#x27;sigmoid&#x27;, input_shape=(784,))# output layer dense2 = keras.layers.Dense(10, activation=&#x27;softmax&#x27;) 12model = keras.Sequential([dense1, dense2])model.summary() Model: &quot;sequential_3&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_9 (Dense) (None, 100) 78500 dense_10 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ Another way to add layers123456model = keras.Sequential([ keras.layers.Dense(12, activation=&#x27;sigmoid&#x27;, input_shape=(16,), name=&#x27;hidden&#x27;), keras.layers.Dense(10, activation=&#x27;softmax&#x27;, name=&#x27;hidden_2&#x27;), keras.layers.Dense(1, activation=&#x27;softmax&#x27;, name=&#x27;output&#x27;)], name=&#x27;fashion MNIST&#x27;)model.summary() Model: &quot;fashion MNIST&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= hidden (Dense) (None, 12) 204 hidden_2 (Dense) (None, 10) 130 output (Dense) (None, 1) 11 ================================================================= Total params: 345 Trainable params: 345 Non-trainable params: 0 _________________________________________________________________ 1234model = keras.Sequential()model.add(keras.layers.Dense(100, activation=&#x27;sigmoid&#x27;, input_shape=(784,)))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;))model.summary() Model: &quot;sequential_4&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_11 (Dense) (None, 100) 78500 dense_12 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 12model.compile(loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5627 - accuracy: 0.8077 Epoch 2/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.4080 - accuracy: 0.8529 Epoch 3/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3740 - accuracy: 0.8660 Epoch 4/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3508 - accuracy: 0.8720 Epoch 5/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3345 - accuracy: 0.8810 &lt;keras.callbacks.History at 0x7fcf5e9c8810&gt; Relu function12345model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;))model.summary() Model: &quot;sequential_6&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 784) 0 dense_16 (Dense) (None, 100) 78500 dense_17 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 1model.summary() Model: &quot;sequential_6&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 784) 0 dense_16 (Dense) (None, 100) 78500 dense_17 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ 123456(train_input, train_target), (test_input, test_target) =\\ keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_traget = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) 12model.compile(loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5362 - accuracy: 0.8096 Epoch 2/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3953 - accuracy: 0.8578 Epoch 3/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3570 - accuracy: 0.8722 Epoch 4/5 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3360 - accuracy: 0.8808 Epoch 5/5 1500/1500 [==============================] - 6s 4ms/step - loss: 0.3200 - accuracy: 0.8871 &lt;keras.callbacks.History at 0x7fcf5e813250&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3565 - accuracy: 0.8775 [0.35651248693466187, 0.8774999976158142] Optimizer a variety of gradient descent algorithms provided by Keras Optimizer have to consider both step direction and width direction : GD, SGD, Momentum, NAG width : GD, SGD, Adagrad, RMSProp direction &amp; width : Adam (generally, the best performance) SGDLearning rate: default 0.011234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) 123model.compile(optimizer=&#x27;sgd&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.8096 - accuracy: 0.7362 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.5421 - accuracy: 0.8163 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4886 - accuracy: 0.8329 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4604 - accuracy: 0.8426 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4404 - accuracy: 0.8490 &lt;keras.callbacks.History at 0x7fcf5e524250&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 1ms/step - loss: 0.4474 - accuracy: 0.8464 [0.44738978147506714, 0.8464166522026062] Learning rate: 0.11234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) 1234sgd = keras.optimizers.SGD(learning_rate=0.1)model.compile(optimizer=sgd, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.5663 - accuracy: 0.7985 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4148 - accuracy: 0.8493 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3765 - accuracy: 0.8620 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3503 - accuracy: 0.8707 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3315 - accuracy: 0.8777 &lt;keras.callbacks.History at 0x7fcf5e614e90&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3469 - accuracy: 0.8744 [0.3468727171421051, 0.8744166493415833] Nesterov momentum1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) 1234sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)model.compile(optimizer=sgd, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.5365 - accuracy: 0.8099 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4051 - accuracy: 0.8562 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3659 - accuracy: 0.8690 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3448 - accuracy: 0.8737 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3255 - accuracy: 0.8802 &lt;keras.callbacks.History at 0x7fcf5e1de1d0&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3611 - accuracy: 0.8718 [0.36112430691719055, 0.871833324432373] Adagrad1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) 1234adagrad = keras.optimizers.Adagrad()model.compile(optimizer=adagrad, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 2ms/step - loss: 1.1751 - accuracy: 0.6441 Epoch 2/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.7733 - accuracy: 0.7556 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.6848 - accuracy: 0.7837 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.6372 - accuracy: 0.7972 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.6071 - accuracy: 0.8053 &lt;keras.callbacks.History at 0x7fcf5e480390&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 1ms/step - loss: 0.6081 - accuracy: 0.8025 [0.6081421375274658, 0.8025000095367432] RMSprop1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) 1234rmsprop = keras.optimizers.RMSprop()model.compile(optimizer=rmsprop, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.5261 - accuracy: 0.8139 Epoch 2/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3931 - accuracy: 0.8598 Epoch 3/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.3556 - accuracy: 0.8733 Epoch 4/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3349 - accuracy: 0.8806 Epoch 5/5 1500/1500 [==============================] - 4s 3ms/step - loss: 0.3173 - accuracy: 0.8869 &lt;keras.callbacks.History at 0x7fcf5e374710&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3817 - accuracy: 0.8742 [0.3816753029823303, 0.8741666674613953] Adam1234model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(10, activation=&#x27;softmax&#x27;)) 123model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;)model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.5273 - accuracy: 0.8153 Epoch 2/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3943 - accuracy: 0.8584 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3522 - accuracy: 0.8727 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.3264 - accuracy: 0.8815 Epoch 5/5 1500/1500 [==============================] - 5s 3ms/step - loss: 0.3074 - accuracy: 0.8882 &lt;keras.callbacks.History at 0x7fcf5e47b050&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 2ms/step - loss: 0.3356 - accuracy: 0.8788 [0.33555400371551514, 0.8788333535194397] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 7_1","slug":"Python/ML/ML_ch_7_1","date":"2022-04-04T01:25:10.000Z","updated":"2022-10-05T05:39:53.444Z","comments":true,"path":"2022/04/04/Python/ML/ML_ch_7_1/","link":"","permalink":"http://gonekng.github.io/2022/04/04/Python/ML/ML_ch_7_1/","excerpt":"","text":"Fashion MNISTDeep Learning Library tensorflow : https://www.tensorflow.org/ pytorch : https://pytorch.org/ 12import tensorflowprint(tensorflow.__version__) 2.8.0 Load Data12from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() 60,000 images, which is 28 * 28 size 12print(train_input.shape, train_target.shape)print(test_input.shape, test_target.shape) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) image visualization 123456import matplotlib.pyplot as pltfig, axs = plt.subplots(1, 10, figsize=(10, 10))for i in range(10): axs[i].imshow(train_input[i], cmap=&quot;gray_r&quot;) axs[i].axis(&#x27;off&#x27;)plt.show() list of target values 1print([train_target[i] for i in range(10)]) [9, 0, 0, 3, 0, 2, 7, 2, 5, 5] real target values 6,000 images per label. 12import numpy as npprint(np.unique(train_target, return_counts=True)) (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) Classify by Logistic Regression123train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)print(train_scaled.shape) (60000, 784) 1234567from sklearn.model_selection import cross_validatefrom sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss=&#x27;log&#x27;, max_iter=10, random_state=42)scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)print(np.mean(scores[&#x27;test_score&#x27;])) # 0.82 0.8243124999999999 Is it reasonable to apply a linear or nonlinear model to unstructured data? : No One alternative is artificial neural networks. Is it reasonable to apply artificial neural networks and deep learning models to structured data? : No Classify by Artificial Neural Network1234from sklearn.model_selection import train_test_splittrain_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) 12print(train_scaled.shape, train_target.shape)print(val_scaled.shape, val_target.shape) (48000, 784) (48000,) (12000, 784) (12000,) A dense connection is called a fully connected layer. Specify activation functions to be applied to neuronal output binary classification : Sigmoid function multi classification : Softmax function specifying the type of loss function binary classification : binary_crossentropy multi classification : catogorical_crossentropy The integer target value should be one-hot encoded as 0, 1, 2, etc.but it can distort the operation of the artificial neural network. In tensorflow, by using sparse_categorical_crossentropy as a loss function, an integer target value can be used as it is. 123dense = keras.layers.Dense(10, activation = &quot;softmax&quot;, input_shape=(784, ))model = keras.Sequential(dense)model.compile(loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=&#x27;accuracy&#x27;) 1print(train_target[:10]) [7 3 5 8 6 9 3 3 9 9] 1model.fit(train_scaled, train_target, epochs=5) Epoch 1/5 1500/1500 [==============================] - 4s 2ms/step - loss: 0.6125 - accuracy: 0.7900 Epoch 2/5 1500/1500 [==============================] - 2s 2ms/step - loss: 0.4797 - accuracy: 0.8402 Epoch 3/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4562 - accuracy: 0.8479 Epoch 4/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4457 - accuracy: 0.8524 Epoch 5/5 1500/1500 [==============================] - 3s 2ms/step - loss: 0.4365 - accuracy: 0.8549 &lt;keras.callbacks.History at 0x7efd2ea9ded0&gt; 1model.evaluate(val_scaled, val_target) 375/375 [==============================] - 1s 1ms/step - loss: 0.4553 - accuracy: 0.8475 [0.45534512400627136, 0.8475000262260437] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 6_3","slug":"Python/ML/ML_ch_6_3","date":"2022-03-31T08:05:57.000Z","updated":"2022-10-05T05:39:53.364Z","comments":true,"path":"2022/03/31/Python/ML/ML_ch_6_3/","link":"","permalink":"http://gonekng.github.io/2022/03/31/Python/ML/ML_ch_6_3/","excerpt":"","text":"Dimensionaliy Reduction: Decreasing the size of the data by selecting some features that best represent the data To prevent overfitting and improve model performance PCA(Principal Component Analysis), LDA(Linear Discriminant Analysis), etc PCA principal component(PC) axis of data with the highest variance when projected on an axis expressed as a linear combination of existing variables Generally, it can be found as many as the features of the data as possible. To explain the overall variation with 2 to 3 principal component Import Data1234!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load(&#x27;fruits_300.npy&#x27;)fruits_2d = fruits.reshape(-1, 100*100) --2022-03-31 06:10:20-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-03-31 06:10:20-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 192.30.255.112 Connecting to github.com (github.com)|192.30.255.112|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-03-31 06:10:20-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.04s 2022-03-31 06:10:21 (79.3 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] PCA Model1234from sklearn.decomposition import PCApca = PCA(n_components=50)pca.fit(fruits_2d)print(pca.components_.shape) (50, 10000) 1234567891011121314import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # the number of sample rows = int(np.ceil(n/10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap=&#x27;gray_r&#x27;) axs[i, j].axis(&#x27;off&#x27;) plt.show() 1draw_fruits(pca.components_.reshape(-1, 100, 100)) 123print(fruits_2d.shape) # 10000 featuresfruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape) # 50 features (300, 10000) (300, 50) reduced to 1&#x2F;200 compared to the original size of the data Reconstruction of original data1234fruits_inverse = pca.inverse_transform(fruits_pca)print(fruits_inverse.shape)fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)print(fruits_reconstruct.shape) (300, 10000) (300, 100, 100) 1draw_fruits(fruits_reconstruct[0:100]) 1draw_fruits(fruits_reconstruct[100:200]) 1draw_fruits(fruits_reconstruct[200:300]) Even though 10,000 features were reduced to 50, the original data were preserved fairly well. Explained Variance: How well the principal component represents the variance of the original data.1print(np.cumsum(pca.explained_variance_ratio_)) 0.9215624972723878 [0.42357017 0.52298772 0.58876636 0.62907807 0.66324682 0.69606011 0.72179277 0.7423424 0.75606517 0.76949289 0.78101436 0.79046031 0.79924263 0.8077096 0.8146401 0.82109198 0.82688094 0.83199296 0.83685678 0.84166025 0.8461386 0.85051178 0.85459218 0.85848695 0.86221133 0.86580421 0.86911888 0.87229685 0.87534014 0.87837793 0.8812672 0.88402533 0.88667509 0.88923363 0.89175254 0.8942257 0.89662179 0.89893062 0.90115012 0.90331513 0.90544476 0.90740924 0.90933715 0.91123892 0.91308592 0.91491101 0.91664894 0.91833369 0.91995394 0.9215625 ] 123fig, ax = plt.subplots()ax.plot(pca.explained_variance_ratio_)plt.show() The first 10 PC represent most variance of the data. Subsequent PC could hardly explain the variance of the data. Use with other algorithms. Logistic Regression of 3 classes 1234from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()target = np.array([0]*100 + [1]*100 + [2]*100) # create target values cross-validation with original data 1234from sklearn.model_selection import cross_validatescores = cross_validate(lr, fruits_2d, target)print(np.mean(scores[&#x27;test_score&#x27;]))print(np.mean(scores[&#x27;fit_time&#x27;])) 0.9966666666666667 1.511155652999878 cross-validation with reduced data in PCA 123scores = cross_validate(lr, fruits_pca, target)print(np.mean(scores[&#x27;test_score&#x27;]))print(np.mean(scores[&#x27;fit_time&#x27;])) 1.0 0.07492985725402831 Specify the variance ratio123pca = PCA(n_components=0.5)pca.fit(fruits_2d)print(pca.n_components_) # 2 PC needed 2 12fruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape) (300, 2) 123scores = cross_validate(lr, fruits_pca, target)print(np.mean(scores[&#x27;test_score&#x27;]))print(np.mean(scores[&#x27;fit_time&#x27;])) 0.9933333333333334 0.03829236030578613 /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, 12345from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruits_pca)print(np.unique(km.labels_, return_counts=True))# label 0: 110 / label 1: 99 / label 2: 91 (array([0, 1, 2], dtype=int32), array([110, 99, 91])) 1draw_fruits(fruits[km.labels_==0]) 1draw_fruits(fruits[km.labels_==1]) 1draw_fruits(fruits[km.labels_==2]) 123456fig, ax = plt.subplots()for label in range(3): data = fruits_pca[km.labels_==label] ax.scatter(data[:,0], data[:,1])ax.legend([&#x27;apple&#x27;,&#x27;banana&#x27;,&#x27;pineapple&#x27;])plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 6_2","slug":"Python/ML/ML_ch_6_2","date":"2022-03-31T08:04:50.000Z","updated":"2022-10-05T05:39:53.285Z","comments":true,"path":"2022/03/31/Python/ML/ML_ch_6_2/","link":"","permalink":"http://gonekng.github.io/2022/03/31/Python/ML/ML_ch_6_2/","excerpt":"","text":"K-means Clustering Find mean of pixel value : cluster center, centroid Determine the centers of k clusters at random. Find the nearest cluster center from each sample and designate it as a sample of that cluster. Change the center of the cluster to the average value of the samples belonging to the cluster. Repeat 2~3 until there is no change in the center of the cluster. Import Data1!wget https://bit.ly/fruits_300_data -O fruits_300.npy --2022-03-31 02:09:21-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-03-31 02:09:21-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.114.3 Connecting to github.com (github.com)|140.82.114.3|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-03-31 02:09:22-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.01s 2022-03-31 02:09:22 (223 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] 1234import numpy as npfruits = np.load(&#x27;fruits_300.npy&#x27;)print(fruits.shape) (300, 100, 100) 12fruits_2d = fruits.reshape(-1, 100*100)print(fruits_2d.shape) (300, 10000) KMeans Class123from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruits_2d) # no target KMeans(n_clusters=3, random_state=42) 1print(km.labels_) # labels : [0, 1, 2] [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 12print(np.unique(km.labels_, return_counts=True))# label 0: 111 samples / label 1: 98 samples / label 2: 91 samples (array([0, 1, 2], dtype=int32), array([111, 98, 91])) Images of each label1234567891011121314import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # the number of sample rows = int(np.ceil(n/10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap=&#x27;gray_r&#x27;) axs[i, j].axis(&#x27;off&#x27;) plt.show() 1draw_fruits(fruits[km.labels_==0]) 1draw_fruits(fruits[km.labels_==1]) 1draw_fruits(fruits[km.labels_==2]) label 0: mostly pineapples label 1: mostly bananas label 2: mostly apples Centroid1print(km.cluster_centers_.shape) (6, 10000) (6, 100, 100) 1draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) 1print(km.transform(fruits_2d[100:101])) # two-dimension array input required [[3393.8136117 8837.37750892 5267.70439881]] 1print(km.predict(fruits_2d[100:101])) [0] 1draw_fruits(fruits[100:101]) Finding the best K (Elbow method) inertia : sum of squares of the distance between centroid and each sample As K increases, inertia decreases. Set the optimal K at the point where the inertia graph is bent. 1234567891011121314151617181920inertia = []for k in range(2,7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)slope = []lst = []for idx, val in enumerate(inertia): if idx==0: slope.append(0) lst.append(0) else: slope.append(val - inertia[idx-1]) lst.append(slope[idx-1]-slope[idx])fig, ax = plt.subplots()ax.plot(range(2,7), inertia)ax.scatter(2+np.argmax(lst), inertia[np.argmax(lst)], marker=&quot;o&quot;, color=&quot;red&quot;)plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 6_1","slug":"Python/ML/ML_ch_6_1","date":"2022-03-31T08:03:00.000Z","updated":"2022-10-05T05:39:53.204Z","comments":true,"path":"2022/03/31/Python/ML/ML_ch_6_1/","link":"","permalink":"http://gonekng.github.io/2022/03/31/Python/ML/ML_ch_6_1/","excerpt":"","text":"Unsupervised Learning No dependent variables and targets. (↔ Supervised Learning) Clustering (Multiple class) Must be many different types of data Linked to deep learning Dimensionality reduction Import Numpy Data1!wget https://bit.ly/fruits_300_data -O fruits_300.npy --2022-03-31 01:12:51-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10 Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-03-31 01:12:51-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.113.4 Connecting to github.com (github.com)|140.82.113.4|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-03-31 01:12:51-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.02s 2022-03-31 01:12:51 (157 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] 12345import numpy as np# 100 apples, 100 pineapples, 100 bananasfruits = np.load(&#x27;fruits_300.npy&#x27;)print(fruits.shape) (300, 100, 100) image samples of three dimensions dimension 1: the number of samples dimension 2: the height of image dimension 3: the width of image 300 pieces of image sample of 100 x 100 size. Visualize Image Data black-and-white photographs integer value from 0 to 255 1234import matplotlib.pyplot as pltplt.imshow(fruits[0], cmap=&#x27;gray&#x27;) # 0: black, 255: whiteplt.show() 12plt.imshow(fruits[0], cmap=&#x27;gray_r&#x27;) # 0: white, 255: blackplt.show() multiple images 1234fig, ax = plt.subplots(1,2)ax[0].imshow(fruits[100], cmap=&#x27;gray_r&#x27;)ax[1].imshow(fruits[200], cmap=&#x27;gray_r&#x27;)plt.show() Pixel value analysis12345# convert 100*100 images to one-dimensional array with a length of 10000apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100)print(apple.shape, pineapple.shape, banana.shape) (100, 10000) (100, 10000) (100, 10000) average comparison of pixel values for each image 12345plt.hist(np.mean(apple, axis=1), alpha=0.8)plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend([&#x27;apple&#x27;,&#x27;pineapple&#x27;,&#x27;banana&#x27;])plt.show() 12345fig, ax = plt.subplots(1,3,figsize=(15,5))ax[0].bar(range(10000),np.mean(apple, axis=0))ax[1].bar(range(10000),np.mean(pineapple, axis=0))ax[2].bar(range(10000),np.mean(banana, axis=0))plt.show() representative image using pixel mean 123456789apple_mean = np.mean(apple, axis=0).reshape(100,100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100,100)banana_mean = np.mean(banana, axis=0).reshape(100,100)fig, ax = plt.subplots(1,3,figsize=(15,5))ax[0].imshow(apple_mean, cmap=&#x27;gray_r&#x27;)ax[1].imshow(pineapple_mean, cmap=&#x27;gray_r&#x27;)ax[2].imshow(banana_mean, cmap=&#x27;gray_r&#x27;)plt.show() 100 images close to the average value 1234# MAE(Mean Absolute Error)abs_diff = np.abs(fruits - apple_mean)abs_mean = np.mean(abs_diff, axis=(1,2))print(abs_mean.shape) # one-dimensions array (300,) 1234567apple_index = np.argsort(abs_mean)[:100] # extract 100 indexes in the smallest order of MAEfig, ax = plt.subplots(10,10,figsize=(10,10))for i in range(10): for j in range(10): ax[i,j].imshow(fruits[apple_index[i*10+j]], cmap=&#x27;gray_r&#x27;) ax[i,j].axis(&#x27;off&#x27;) # remove axisplt.show() 33 48 70 57 87 12 78 59 1 74 86 38 50 92 69 27 68 30 66 24 76 98 15 84 47 90 3 94 53 23 14 71 32 7 73 36 55 77 21 10 17 39 99 95 11 35 65 6 61 22 56 89 2 13 80 0 97 4 58 34 40 43 75 82 54 16 31 49 93 37 63 64 41 28 67 25 96 8 83 46 19 79 72 5 85 29 20 60 81 9 45 51 88 62 91 26 52 18 44 42 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice Tree plot Example","slug":"Python/ML/plot_tree_ex","date":"2022-03-31T03:40:40.000Z","updated":"2022-10-05T05:39:54.111Z","comments":true,"path":"2022/03/31/Python/ML/plot_tree_ex/","link":"","permalink":"http://gonekng.github.io/2022/03/31/Python/ML/plot_tree_ex/","excerpt":"","text":"Goal : To change the color of tree plot1!pip install -U matplotlib Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.3) Collecting matplotlib Downloading matplotlib-3.5.1-cp39-cp39-win_amd64.whl (7.2 MB) Requirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1) Requirement already satisfied: numpy&gt;=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3) Requirement already satisfied: pillow&gt;=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0) Requirement already satisfied: cycler&gt;=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0) Requirement already satisfied: fonttools&gt;=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0) Requirement already satisfied: pyparsing&gt;=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4) Requirement already satisfied: python-dateutil&gt;=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2) Requirement already satisfied: packaging&gt;=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (21.0) Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler&gt;=0.10-&gt;matplotlib) (1.16.0) Installing collected packages: matplotlib Attempting uninstall: matplotlib Found existing installation: matplotlib 3.4.3 Uninstalling matplotlib-3.4.3: ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: &#39;c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\__pycache__\\\\pylab.cpython-39.pyc&#39; Consider using the `--user` option or check the permissions. Stackflow Ex.1234567891011121314151617181920212223242526from matplotlib import pyplot as pltfrom matplotlib.colors import ListedColormap, to_rgbimport numpy as npfrom sklearn import treeX = np.random.rand(50, 2) * np.r_[100, 50]y = X[:, 0] - X[:, 1] &gt; 20clf = tree.DecisionTreeClassifier(random_state=2021)clf = clf.fit(X, y)fig, ax = plt.subplots(figsize=(15, 10))colors = [&#x27;crimson&#x27;, &#x27;dodgerblue&#x27;]artists = tree.plot_tree(clf, feature_names=[&quot;X&quot;, &quot;y&quot;], class_names=colors, filled=True, rounded=True)for artist, impurity, value in zip(artists, clf.tree_.impurity, clf.tree_.value): # let the max value decide the color; whiten the color depending on impurity (gini) r, g, b = to_rgb(colors[np.argmax(value)]) f = impurity * 2 # for N colors: f = impurity * N/(N-1) if N&gt;1 else 0 artist.get_bbox_patch().set_facecolor((f + (1-f)*r, f + (1-f)*g, f + (1-f)*b)) artist.get_bbox_patch().set_edgecolor(&#x27;black&#x27;)plt.tight_layout()plt.show() Iris Ex.Tree plot1234567891011121314151617181920212223%matplotlib inline import sklearnprint(sklearn.__version__)import matplotlibprint(matplotlib.__version__)from sklearn.datasets import load_irisfrom sklearn import tree import matplotlib.pyplot as pltiris = load_iris()print(iris.data.shape, iris.target.shape)print(&quot;feature names&quot;, iris.feature_names)print(&quot;class names&quot;, iris.target_names)dt = tree.DecisionTreeClassifier(random_state=0)dt.fit(iris.data, iris.target)fig, ax = plt.subplots(figsize=(18, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)plt.show() 0.24.2 3.4.3 (150, 4) (150,) feature names [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] class names [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] matplotlib.text.Annotation123456789%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)for i in range(0, len(ax)): print(type(ax[i])) &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; &lt;class &#39;matplotlib.text.Annotation&#39;&gt; get_bbox_patch() method 123456789%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)for i in range(0, len(ax)): print(ax[i].get_bbox_patch()) # get patch properties (facecolor, edgewidth,,,) FancyBboxPatch((0, 0), width=120.875, height=56.4) FancyBboxPatch((0, 0), width=87.875, height=44.8) FancyBboxPatch((0, 0), width=127.25, height=56.4) FancyBboxPatch((0, 0), width=131.625, height=56.4) FancyBboxPatch((0, 0), width=30, height=33.2) FancyBboxPatch((0, 0), width=30, height=33.2) FancyBboxPatch((0, 0), width=131.625, height=56.4) FancyBboxPatch((0, 0), width=30, height=33.2) FancyBboxPatch((0, 0), width=30, height=33.2) set_boxstyle() 12345678910111213%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 2, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)for i in range(0, len(ax)): # set patch properties if i % 2 == 0: ax[i].get_bbox_patch().set_boxstyle(&quot;Rarrow&quot;, pad=0.3) else: ax[i].get_bbox_patch().set_boxstyle(&quot;Round&quot;, pad=0.3) Final ex.1234567import numpy as np colors = [&quot;indigo&quot;, &quot;violet&quot;, &quot;crimson&quot;]print(colors[np.argmax([[0., 0., 50.]])])print(colors[np.argmax([[50., 0., 0.]])])print(colors[np.argmax([[0., 50., 0.]])])print(colors[np.argmax([[50., 50., 50.]])]) crimson indigo violet indigo 12345678910111213141516171819202122232425from matplotlib.colors import to_rgb%matplotlib inlinefig, ax = plt.subplots(figsize=(15, 10))ax = tree.plot_tree(dt, max_depth = 3, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)i = 0colors = [&quot;yellow&quot;, &quot;violet&quot;, &quot;lavenderblush&quot;]for artist, impurity, value in zip(ax, dt.tree_.impurity, dt.tree_.value): r, g, b = to_rgb(colors[np.argmax(value)]) # 코드가 길어서 i로 재 저장 ip = impurity # print(ip + (1-ip)*r, ip + (1-ip)*g, ip + (1-ip)*b) if i % 2 == 0: # set_boxtyle 적용 ax[i].get_bbox_patch().set_boxstyle(&quot;round&quot;, pad=0.3) ax[i].get_bbox_patch().set_facecolor((ip + (1-ip)*r, ip + (1-ip)*g, ip + (1-ip)*b)) ax[i].get_bbox_patch().set_edgecolor(&#x27;black&#x27;) else: ax[i].get_bbox_patch().set_boxstyle(&quot;circle&quot;, pad=0.3) ax[i].get_bbox_patch().set_facecolor((ip + (1-ip)*r, ip + (1-ip)*g, ip + (1-ip)*b)) ax[i].get_bbox_patch().set_edgecolor(&#x27;black&#x27;) i = i+1 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 5_3","slug":"Python/ML/ML_ch_5_3","date":"2022-03-30T07:48:30.000Z","updated":"2022-10-05T05:39:53.131Z","comments":true,"path":"2022/03/30/Python/ML/ML_ch_5_3/","link":"","permalink":"http://gonekng.github.io/2022/03/30/Python/ML/ML_ch_5_3/","excerpt":"","text":"Ensemble algorithm that performs best in dealing with structured data Bagging : A method of aggregating results by taking multiple bootstrap samples and training each model. (parallel learning) Random Forest Boosting : (sequential learning) GBM –&gt; XGBoost –&gt; LightGBM Random Forest Create decision trees randomly and make final predictions based on each tree’s predictions. Classification : Average the probabilities for each class of each tree and uses the class with the highest probability as a prediction. Regression : Average the predictions of each tree. Bootstrap : method of sampling data by permitting duplication in a dataset 1234567891011import numpy as npimport pandas as pdwine = pd.read_csv(&#x27;https://bit.ly/wine_csv_data&#x27;)data = wine[[&#x27;alcohol&#x27;, &#x27;sugar&#x27;, &#x27;pH&#x27;]].to_numpy()target = wine[&#x27;class&#x27;].to_numpy()from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42) 12345from sklearn.model_selection import cross_validatefrom sklearn.ensemble import RandomForestClassifierrf = RandomForestClassifier(n_jobs=-1, random_state=42)scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores[&#x27;train_score&#x27;]), np.mean(scores[&#x27;test_score&#x27;])) # overfitting 0.9973541965122431 0.8905151032797809 12rf.fit(train_input, train_target)print(rf.feature_importances_) [0.23167441 0.50039841 0.26792718] OOB(out of bag) Sample : remaining sample not included in bootstrap sample same effect as cross-validation using a verification set 123rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)rf.fit(train_input, train_target)print(rf.oob_score_) 0.8934000384837406 GBM(Gradient Boosting Machine) Correct errors in previous trees by using shallow trees. Adjust the speed (step width) through the learning rate parameter less likely to overfit but speed is slow 1234from sklearn.ensemble import GradientBoostingClassifiergb = GradientBoostingClassifier(random_state=42)scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores[&#x27;train_score&#x27;]), np.mean(scores[&#x27;test_score&#x27;])) # good fitting 0.8881086892152563 0.8720430147331015 1234# n_estimators = 500 (default 100), learning rate = 0.2 (default 0.1)gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)print(np.mean(scores[&#x27;train_score&#x27;]), np.mean(scores[&#x27;test_score&#x27;])) # good fitting 0.9464595437171814 0.8780082549788999 Overall Flow of ML Data preprocessing, EDA, Visualization Design the entire flow as a basic model compare multiple models with default hyperparameter Cross-validation and Hyperparameter tuning Repeat the above process until finding the best result Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 5_2","slug":"Python/ML/ML_ch_5_2","date":"2022-03-30T06:40:40.000Z","updated":"2022-10-05T05:39:53.059Z","comments":true,"path":"2022/03/30/Python/ML/ML_ch_5_2/","link":"","permalink":"http://gonekng.github.io/2022/03/30/Python/ML/ML_ch_5_2/","excerpt":"","text":"Cross Validation: Repeated process of spliting validation set and evaluating model. Train set : Validation set : Test set &#x3D; 6 : 2 : 2 (generally) Test sets are not used in the model learning process. In Kagge competition, test sets are given separately. purpose : To make a good model A good model doesn’t mean high-performance model. A good model means low-error and stable model. Because it takes a long time, it is useful when there is not much data. Prepare data1234import pandas as pdwine = pd.read_csv(&#x27;https://bit.ly/wine_csv_data&#x27;)data = wine[[&#x27;alcohol&#x27;,&#x27;sugar&#x27;,&#x27;pH&#x27;]].to_numpy()target = wine[[&#x27;class&#x27;]].to_numpy() 1234567from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42)sub_input, val_input, sub_target, val_target = train_test_split( train_input, train_target, test_size=0.2, random_state=42) 1sub_input.shape, val_input.shape, test_input.shape ((4157, 3), (1040, 3), (1300, 3)) Create model12345from sklearn.tree import DecisionTreeClassifierdt = DecisionTreeClassifier(random_state=42)dt.fit(sub_input, sub_target)print(dt.score(sub_input, sub_target))print(dt.score(val_input, val_target)) # overfitting 0.9971133028626413 0.864423076923077 Validate model1234from sklearn.model_selection import cross_validatescores = cross_validate(dt, train_input, train_target) # dictionary typefor item in scores.items(): print(item) (&#39;fit_time&#39;, array([0.01251197, 0.00755358, 0.0074594 , 0.00742102, 0.00734329])) (&#39;score_time&#39;, array([0.00133634, 0.00079608, 0.0007925 , 0.00083232, 0.00076413])) (&#39;test_score&#39;, array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])) 12import numpy as npprint(np.mean(scores[&#x27;test_score&#x27;])) 0.855300214703487 In cross-validation, a splitter must be specified to mix training sets. Regression model &gt; KFold Classification model &gt; StratifiedKFold 1234from sklearn.model_selection import StratifiedKFoldsplitter = StratifiedKFold(shuffle=True, random_state=42) # default : 5 foldscores = cross_validate(dt, train_input, train_target, cv=splitter)print(np.mean(scores[&#x27;test_score&#x27;])) 0.8539548012141852 123splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # 10 foldscores = cross_validate(dt, train_input, train_target, cv=splitter)print(np.mean(scores[&#x27;test_score&#x27;])) 0.8574181117533719 Hyperparameter Tuning ex) max_depth&#x3D;3, accuracy&#x3D;0.84 Finding the best value by adjusting multiple parameters simultaneously. AutoML : technology that automatically performs hyperparameter tuning without intervention of person. Grid Search, Random Search Grid Search Perform hyperparameter tuning and cross-validation simultaneously Find the optimal hyperparameters based on all combinations of predetermined values. 12345678%%timefrom sklearn.model_selection import GridSearchCVparams = &#123; &#x27;min_impurity_decrease&#x27; : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]&#125;gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)gs.fit(train_input, train_target) CPU times: user 70.1 ms, sys: 6.06 ms, total: 76.1 ms Wall time: 183 ms 123dt = gs.best_estimator_print(dt)print(dt.score(train_input, train_target)) DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42) 0.9615162593804117 12print(gs.cv_results_[&#x27;mean_test_score&#x27;])print(gs.best_params_) [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605] &#123;&#39;min_impurity_decrease&#39;: 0.0001&#125; 123456789101112%%timefrom sklearn.model_selection import GridSearchCVparams = &#123; &#x27;min_impurity_decrease&#x27; : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005], &#x27;max_depth&#x27; : [3, 4, 5, 6, 7]&#125;# Change the values in params and create a total of 5 models with each value.# n_jobs=-1 : to enable all cores in the systemgs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)gs.fit(train_input, train_target) CPU times: user 167 ms, sys: 4.85 ms, total: 172 ms Wall time: 585 ms 123dt = gs.best_estimator_print(dt)print(dt.score(train_input, train_target)) DecisionTreeClassifier(max_depth=7, min_impurity_decrease=0.0005, random_state=42) 0.8830094285164518 12print(gs.cv_results_[&#x27;mean_test_score&#x27;]) # 5*5=25print(gs.best_params_) [0.84125583 0.84125583 0.84125583 0.84125583 0.84125583 0.85337806 0.85337806 0.85337806 0.85337806 0.85318557 0.85780355 0.85799604 0.85857352 0.85857352 0.85838102 0.85645721 0.85799678 0.85876675 0.85972866 0.86088306 0.85607093 0.85761031 0.85799511 0.85991893 0.86280466] &#123;&#39;max_depth&#39;: 7, &#39;min_impurity_decrease&#39;: 0.0005&#125; The optimal value of ‘min_impurity_decrease’ varies when the value of ‘max_depth’ changes. Random Search Find the optimal hyperparameters based on possible combinations within a predetermined range of values. Delivers probability distribution objects that can sample parameters. 1234567# randint : sampling int# uniform : sampling floatfrom scipy.stats import uniform, randintparams = &#123; &#x27;min_impurity_decrease&#x27; : uniform(0.0001, 0.001), &#x27;max_depth&#x27; : randint(20, 50)&#125; 123456%%timefrom sklearn.model_selection import RandomizedSearchCVgs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)gs.fit(train_input, train_target) CPU times: user 629 ms, sys: 15.8 ms, total: 645 ms Wall time: 2.54 s 123dt = gs.best_estimator_print(dt)print(dt.score(train_input, train_target)) DecisionTreeClassifier(max_depth=29, min_impurity_decrease=0.000437615171403628, random_state=42) 0.8903213392341736 1print(gs.best_params_) &#123;&#39;max_depth&#39;: 29, &#39;min_impurity_decrease&#39;: 0.000437615171403628&#125; Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 5_1","slug":"Python/ML/ML_ch_5_1","date":"2022-03-30T06:10:20.000Z","updated":"2022-10-05T05:39:52.983Z","comments":true,"path":"2022/03/30/Python/ML/ML_ch_5_1/","link":"","permalink":"http://gonekng.github.io/2022/03/30/Python/ML/ML_ch_5_1/","excerpt":"","text":"Prepare Data Import wine data set class 0: red wine class 1: white wine 123import pandas as pdwine = pd.read_csv(&quot;https://bit.ly/wine_csv_data&quot;)print(wine.head()) alcohol sugar pH class 0 9.4 1.9 3.51 0.0 1 9.8 2.6 3.20 0.0 2 9.8 2.3 3.26 0.0 3 9.8 1.9 3.16 0.0 4 9.4 1.9 3.51 0.0 12# checking missing value and types of variablewine.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB 1print(wine.describe()) # vary in scale of variables, in need of standardization alcohol sugar pH class count 6497.000000 6497.000000 6497.000000 6497.000000 mean 10.491801 5.443235 3.218501 0.753886 std 1.192712 4.757804 0.160787 0.430779 min 8.000000 0.600000 2.720000 0.000000 25% 9.500000 1.800000 3.110000 1.000000 50% 10.300000 3.000000 3.210000 1.000000 75% 11.300000 8.100000 3.320000 1.000000 max 14.900000 65.800000 4.010000 1.000000 Split data into training sets and test sets 12data = wine[[&#x27;alcohol&#x27;, &#x27;sugar&#x27;, &#x27;pH&#x27;]].to_numpy()target = wine[&#x27;class&#x27;].to_numpy() 12345from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( data, target, test_size=0.2, random_state=42)print(train_input.shape, test_input.shape) (5197, 3) (1300, 3) Standardize Data Decision tree does not require standardized preprocessing, but it is recommended to perform standardization basically. 12345from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) Logistic Regression Model12345from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target)) 0.7808350971714451 0.7776923076923077 12print(lr.predict_proba(train_scaled[:5]))print(lr.predict(train_scaled[:5])) [[0.06189333 0.93810667] [0.21742616 0.78257384] [0.40703571 0.59296429] [0.45226659 0.54773341] [0.00530794 0.99469206]] [1. 1. 1. 1. 1.] 1print(lr.coef_, lr.intercept_) [[ 0.51270274 1.6733911 -0.68767781]] [1.81777902] Decision Tree Model: a non-parametric supervised learning method used for classification and regression to predict the value of a target variable by learning simple decision rules inferred from the data features Simple to understand and to interpret More likely to be overfitting the training set. New Algorithm Using Decision Tree Algorithms XGBoost, LightGBM, CatBoost, etc In particular, LightGBM is now widely used in practice. DecisionTreeClassifier12345from sklearn.tree import DecisionTreeClassifierdt = DecisionTreeClassifier(random_state=42)dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled, test_target)) # appear to be overfitting 0.996921300750433 0.8592307692307692 12345678910import matplotlib.pyplot as pltfrom sklearn.tree import plot_treefig, ax = plt.subplots(figsize=(18,10))plot_tree(dt, filled=True, feature_names=[&#x27;alcohol&#x27;,&#x27;sugar&#x27;,&#x27;pH&#x27;])plt.show()# - conditions for testing : sugar# - impurity : gini# - samples : total number of samples# - value : number of samples by class Impurity(불순도) parameter criterion; default ‘gini’ gini &#x3D; 1 - (negative_prop^2 + positive_prop^2) best : 0 (pure node) worst : 0.5 (exactly half and half) entropy &#x3D; - negative_prop * log_2(negative_prop) - positive_prop * log_2(positive_prop) Information gain(정보 이득) : impurity differences between parent node and child node Decision tree splits nodes to maximize information gain using impurity criteria. Pruning(가지치기) In order to prevent overfitting the training set By specifying the maximum depth of a tree that can grow 1234dt = DecisionTreeClassifier(max_depth=3, random_state=42)dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled, test_target)) # successful in reducing overfitting 0.8454877814123533 0.8415384615384616 123fig, ax = plt.subplots(figsize=(18,10))plot_tree(dt, filled=True, feature_names=[&#x27;alcohol&#x27;,&#x27;sugar&#x27;,&#x27;pH&#x27;])plt.show() 123456789# Tree Plot Image Downloadimport graphvizfrom sklearn import treedot_data = tree.export_graphviz( dt, out_file=None, feature_names = [&#x27;alcohol&#x27;,&#x27;sugar&#x27;,&#x27;pH&#x27;], filled=True)graph = graphviz.Source(dot_data, format=&quot;png&quot;) graph 1graph.render(&quot;decision_tree_graphivz&quot;) &#39;decision_tree_graphivz.png&#39; 12345678910111213141516# Customize color of nodesfrom matplotlib.colors import ListedColormap, to_rgbimport numpy as npplt.figure(figsize=(20, 15))artists = plot_tree(dt, filled = True, feature_names = [&#x27;alcohol&#x27;,&#x27;sugar&#x27;,&#x27;pH&#x27;])colors = [&#x27;blue&#x27;, &#x27;red&#x27;]for artist, impurity, value in zip(artists, dt.tree_.impurity, dt.tree_.value): r, g, b = to_rgb(colors[np.argmax(value)]) f = impurity * 2 artist.get_bbox_patch().set_facecolor((f + (1-f)*r, f + (1-f)*g, f + (1-f)*b)) artist.get_bbox_patch().set_edgecolor(&#x27;black&#x27;)plt.show() parameter min_impurity_decrease; default 0.0 Split nodes if this split induces a decrease of the impurity greater than or equal to this value. More likely to be asymmetric tree 12345678dt = DecisionTreeClassifier(min_impurity_decrease=0.0005, random_state=42)dt.fit(train_scaled, train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled, test_target))fig, ax = plt.subplots(figsize=(18, 10))plot_tree(dt, filled=True, feature_names=[&#x27;alcohol&#x27;,&#x27;sugar&#x27;,&#x27;pH&#x27;])plt.show() 0.8874350586877044 0.8615384615384616 Feature importance: an indicator of the degree to which each feature contributed to reducing impurities Multiply the information gain and the ratio of the total sample by each node, and add it up by feature. 1print(dt.feature_importances_) # Sugar is the most important feature. [0.12345626 0.86862934 0.0079144 ] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 4_2","slug":"Python/ML/ML_ch_4_2","date":"2022-03-29T07:10:25.000Z","updated":"2022-10-05T05:39:52.900Z","comments":true,"path":"2022/03/29/Python/ML/ML_ch_4_2/","link":"","permalink":"http://gonekng.github.io/2022/03/29/Python/ML/ML_ch_4_2/","excerpt":"","text":"Gradient Descent(경사 하강법): Algorithm for finding the minimum value of a loss function using a sample of a training set stochastic gradient descent(확률적 경사 하강법; SGD) method of randomly selecting one sample from a training set minibatch gradient descent(미니배치 경사 하강법) method of randomly selecting several samples from a training set batch gradient descent(배치 경사 하강법) method of selecting all the samples from a training set at once Sampling method is different from the existing model. (more detailed approach) It aims to correct errors by reducing the slope of the loss function SGDClassifier : Create a classification model using SGD. SGDRegressor : Create a regression model using SGD. Epoch : process of using the entire training set once Usage Deep learning algorithm (especially, image and text) Tree algorithm + Gradient Descent &#x3D; Boosting ex) LightGBM, Xgboost, Catboost Loss function(손실 함수) Cost function 비용 함수) Loss is the difference between the predicted value and the actual value of the model (equivalent to error) Loss function is a function that expresses loss of the model an indicator of how poorly a model processes data Loss function must be differentiable. Prepare Data12345# Importimport pandas as pdfish = pd.read_csv(&quot;https://bit.ly/fish_csv_data&quot;)fish_input = fish[[&#x27;Weight&#x27;, &#x27;Length&#x27;, &#x27;Diagonal&#x27;, &#x27;Height&#x27;, &#x27;Width&#x27;]].to_numpy()fish_target = fish[&#x27;Species&#x27;].to_numpy() 123456# Splitfrom sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((119, 5), (40, 5), (119,), (40,)) 1234567# Normalizefrom sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) ※ To prevent data leakage, make sure to convert the test set to the statistics learned from the training set.※ Data leakage : containing the information you want to predict in the data used for model training SGD ClassifierFitting model set 2 parameter in SGD Classifier loss : specifying the type of loss function max_iter : specifying the number of epochs to be executed In the case of a multi-classification model, if loss is set as ‘log’, a binary classification model is created for each class. 123456from sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss=&#x27;log&#x27;, max_iter=10, random_state=42)sc.fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.773109243697479 0.775 /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, 1234# partial_fit() : continue training one epoch per callsc.partial_fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.8151260504201681 0.85 Finding appropriate epoch12345678910import numpy as npsc = SGDClassifier(loss=&#x27;log&#x27;, random_state=42)train_score = []test_score = []classes = np.unique(train_target)for _ in range(0, 300): # _ : temporal variable sc.partial_fit(train_scaled, train_target, classes=classes) train_score.append(sc.score(train_scaled, train_target)) test_score.append(sc.score(test_scaled, test_target)) 12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(train_score)ax.plot(test_score)ax.set_xlabel(&#x27;epoch&#x27;)ax.set_ylabel(&#x27;accuracy&#x27;)plt.show() In the early stages of epoch, the scores of training sets and test sets are low because they are underfitting. After epoch 100, the score difference between the training set and the test set gradually increases. Epoch 100 appears to be the most appropriate number of iterations. 123456# SGD classifier stops by itself, if performance does not improve during a certain epoch.# tol = None : to repeat unconditionally untill max_itersc = SGDClassifier(loss=&#x27;log&#x27;, max_iter=100, tol=None, random_state=42)sc.fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled, test_target)) 0.957983193277311 0.925 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 4_1","slug":"Python/ML/ML_ch_4_1","date":"2022-03-29T03:50:45.000Z","updated":"2022-10-05T05:39:52.815Z","comments":true,"path":"2022/03/29/Python/ML/ML_ch_4_1/","link":"","permalink":"http://gonekng.github.io/2022/03/29/Python/ML/ML_ch_4_1/","excerpt":"","text":"Prepare DataImport data set1234import pandas as pdfish = pd.read_csv(&#x27;https://bit.ly/fish_csv_data&#x27;)print(fish.head()) Species Weight Length Diagonal Height Width 0 Bream 242.0 25.4 30.0 11.5200 4.0200 1 Bream 290.0 26.3 31.2 12.4800 4.3056 2 Bream 340.0 26.5 31.1 12.3778 4.6961 3 Bream 363.0 29.0 33.5 12.7300 4.4555 4 Bream 430.0 29.0 34.0 12.4440 5.1340 1print(pd.unique(fish[&#x27;Species&#x27;])) [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] Convert to Numpy array123fish_input = fish[[&#x27;Weight&#x27;,&#x27;Length&#x27;,&#x27;Diagonal&#x27;,&#x27;Height&#x27;,&#x27;Width&#x27;]].to_numpy()print(fish_input.shape)print(fish_input[:3]) (159, 5) [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961]] 12fish_target = fish[&#x27;Species&#x27;].to_numpy()print(fish_target.shape) (159,) Split and Standardize1234from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state=42) 12345from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) 12print(train_input[:3])print(train_scaled[:3]) [[720. 35. 40.6 16.3618 6.09 ] [500. 45. 48. 6.96 4.896 ] [ 7.5 10.5 11.6 1.972 1.16 ]] [[ 0.91965782 0.60943175 0.81041221 1.85194896 1.00075672] [ 0.30041219 1.54653445 1.45316551 -0.46981663 0.27291745] [-1.0858536 -1.68646987 -1.70848587 -1.70159849 -2.0044758 ] [-0.79734143 -0.60880176 -0.67486907 -0.82480589 -0.27631471] [-0.71289885 -0.73062511 -0.70092664 -0.0802298 -0.7033869 ]] [[-0.88741352 -0.91804565 -1.03098914 -0.90464451 -0.80762518] [-1.06924656 -1.50842035 -1.54345461 -1.58849582 -1.93803151] [-0.54401367 0.35641402 0.30663259 -0.8135697 -0.65388895] [-0.34698097 -0.23396068 -0.22320459 -0.11905019 -0.12233464] [-0.68475132 -0.51509149 -0.58801052 -0.8998784 -0.50124996]] KNN ClassifierModel fitting1234567from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors=3)kn.fit(train_scaled, train_target)print(kn.score(train_scaled, train_target))print(kn.score(test_scaled, test_target)) 0.8907563025210085 0.85 Multi-class Classfication1234import numpy as npproba = kn.predict_proba(test_scaled[:5])print(kn.classes_)print(np.round(proba, decimals=4)) [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] 12distances, indexes = kn.kneighbors(test_scaled[3:4]) # Two-dimensional array must be inputprint(train_target[indexes]) [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] The probability calculated by the model is the ratio of the nearest neighbor. In this model(k&#x3D;3), the probability values are 0, 1&#x2F;3, 2&#x2F;3, and 1. If k is set as 5, the probability values may be 0, 0.2, 0.4, 0.6, 0.8 and 1. 123456kn = KNeighborsClassifier(n_neighbors=5)kn.fit(train_scaled, train_target)proba = kn.predict_proba(test_scaled[:5])print(kn.classes_)print(np.round(proba, decimals=4))print(kn.predict(test_scaled[:5])) [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] [[0. 0. 0.6 0. 0.4 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0.2 0.8 0. 0. 0. ] [0. 0. 0.8 0. 0.2 0. 0. ] [0. 0. 0.8 0. 0.2 0. 0. ]] [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] Logistic Regression: Estimating a model with a regression equation for categorical dependent variables. Despite its name, a classification model rather than regression model Highly important model used as basic statistics (especially medical statistics) the basis of the machine learning classification model. early model of deep learning To overcome the linearity assumption problem of general regression equation Logit transformation : the log of the odds ratio Using the logit of Y as the dependent variable of the regression Sigmoid function also called a logistic function Convert the value z calculated by linear regression to a probability value between 0 and 1 z &lt; 0: the function approaches 0 z &gt; 0: the function approaches 1 z &#x3D; 0: the function value is 0.5 1234567891011import numpy as npimport matplotlib.pyplot as pltz = np.arange(-5, 5, 0.1)phi = 1 / (1 + np.exp(-z)) # sigmoid functionfig, ax = plt.subplots()ax.plot(z, phi)ax.set_xlabel(&#x27;z&#x27;, fontsize=12)ax.set_ylabel(&#x27;phi&#x27;, fontsize=12)ax.set_title(&quot;Sigmoid Function for Logistic Regression&quot;, fontsize=15)plt.show() Binary classification12345# Boolean Indexing: using a boolean vector to filter the data. # Choose only Bream and Smelt from the training set.bream_smelt_indexes = (train_target == &#x27;Bream&#x27;) | (train_target == &#x27;Smelt&#x27;)train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes] 12345678from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt)print(lr.classes_) # 0: Bream / 1: Smeltproba = lr.predict_proba(train_bream_smelt[:5])print(np.round(proba, decimals=3)) # 5 rows, 2 columnsprint(lr.predict(train_bream_smelt[:5])) [&#39;Bream&#39; &#39;Smelt&#39;] [[0.998 0.002] [0.027 0.973] [0.995 0.005] [0.986 0.014] [0.998 0.002]] [&#39;Bream&#39; &#39;Smelt&#39; &#39;Bream&#39; &#39;Bream&#39; &#39;Bream&#39;] 1print(lr.coef_, lr.intercept_) [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132] z &#x3D; - 0.404 * Weight - 0. 576 * Length - 0.663 * Diagonal - 1.013 * Height - 0.732 * Width - 2.162 12decisions = lr.decision_function(train_bream_smelt[:5])print(decisions) # original z-value of positive class(Smelt) [-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] 12from scipy.special import expitprint(expit(decisions)) # probability value through sigmoid function [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] Multi-class classification basically use iterative algorithms (max_iter, default 100) in this model, set max_iter as 1000 (for sufficient training) L2 Regularization based on the square value of the coefficient such as ridge regression hyperparameter; C ( default 1) the smaller the value, the greater the regulation. in this model, set C as 20 (in order to ease regulations a little) 1234lr = LogisticRegression(C=20, max_iter=1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target)) 0.9327731092436975 0.925 1234print(lr.classes_)proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals=3)) # 5 rows, 7 columnsprint(lr.predict(test_scaled[:5])) [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;] 1print(lr.coef_.shape, lr.intercept_.shape) (7, 5) (7,) [[-1.49002087 -1.02912886 2.59345551 7.70357682 -1.2007011 ] [ 0.19618235 -2.01068181 -3.77976834 6.50491489 -1.99482722] [ 3.56279745 6.34357182 -8.48971143 -5.75757348 3.79307308] [-0.10458098 3.60319431 3.93067812 -3.61736674 -1.75069691] [-1.40061442 -6.07503434 5.25969314 -0.87220069 1.86043659] [-1.38526214 1.49214574 1.39226167 -5.67734118 -4.40097523] [ 0.62149861 -2.32406685 -0.90660867 1.71599038 3.6936908 ]] [-0.09205179 -0.26290885 3.25101327 -0.14742956 2.65498283 -6.78782948 1.38422358] The z value is calculated one by one for each class and classified into the class that outputs the highest value. Softmax function also called a normalized exponential function (because of using exponential functions) The outputs of several linear equations are compressed from 0 to 1, and the total sum is 1. e_sum &#x3D; e^z1 + e^z2 + … + e^z7 s1 &#x3D; e^z1&#x2F;e_sum, s2 &#x3D; e^z2&#x2F;e_sum, … , s7 &#x3D; e^z7&#x2F;e_sum 12decision = lr.decision_function(test_scaled[:5])print(np.round(decision, decimals=3)) # original z value [[ -6.498 1.032 5.164 -2.729 3.339 0.327 -0.634] [-10.859 1.927 4.771 -2.398 2.978 7.841 -4.26 ] [ -4.335 -6.233 3.174 6.487 2.358 2.421 -3.872] [ -0.683 0.453 2.647 -1.187 3.265 -5.753 1.259] [ -6.397 -1.993 5.816 -0.11 3.503 -0.112 -0.707]] 123from scipy.special import softmaxproba = softmax(decision, axis=1)print(np.round(proba, decimals=3)) # probability value through softmax function [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 3_3","slug":"Python/ML/ML_ch_3_3","date":"2022-03-28T15:34:50.000Z","updated":"2022-10-05T05:39:52.733Z","comments":true,"path":"2022/03/29/Python/ML/ML_ch_3_3/","link":"","permalink":"http://gonekng.github.io/2022/03/29/Python/ML/ML_ch_3_3/","excerpt":"","text":"Prepare Data123import pandas as pddf = pd.read_csv(&#x27;https://bit.ly/perch_csv_data&#x27;)perch_full = df.to_numpy() # Convert Pandas DataFrame to Numpy Array 12345678import numpy as npperch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) 1234from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_full, perch_weight, random_state=42) Transform Data※ Scikit-Learn Class Estimator(추정기; model class) : Fitting and predicting KNeighborsClassifier, LinearRegression, etc. common method : fit(), score(), predict() Transformer(변환기) and Pre-processors : transforming or imputing data PolynomialFeatures, StandardScaler, etc common method : fit(), transform() ※ Feature engineering(특성 공학) extracting new features using existing features existing features, square features of each, and features multiplied by each other. Import transformer1from sklearn.preprocessing import PolynomialFeatures Transform sample data case 1: Including a bias 123poly = PolynomialFeatures()poly.fit([[2,3]])print(poly.transform([[2,3]])) [[1. 2. 3. 4. 6. 9.]] &gt; existing features : 2, 3 &gt; new features : 1(for intercept), 4(2^2), 6(2*3), 9(3^2) case 2: Not including a bias (recommended) 123poly = PolynomialFeatures(include_bias=False)poly.fit([[2,3]])print(poly.transform([[2,3]])) [[2. 3. 4. 6. 9.]] &gt; existing features : 2, 3 &gt; new features : 4(2^2), 6(2*3), 9(3^2) Transform perch data1234567poly = PolynomialFeatures(include_bias=False)poly.fit(train_input)train_poly = poly.transform(train_input)print(train_input.shape) # have 3 featuresprint(train_poly.shape) # have 9 featuresprint(poly.get_feature_names_out()) (42, 3) (42, 9) [&#39;x0&#39; &#39;x1&#39; &#39;x2&#39; &#39;x0^2&#39; &#39;x0 x1&#39; &#39;x0 x2&#39; &#39;x1^2&#39; &#39;x1 x2&#39; &#39;x2^2&#39;] 123test_poly = poly.transform(test_input)print(test_input.shape) # have 3 featuresprint(test_poly.shape) # have 9 features (14, 3) (14, 9) Mutiple Regression same process as training a linear regression model linear regression using multiple features degree 212345from sklearn.linear_model import LinearRegressionlr = LinearRegression()lr.fit(train_poly, train_target)print(lr.score(train_poly, train_target))print(lr.score(test_poly, test_target)) 0.9903183436982124 0.9714559911594134 Multiple regression solves the linear model’s underfitting problem. The score for the training set is very high. degree 5123456poly = PolynomialFeatures(degree=5, include_bias=False)poly.fit(train_input)train_poly = poly.transform(train_input)test_poly = poly.transform(test_input)print(train_poly.shape)print(test_poly.shape) (42, 55) (14, 55) 123lr.fit(train_poly, train_target)print(lr.score(train_poly, train_target))print(lr.score(test_poly, test_target)) 0.9999999999991097 -144.40579242684848 The score for the training set is almost perfect. But the score for the testing set is extremely negative. The model appears to be too overfitting to the training set. Regularization(규제) preventing the model from overfitting the training set linear regression model : reducing the size of the coefficient multiplied by the feature. hyperparameter: alpha parameter which has to be set in advance increase&#x2F;decrease in regulatory intensity adjusted to increase the performance of the model Conceptual understanding is important, but it doesn’t mean much in practice. No guarantee of performance compared to working hours. More than 100 libraries in scikit-learn, and the types and numbers of hyperparameters vary. Better to use the existing hyperparameters, for unfamiliar models. Normalize feature scales using StandardScaler class in scikit-learn 1234567from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_poly)train_scaled = ss.transform(train_poly)test_scaled = ss.transform(test_poly)print(train_poly.shape)print(test_poly.shape) (42, 55) (14, 55) Ridge regression based on the square value of the coefficient 12345from sklearn.linear_model import Ridgeridge = Ridge()ridge.fit(train_scaled, train_target)print(ridge.score(train_scaled, train_target))print(ridge.score(test_scaled, test_target)) 0.9896101671037343 0.9790693977615397 Many features are used, but they’re not overfitting the training set and perform well on the test set. 123import matplotlib.pyplot as plttrain_score = []test_score = [] 123456alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]for alpha in alpha_list: ridge = Ridge(alpha=alpha) ridge.fit(train_scaled, train_target) train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) 123456fig, ax = plt.subplots()ax.plot(np.log10(alpha_list), train_score)ax.plot(np.log10(alpha_list), test_score)ax.set_xlabel(&#x27;log10(alpha)&#x27;)ax.set_ylabel(&#x27;R^2&#x27;)plt.show() left side : overfitting right side : underfitting appropriate alpha : 0.1 1234ridge = Ridge(alpha=0.1)ridge.fit(train_scaled, train_target)print(ridge.score(train_scaled, train_target))print(ridge.score(test_scaled, test_target)) 0.9903815817570366 0.9827976465386926 Lasso regression based on the absolute value of the coefficient The coefficient can be completely zero. 12345from sklearn.linear_model import Lassolasso = Lasso()lasso.fit(train_scaled, train_target)print(lasso.score(train_scaled, train_target))print(lasso.score(test_scaled, test_target)) 0.989789897208096 0.9800593698421883 Many features are used, but they’re not overfitting the training set and perform well on the test set. 123456789train_score = []test_score = []alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]for alpha in alpha_list: lasso = Lasso(alpha=alpha, max_iter=10000) lasso.fit(train_scaled, train_target) train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+04, tolerance: 5.183e+02 coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e+04, tolerance: 5.183e+02 coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive 123456fig, ax = plt.subplots()ax.plot(np.log10(alpha_list), train_score)ax.plot(np.log10(alpha_list), test_score)ax.set_xlabel(&#x27;log10(alpha)&#x27;)ax.set_ylabel(&#x27;R^2&#x27;)plt.show() left side : overfitting right side : underfitting appropriate alpha : 10 1234lasso = Lasso(alpha=10)lasso.fit(train_scaled, train_target)print(lasso.score(train_scaled, train_target))print(lasso.score(test_scaled, test_target)) 0.9888067471131867 0.9824470598706695 1print(np.sum(lasso.coef_==0)) 40 40 coefficients became zero Of the 55 features, only 15 were finally used. Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 3_2","slug":"Python/ML/ML_ch_3_2","date":"2022-03-28T08:34:00.000Z","updated":"2022-10-05T05:39:52.607Z","comments":true,"path":"2022/03/28/Python/ML/ML_ch_3_2/","link":"","permalink":"http://gonekng.github.io/2022/03/28/Python/ML/ML_ch_3_2/","excerpt":"","text":"Data Set12345678910111213141516171819import numpy as npperch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] ) 12345from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((42,), (14,), (42,), (14,)) 1234train_input = train_input.reshape(-1,1)test_input = test_input.reshape(-1,1)print(train_input.shape, test_input.shape) (42, 1) (14, 1) KNN Regression12345from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor(n_neighbors=3)knr.fit(train_input, train_target)knr.score(test_input, test_target) 0.9746459963987609 Predict a data 1 the weight of a 50-centimeter-long perch 1print(knr.predict([[50]])) [1033.33333333] 123456789import matplotlib.pyplot as pltdistances, indexes = knr.kneighbors([[50]])fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.scatter(train_input[indexes], train_target[indexes], marker=&quot;D&quot;) # 3 neighborsax.scatter(50, knr.predict([[50]]), marker=&#x27;^&#x27;) # new dataplt.show() Predict a data 2 the weight of a 100-centimeter-long perch 1print(knr.predict([[100]])) [1033.33333333] 1234567distances, indexes = knr.kneighbors([[100]])fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.scatter(train_input[indexes], train_target[indexes], marker=&quot;D&quot;) # 3 neighborsax.scatter(100, knr.predict([[100]]), marker=&#x27;^&#x27;) # new dataplt.show() Beyond the scope of the new training set, incorrect values can be predicted. No matter how big the length is, the weight doesn’t increase anymore. ※ Machine learning models must be trained periodically. MLOps (Machine Learning &amp; Opearations) the essential skill for data scientist, ML engineer. Linear Regression in statistics: The process of finding causal relationships is more important. 4 assumptions (linearity, normality, independence, equal variance) in ML: Predicting results is more important. R-squared, MAE, RMSE, etc Predict a data123456from sklearn.linear_model import LinearRegressionlr = LinearRegression()lr.fit(train_input, train_target)print(lr.predict([[50]])) [1241.83860323] 12345fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.scatter(train_input[indexes], train_target[indexes], marker=&quot;D&quot;) # 3 neighborsax.scatter(50, lr.predict([[50]]), marker=&#x27;^&#x27;) # new dataplt.show() Regression equation coef_ : regression coefficient(weight) intercept_ : regression intercept $y &#x3D; a + bx$ coefficient &amp; intercept : model parameter Linear Regression is a model-based learning. KNN Regression is a case-based learning. 1print(lr.coef_, lr.intercept_) [39.01714496] -709.0186449535477 123456789101112fig, ax = plt.subplots()# scatter plot of training setax.scatter(train_input, train_target)# linear equation from 0 to 50ax.plot([0,50], [0*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])ax.scatter(50, lr.predict([[50]]), marker=&quot;^&quot;)ax.set_label(&quot;length&quot;)ax.set_label(&quot;weight&quot;)plt.show() 12print(lr.score(train_input, train_target))print(lr.score(test_input, test_target)) # Underfitting 0.939846333997604 0.8247503123313558 The model is so simple that it is underfit overall. It seems that polynomial regression is needed. Polynomial Regression coef_ : regression coefficients(weights) intercept_ : regression intercept $y &#x3D; a + b_1x_1 + b_2x_2 + … + b_nx_n$ Predict a data1234# Broadcasting in Numpytrain_poly = np.column_stack((train_input ** 2, train_input))test_poly = np.column_stack((test_input ** 2, test_input))print(train_poly.shape, test_poly.shape) (42, 2) (14, 2) ※ Broadcasting in Numpy tutorial : https://numpy.org/doc/stable/user/basics.broadcasting.html 123lr2 = LinearRegression()lr2.fit(train_poly, train_target)print(lr2.predict([[50**2, 50]])) [1573.98423528] Regression equation1print(lr2.coef_, lr2.intercept_) [ 1.01433211 -21.55792498] 116.0502107827827 123456789point = np.arange(15,50)fig, ax = plt.subplots()ax.scatter(train_input, train_target)ax.plot(point, lr2.coef_[0]*point**2 + lr2.coef_[1]*point + lr2.intercept_)ax.scatter(50, lr2.predict([[50**2, 50]]), marker=&quot;^&quot;)ax.set_xlabel(&#x27;length&#x27;)ax.set_ylabel(&#x27;weight&#x27;)plt.show() 12print(lr2.score(train_poly, train_target))print(lr2.score(test_poly, test_target)) # Underfitting 0.9706807451768623 0.9775935108325122 The model has improved a lot, but it is still underfit. It seems that a more complex model is needed. Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 3_1","slug":"Python/ML/ML_ch_3_1","date":"2022-03-28T08:33:50.000Z","updated":"2022-10-05T05:39:52.462Z","comments":true,"path":"2022/03/28/Python/ML/ML_ch_3_1/","link":"","permalink":"http://gonekng.github.io/2022/03/28/Python/ML/ML_ch_3_1/","excerpt":"","text":"Prepare DataData Set12345678910111213141516171819import numpy as npperch_length = np.array( [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0] )perch_weight = np.array( [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0] ) Visualize Data12345678import matplotlib.pyplot as plt# object orientationfig, ax = plt.subplots()ax.scatter(perch_length, perch_weight)ax.set_label(&quot;length&quot;)ax.set_label(&quot;weight&quot;)plt.show() KNN Regression low importance Split Data12345from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( perch_length, perch_weight, random_state=42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((42,), (14,), (42,), (14,)) 12345# change data set to two-dimensional arraytrain_input = train_input.reshape(-1,1)test_input = test_input.reshape(-1,1)print(train_input.shape, test_input.shape) (42, 1) (14, 1) Model fitting12345from sklearn.neighbors import KNeighborsRegressorknr = KNeighborsRegressor()knr.fit(train_input, train_target)knr.score(test_input, test_target) # Coefficient of Determination (R-squared) 0.992809406101064 MAE Returns the average of absolute value errors between targets and predictions. 12from sklearn.metrics import mean_absolute_errortest_prediction = knr.predict(test_input) 12mae = mean_absolute_error(test_target, test_prediction)print(mae) # On average, about 19.2 grams different from the target. 19.157142857142862 Overfitting vs. Underfitting Overfitting: Good prediction from training data and poor prediction from testing data difficulty in finding and solving Underfitting: Poor prediction from training data and good prediction from testing data Or, poor prediction on both sides The amount of data is small or the model is too simple. 12print(knr.score(train_input, train_target))print(knr.score(test_input, test_target)) 0.9698823289099254 0.992809406101064 123456789# Set the number of neighbors to 3.knr.n_neighbors = 3knr.fit(train_input, train_target)print(knr.score(train_input, train_target))print(knr.score(test_input, test_target))test_prediction = knr.predict(test_input)mae = mean_absolute_error(test_target, test_prediction)print(mae) # On average, about 35.4 grams different from the target. 0.9804899950518966 0.9746459963987609 35.42380952380951 Conclusion k&#x3D;5 : R^2&#x3D; 0.99, MAE&#x3D;19.2 k&#x3D;3 : R^2&#x3D; 0.97, MAE&#x3D;35.4 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 2_2","slug":"Python/ML/ML_ch_2_2","date":"2022-03-28T08:32:34.000Z","updated":"2022-10-05T05:39:52.304Z","comments":true,"path":"2022/03/28/Python/ML/ML_ch_2_2/","link":"","permalink":"http://gonekng.github.io/2022/03/28/Python/ML/ML_ch_2_2/","excerpt":"","text":"Prepare data with Numpy12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 123import numpy as npfish_data = np.column_stack((fish_length, fish_weight))print(fish_data[:5]) [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ]] 12fish_target = np.concatenate((np.ones(35), np.zeros(14)))print(fish_target) [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Split data with Scikit-learn1234from sklearn.model_selection import train_test_split# stratify: spliting data according to class proportionstrain_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42) 123print(train_input.shape, test_input.shape)print(train_target.shape, test_target.shape)print(test_target) (36, 2) (13, 2) (36,) (13,) [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] KNN 1KNN fitting1234from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 Predicting new data1print(kn.predict([[25,150]])) # the actual data is a bream, but predicted to be smelt. [0.] 123456789import matplotlib.pyplot as plt# Scatter plot with new datafig, ax = plt.subplots()ax.scatter(train_input[:,0], train_input[:,1])ax.scatter(25, 150, marker=&quot;^&quot;)ax.set_xlabel(&#x27;length&#x27;)ax.set_ylabel(&#x27;weight&#x27;)plt.show() 12345678910distances, indexes = kn.kneighbors([[25,150]]) # the nearest neighbors (default: 5)# Scatter plot with 5 nearest neighborsfig, ax = plt.subplots()ax.scatter(train_input[:,0], train_input[:,1])ax.scatter(25, 150, marker=&quot;^&quot;)ax.scatter(train_input[indexes,0], train_input[indexes,1], marker=&#x27;D&#x27;) # rhombus markerax.set_xlabel(&#x27;length&#x27;)ax.set_ylabel(&#x27;weight&#x27;)plt.show() 123456789# Scatter plot on the same scalefig, ax = plt.subplots()ax.scatter(train_input[:,0], train_input[:,1])ax.scatter(25, 150, marker=&quot;^&quot;)ax.scatter(train_input[indexes,0], train_input[indexes,1], marker=&#x27;D&#x27;) # rhombus markerax.set_xlim((0,1000)) # change the x scaleax.set_xlabel(&#x27;length&#x27;)ax.set_ylabel(&#x27;weight&#x27;)plt.show() KNN 2Data Preprocessing1234# standard scoremean = np.mean(train_input, axis=0) # axis=0 : for each featurestd = np.std(train_input, axis=0)train_scaled = (train_input - mean) / std # broadcasting in numpy 12345678# Scatter plot with standard scorenew = ([25, 150] - mean ) / stdfig, ax = plt.subplots()ax.scatter(train_scaled[:,0], train_scaled[:,1])ax.scatter(new[0], new[1], marker=&quot;^&quot;)ax.set_xlabel(&#x27;length&#x27;)ax.set_ylabel(&#x27;weight&#x27;)plt.show() KNN fitting123test_scaled = (test_input - mean) / stdkn.fit(train_scaled, train_target)kn.score(test_scaled, test_target) # 1.0 Predicting new data1print(kn.predict([new])) # the actual data is a bream, and predicted to be bream. [1.] 12345678910distances, indexes = kn.kneighbors([new])# Scatter plot with 5 nearest neighborsfig, ax = plt.subplots()ax.scatter(train_scaled[:,0], train_scaled[:,1])ax.scatter(new[0], new[1], marker=&quot;^&quot;)ax.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker=&#x27;D&#x27;) # rhombus markerax.set_xlabel(&#x27;length&#x27;)ax.set_ylabel(&#x27;weight&#x27;)plt.show() Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 2_1","slug":"Python/ML/ML_ch_2_1","date":"2022-03-28T08:31:35.000Z","updated":"2022-10-05T05:39:52.167Z","comments":true,"path":"2022/03/28/Python/ML/ML_ch_2_1/","link":"","permalink":"http://gonekng.github.io/2022/03/28/Python/ML/ML_ch_2_1/","excerpt":"","text":"ML AlgorithmSupervised Learning(지도 학습) Input(입력; independent variable) &amp; Target(타깃; dependent variable) Question with a correct answer Type 1: Classification(분류) Type 2: Regression(예측) Feature(특성) &#x3D; independent variable(column) Unspervised Learning(비지도 학습) only Input, not Target Question without an answer algorithm automatically categorizes Data set12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 12fish_data = [[l,w] for l, w in zip(fish_length, fish_weight)]fish_target = [1]*35 + [0]*14 # 1: bream, 0: smelt KNN 1Create KNN12from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier() Data Split train set &amp; test set 1234train_input = fish_data[:35]train_target = fish_target[:35]test_input = fish_data[35:]test_target = fish_target[35:] Model fitting12kn = kn.fit(train_input, train_target)kn.score(test_input, test_target) # Sampling bias 0.0 KNN 2Numpy array123456import numpy as npinput_arr = np.array(fish_data)target_arr = np.array(fish_target)input_arr.shape, target_arr.shape ((49, 2), (49,)) Data Shuffle and Split12345np.random.seed(42)index = np.arange(49)print(index)np.random.shuffle(index)print(index) [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] 1234train_input = input_arr[index[:35]]train_target = target_arr[index[:35]]test_input = input_arr[index[35:]]test_target = target_arr[index[35:]] Scatter Plot12345678import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(train_input[:,0],train_input[:,1])ax.scatter(test_input[:,0],test_input[:,1])ax.set_xlabel(&#x27;length&#x27;)ax.set_ylabel(&#x27;weight&#x27;)plt.show() Model fitting12kn = kn.fit(train_input, train_target)kn.score(test_input, test_target) 1.0 1print(kn.predict(test_input) == test_target) [ True True True True True True True True True True True True True True] Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"ML Practice 1_3","slug":"Python/ML/ML_ch_1_3","date":"2022-03-26T06:38:35.000Z","updated":"2022-10-05T05:39:52.024Z","comments":true,"path":"2022/03/26/Python/ML/ML_ch_1_3/","link":"","permalink":"http://gonekng.github.io/2022/03/26/Python/ML/ML_ch_1_3/","excerpt":"","text":"Market and Machine LearningClassify Bream and SmeltBream Data123456bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] 123456import matplotlib.pyplot as pltplt.scatter(bream_length, bream_weight)plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;weight&#x27;)plt.show() Smelt Data12345678smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]plt.scatter(bream_length, bream_weight)plt.scatter(smelt_length, smelt_weight)plt.xlabel(&#x27;length&#x27;)plt.ylabel(&#x27;weight&#x27;)plt.show() 1st ML Program12345length = bream_length + smelt_lengthweight = bream_weight + smelt_weightfish_data = [[l,w] for l, w in zip(length, weight)]print(fish_data) [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]] 12fish_target = [1]*35 + [0]*14print(fish_target) [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] K-Nearest Neighbor12345from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier()kn.fit(fish_data, fish_target)kn.score(fish_data, fish_target) 1.0 1kn.predict([[30,600]]) array([1]) 12print(kn._fit_X)print(kn._y) [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 6.7] [ 10.5 7.5] [ 10.6 7. ] [ 11. 9.7] [ 11.2 9.8] [ 11.3 8.7] [ 11.8 10. ] [ 11.8 9.9] [ 12. 9.8] [ 12.2 12.2] [ 12.4 13.4] [ 13. 12.2] [ 14.3 19.7] [ 15. 19.9]] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 123kn49 = KNeighborsClassifier(n_neighbors=49)kn49.fit(fish_data, fish_target)kn49.score(fish_data, fish_target) 0.7142857142857143 123456for n in range(5, 50): kn.n_neighbors = n score = kn.score(fish_data, fish_target) if score &lt; 1: print(n, score) break 18 0.9795918367346939 Ref.) 혼자 공부하는 머신러닝+딥러닝 (박해선, 한빛미디어)","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"}],"author":"Jiwon Kang"},{"title":"Visualization tutorial","slug":"Python/Tutorial/visual_tutorial_01","date":"2022-03-24T02:58:50.000Z","updated":"2022-10-05T05:39:54.854Z","comments":true,"path":"2022/03/24/Python/Tutorial/visual_tutorial_01/","link":"","permalink":"http://gonekng.github.io/2022/03/24/Python/Tutorial/visual_tutorial_01/","excerpt":"","text":"데이터 시각화의 기본 조건 목적에 맞는 그래프 선정 선형 그래프, 막대 그래프, 산점도, 박스플롯 등등 환경에 맞는 도구 선택 코드 기반 : R, Python 프로그램 기반 : Excel, PowerBI, Tableau 등등 문맥(도메인)에 맞는 색과 도형 사용 파이썬 시각화 라이브러리Matplotlib 정형 데이터 &#x2F; 이미지 데이터 Pyplot API : Pyplot 모듈에 있는 함수를 각각 불러와서 구현 사용하기 편리하나, 세부 옵션 조정이 어려움 객체지향 API : Matplotlib에 구현된 객체지향 라이브러리를 직접 활용 라이브러리가 늘어나고, 코드가 복잡함 그래프의 디테일한 세부 옵션 조정이 용이함 일반적으로 두 API를 혼합하여 사용 Seaborn Matplotlib에 종속된 라이브러리 Matplotlib에 비해 코드가 간결함 통계 그래프 구현이 보다 용이 세부적인 옵션은 Matplotlib에서 조정 라이브러리 불러오기1234import matplotlibimport seaborn as snsprint(&quot;matplotlib ver :&quot;, matplotlib.__version__)print(&quot;seaborn ver :&quot;, sns.__version__) matplotlib ver : 3.2.2 seaborn ver : 0.11.2 시각화 테스트12345678910111213141516import matplotlib.pyplot as pltdates = [ &#x27;2021-01-01&#x27;, &#x27;2021-01-02&#x27;, &#x27;2021-01-03&#x27;, &#x27;2021-01-04&#x27;, &#x27;2021-01-05&#x27;, &#x27;2021-01-06&#x27;, &#x27;2021-01-07&#x27;, &#x27;2021-01-08&#x27;, &#x27;2021-01-09&#x27;, &#x27;2021-01-10&#x27;]min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0]# 파이썬 시각화 수행 전 기본 설정 (숫자는 변경 가능)fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,6))ax.plot(dates, min_temperature, label = &quot;Min Temp.&quot;)ax.plot(dates, max_temperature, label = &quot;Max Temp.&quot;)ax.legend()plt.show() 주식 데이터 예제1!pip install yfinance --upgrade --no-cache-dir Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.70) Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.5) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5) Requirement already satisfied: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10) Requirement already satisfied: lxml&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.8.0) Requirement already satisfied: requests&gt;=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24.0-&gt;yfinance) (1.15.0) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (2.0.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (2021.10.8) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.26-&gt;yfinance) (1.24.3) 12345import yfinance as yfdata = yf.download(&quot;AAPL&quot;, start=&quot;2019-08-01&quot;, end=&quot;2022-03-23&quot;)ts = data[&#x27;Open&#x27;]print(ts.head())print(type(ts)) [*********************100%***********************] 1 of 1 completed Date 2019-08-01 53.474998 2019-08-02 51.382500 2019-08-05 49.497501 2019-08-06 49.077499 2019-08-07 48.852501 Name: Open, dtype: float64 &lt;class &#39;pandas.core.series.Series&#39;&gt; pyplot 모듈1234567import matplotlib.pyplot as pltplt.plot(ts)plt.title(&quot;Stock Market of AAPL&quot;)plt.xlabel(&quot;Date&quot;)plt.ylabel(&quot;Open Price&quot;)plt.show() 객체지향 라이브러리123456789import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.plot(ts)ax.set_title(&quot;Stock Market of AAPL&quot;)ax.set_xlabel(&quot;Date&quot;)ax.set_ylabel(&quot;Open Price&quot;)plt.show() 막대 그래프Matplotlib12345678910111213141516171819202122import matplotlib.pyplot as pltimport numpy as npimport calendarmonth_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450]fig, ax = plt.subplots(figsize=(10,6))barplots = ax.bar(month_list, sold_list)print(&quot;barplots :&quot;, barplots) # artists 레이어에 12개의 막대가 저장됨for plot in barplots: print(plot) # print(plot.get_x()) # print(plot.get_y()) # print(plot.get_width()) # print(&quot;height:&quot;, plot.get_height()) height = plot.get_height() ax.text(plot.get_x() + plot.get_width()/2, height, height, ha = &#x27;center&#x27;, va = &#x27;bottom&#x27;)plt.xticks(month_list, calendar.month_name[1:13], rotation=30) # x축plt.show() barplots : &lt;BarContainer object of 12 artists&gt; Rectangle(xy=(0.6, 0), width=0.8, height=300, angle=0) Rectangle(xy=(1.6, 0), width=0.8, height=400, angle=0) Rectangle(xy=(2.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(3.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(4.6, 0), width=0.8, height=600, angle=0) Rectangle(xy=(5.6, 0), width=0.8, height=960, angle=0) Rectangle(xy=(6.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(7.6, 0), width=0.8, height=910, angle=0) Rectangle(xy=(8.6, 0), width=0.8, height=800, angle=0) Rectangle(xy=(9.6, 0), width=0.8, height=700, angle=0) Rectangle(xy=(10.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(11.6, 0), width=0.8, height=450, angle=0) Seaborn123fig, ax = plt.subplots()sns.countplot(x=&quot;day&quot;, data=tips)plt.show() 1234print(tips[&#x27;day&#x27;].value_counts().index)print(tips[&#x27;day&#x27;].value_counts().values)print()print(tips[&#x27;day&#x27;].value_counts(ascending=True)) CategoricalIndex([&#39;Sat&#39;, &#39;Sun&#39;, &#39;Thur&#39;, &#39;Fri&#39;], categories=[&#39;Thur&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;], ordered=False, dtype=&#39;category&#39;) [87 76 62 19] Fri 19 Thur 62 Sun 76 Sat 87 Name: day, dtype: int64 12345678910fig, ax = plt.subplots()ax = sns.countplot(x=&quot;day&quot;, data=tips, order=tips[&#x27;day&#x27;].value_counts().index, alpha=0.5)for plot in ax.patches: print(plot) height = plot.get_height() ax.text(plot.get_x() + plot.get_width()/2, height, height, ha = &#x27;center&#x27;, va = &#x27;bottom&#x27;)ax.set_ylim(-1, 100)plt.show() Rectangle(xy=(-0.4, 0), width=0.8, height=87, angle=0) Rectangle(xy=(0.6, 0), width=0.8, height=76, angle=0) Rectangle(xy=(1.6, 0), width=0.8, height=62, angle=0) Rectangle(xy=(2.6, 0), width=0.8, height=19, angle=0) 산점도Matplotlib123456789101112131415import seaborn as snstips = sns.load_dataset(&quot;tips&quot;)print(tips.info())x = tips[&#x27;total_bill&#x27;]y = tips[&#x27;tip&#x27;]fig, ax = plt.subplots(figsize=(10,6))ax.scatter(x,y)ax.set_title(&#x27;Scatter of tips&#x27;)ax.set_xlabel(&#x27;Total Bill&#x27;)ax.set_ylabel(&#x27;Tip&#x27;)plt.show() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 244 entries, 0 to 243 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 total_bill 244 non-null float64 1 tip 244 non-null float64 2 sex 244 non-null category 3 smoker 244 non-null category 4 day 244 non-null category 5 time 244 non-null category 6 size 244 non-null int64 dtypes: category(4), float64(2), int64(1) memory usage: 7.4 KB None 12345678910tips[&#x27;sex_color&#x27;] = tips[&#x27;sex&#x27;].map(&#123;&#x27;Male&#x27;:&#x27;#4663F5&#x27;, &#x27;Female&#x27;:&#x27;#FF5F2E&#x27;&#125;)fig, ax = plt.subplots(figsize=(10,6))for label, data in tips.groupby(&#x27;sex&#x27;): ax.scatter(data[&#x27;total_bill&#x27;], data[&#x27;tip&#x27;], label=label, color=data[&#x27;sex_color&#x27;], alpha=0.5) ax.set_xlabel(&#x27;Total Bill&#x27;) ax.set_ylabel(&#x27;Tip&#x27;)ax.legend()plt.show() Seaborn12345678910import matplotlib.pyplot as pltimport seaborn as snstips = sns.load_dataset(&quot;tips&quot;)print(tips.info())fig, ax = plt.subplots(figsize=(10,6))sns.scatterplot(x=&#x27;total_bill&#x27;, y=&#x27;tip&#x27;, hue=&#x27;sex&#x27;, data=tips)ax.legend()plt.show() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 244 entries, 0 to 243 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 total_bill 244 non-null float64 1 tip 244 non-null float64 2 sex 244 non-null category 3 smoker 244 non-null category 4 day 244 non-null category 5 time 244 non-null category 6 size 244 non-null int64 dtypes: category(4), float64(2), int64(1) memory usage: 7.4 KB None 두 개의 그래프를 동시에 표현123456fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))sns.regplot(x=&#x27;total_bill&#x27;, y=&#x27;tip&#x27;, data=tips, ax=ax[0], fit_reg=True)ax[0].set_title(&quot;Scatterplot with Regression Line&quot;)sns.regplot(x=&#x27;total_bill&#x27;, y=&#x27;tip&#x27;, data=tips, ax=ax[1], fit_reg=False)ax[1].set_title(&quot;Scatterplot without Regression Line&quot;)plt.show() 종합 예제123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import matplotlib.pyplot as pltimport seaborn as snsimport numpy as npfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FuncFormatter)def major_formatter(x, pos): return &quot;$ %.2f&quot; % xformatter = FuncFormatter(major_formatter)tips = sns.load_dataset(&quot;tips&quot;)fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,6))### 왼쪽 막대 그래프ax0 = sns.barplot(x=&quot;day&quot;, y=&#x27;total_bill&#x27;, data=tips, ax=ax[0], ci=None, alpha=0.85)# 텍스트 입력for p in ax0.patches: height = np.round(p.get_height(), 2) ax0.text(p.get_x() + p.get_width()/2., height+1, height, ha = &#x27;center&#x27;, size=12)# 축 범위 및 제목 수정ax0.set_ylim(-3, 30)ax0.set_title(&quot;Basic Bar Graph&quot;)### 왼쪽 막대 그래프ax1 = sns.barplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, data=tips, ax=ax[1], ci=None, color=&#x27;lightgray&#x27;, alpha=0.85, zorder=2)# total_bill 평균이 가장 큰 요일group_mean = tips.groupby([&#x27;day&#x27;])[&#x27;total_bill&#x27;].agg(&#x27;mean&#x27;)h_day = group_mean.sort_values(ascending=False).index[0] # sundayh_mean = group_mean.sort_values(ascending=False).values[0] # 21.41# 막대별 옵션 설정for plot in ax1.patches: height = np.round(plot.get_height(), 2) # 기본 설정 fontweight = &quot;normal&quot; color = &quot;k&quot; # 조건 설정 if h_mean == height: fontweight = &quot;bold&quot; color = &quot;darkred&quot; plot.set_facecolor(color) plot.set_edgecolor(&quot;black&quot;) # 텍스트 입력 ax1.text(plot.get_x() + plot.get_width()/2., height+1, height, ha =&#x27;center&#x27;, size=12, fontweight=fontweight, color=color)# spines 제거ax1.spines[&#x27;top&#x27;].set_visible(False)ax1.spines[&#x27;left&#x27;].set_position((&quot;outward&quot;, 20))ax1.spines[&#x27;left&#x27;].set_visible(False)ax1.spines[&#x27;right&#x27;].set_visible(False)ax1.spines[&#x27;bottom&#x27;].set_visible(False)# 축 범위 및 제목 수정ax1.set_ylim(-1, 30)ax1.set_title(&quot;Ideal Bar Graph&quot;, size=16)ax1.yaxis.set_major_locator(MultipleLocator(10))ax1.yaxis.set_major_formatter(formatter)ax1.yaxis.set_minor_locator(MultipleLocator(5))# 축 레이블 수정ax1.set_ylabel(&quot;Avg. Total Bill($)&quot;, fontsize=14)ax1.set_xlabel(&quot;Weekday&quot;, fontsize=14)# 그리드ax1.grid(axis=&quot;y&quot;, which=&quot;major&quot;, color=&quot;lightgray&quot;)ax1.grid(axis=&quot;y&quot;, which=&quot;minor&quot;, ls=&quot;:&quot;)# 축 범주 수정for xtick in ax1.get_xticklabels(): if xtick.get_text() == h_day: xtick.set_color(&quot;darkred&quot;) xtick.set_fontweight(&quot;demibold&quot;)ax1.set_xticklabels([&#x27;Thursday&#x27;, &#x27;Friday&#x27;, &#x27;Saturday&#x27;, &#x27;Sunday&#x27;], size=12)plt.show()","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"visualization","slug":"visualization","permalink":"http://gonekng.github.io/tags/visualization/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://gonekng.github.io/tags/matplotlib/"},{"name":"seaborn","slug":"seaborn","permalink":"http://gonekng.github.io/tags/seaborn/"}],"author":"Jiwon Kang"},{"title":"10minutes to Pandas","slug":"Python/Tutorial/10m_to_pd","date":"2022-03-24T02:55:23.000Z","updated":"2022-10-05T05:39:54.218Z","comments":true,"path":"2022/03/24/Python/Tutorial/10m_to_pd/","link":"","permalink":"http://gonekng.github.io/2022/03/24/Python/Tutorial/10m_to_pd/","excerpt":"","text":"Import Library1234import numpy as npprint(&quot;numpy ver.&quot; + np.__version__)import pandas as pdprint(&quot;pandas ver.&quot; + pd.__version__) numpy ver.1.21.5 pandas ver.1.3.5 Object CreationCreating Series by passing a list of values, letting pandas create a default integer index 12s = pd.Series([1,3,5,np.nan, 6,8])print(s) 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 Creation DataFrame by passing a NumPy array, with a datetime index and labeled columns 12345dates = pd.date_range(&quot;20130101&quot;, periods=6)print(dates)print()df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&quot;ABCD&quot;))print(df) DatetimeIndex([&#39;2013-01-01&#39;, &#39;2013-01-02&#39;, &#39;2013-01-03&#39;, &#39;2013-01-04&#39;, &#39;2013-01-05&#39;, &#39;2013-01-06&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) A B C D 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 by passing a dictionary of objects that can be converted into a series-like structure 1234567891011121314df2 = pd.DataFrame( &#123; &quot;A&quot;: 1.0, &quot;B&quot;: pd.Timestamp(&quot;20130102&quot;), &quot;C&quot;: pd.Series(1, index=list(range(4)), dtype=&quot;float32&quot;), &quot;D&quot;: np.array([3] * 4, dtype=&quot;int32&quot;), &quot;E&quot;: pd.Categorical([&quot;test&quot;, &quot;train&quot;, &quot;test&quot;, &quot;train&quot;]), &quot;F&quot;: &quot;foo&quot;, &#125;)print(df2)print()print(&quot;dtypes:&quot;)print(df2.dtypes) A B C D E F 0 1.0 2013-01-02 1.0 3 test foo 1 1.0 2013-01-02 1.0 3 train foo 2 1.0 2013-01-02 1.0 3 test foo 3 1.0 2013-01-02 1.0 3 train foo dtypes: A float64 B datetime64[ns] C float32 D int32 E category F object dtype: object 12# IPython# df2.&lt;TAB&gt; Viewing DataBasic Structure123print(&quot;Head 5:\\n&quot;, df.head())print()print(&quot;Tail 5:\\n&quot;, df.tail()) Head 5: A B C D 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 -1.707570 1.163615 Tail 5: A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 12print(&quot;Index :\\n&quot;, df.index)print(&quot;Columns:\\n&quot;, df.columns) Index : DatetimeIndex([&#39;2013-01-01&#39;, &#39;2013-01-02&#39;, &#39;2013-01-03&#39;, &#39;2013-01-04&#39;, &#39;2013-01-05&#39;, &#39;2013-01-06&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) Columns: Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], dtype=&#39;object&#39;) DataFrame to NumPy array This can be an expensive operation when your DataFrame has columns with different data types: NumPy arrays have one dtype for the entire array. Pandas DataFrames have one dtype per column. With heterogeneous data, the lowest common type will have to be used. For a mix of numeric and non-numeric types, the output array will have object dtype. 1df.values array([[-0.81889557, -0.40918379, 0.08744721, -0.93588677], [ 0.68154262, -1.39097644, 2.01310478, 0.64446846], [ 1.01791104, 0.03322364, -0.1039122 , 0.63445887], [-0.45043711, 0.50191541, 1.00377617, 0.69124864], [ 1.633764 , 0.32423439, -1.70757042, 1.16361464], [ 0.28240247, -0.92266305, -1.64131359, 0.50543323]]) 123# For df, our DataFrame of all floating-point values,# DataFrame.to_numpy() is fast and doesn&#x27;t require copying data:df.to_numpy() array([[-0.81889557, -0.40918379, 0.08744721, -0.93588677], [ 0.68154262, -1.39097644, 2.01310478, 0.64446846], [ 1.01791104, 0.03322364, -0.1039122 , 0.63445887], [-0.45043711, 0.50191541, 1.00377617, 0.69124864], [ 1.633764 , 0.32423439, -1.70757042, 1.16361464], [ 0.28240247, -0.92266305, -1.64131359, 0.50543323]]) 1df2.values array([[1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;]], dtype=object) 123# For df2, the DataFrame with multiple dtypes,# DataFrame.to_numpy() is relatively expensive:df2.to_numpy() array([[1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;test&#39;, &#39;foo&#39;], [1.0, Timestamp(&#39;2013-01-02 00:00:00&#39;), 1.0, 3, &#39;train&#39;, &#39;foo&#39;]], dtype=object) Describe, Transpose, Sort1print(df.describe()) A B C D count 6.000000 6.000000 6.000000 6.000000 mean 0.391048 -0.310575 -0.058078 0.450556 std 0.917121 0.739319 1.460690 0.715967 min -0.818896 -1.390976 -1.707570 -0.935887 25% -0.267227 -0.794293 -1.256963 0.537690 50% 0.481973 -0.187980 -0.008232 0.639464 75% 0.933819 0.251482 0.774694 0.679554 max 1.633764 0.501915 2.013105 1.163615 1print(df.T) 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06 A -0.818896 0.681543 1.017911 -0.450437 1.633764 0.282402 B -0.409184 -1.390976 0.033224 0.501915 0.324234 -0.922663 C 0.087447 2.013105 -0.103912 1.003776 -1.707570 -1.641314 D -0.935887 0.644468 0.634459 0.691249 1.163615 0.505433 12345print(&quot;Sorted by axis 1:&quot;)print(df.sort_index(axis=1, ascending=False))print()print(&quot;Sorted by values B:&quot;)print(df.sort_values(by=&quot;B&quot;)) Sorted by axis 1: D C B A 2013-01-01 -0.935887 0.087447 -0.409184 -0.818896 2013-01-02 0.644468 2.013105 -1.390976 0.681543 2013-01-03 0.634459 -0.103912 0.033224 1.017911 2013-01-04 0.691249 1.003776 0.501915 -0.450437 2013-01-05 1.163615 -1.707570 0.324234 1.633764 2013-01-06 0.505433 -1.641314 -0.922663 0.282402 Sorted by values B: A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-04 -0.450437 0.501915 1.003776 0.691249 Selecting DataGetting12print(df[&quot;A&quot;])# print(df.A) 2013-01-01 -0.818896 2013-01-02 0.681543 2013-01-03 1.017911 2013-01-04 -0.450437 2013-01-05 1.633764 2013-01-06 0.282402 Freq: D, Name: A, dtype: float64 12print(df[:3])print(df[&quot;20130102&quot;:&quot;20130104&quot;]) A B C D 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-04 -0.450437 0.501915 1.003776 0.691249 Selection by Label1print(df.loc[dates[0]]) A -0.818896 B -0.409184 C 0.087447 D -0.935887 Name: 2013-01-01 00:00:00, dtype: float64 1print(df.loc[&quot;20130102&quot;:&quot;20130104&quot;, [&quot;A&quot;,&quot;B&quot;]]) A B 2013-01-02 0.681543 -1.390976 2013-01-03 1.017911 0.033224 2013-01-04 -0.450437 0.501915 1print(df.loc[&quot;20130102&quot;, [&quot;A&quot;,&quot;B&quot;]]) A 0.681543 B -1.390976 Name: 2013-01-02 00:00:00, dtype: float64 12print(df.loc[dates[0], &quot;A&quot;])print(df.at[dates[0],&quot;A&quot;]) # equivalent to the prior method -0.818895566676464 -0.818895566676464 Selection by Position1print(df.iloc[3]) A -0.450437 B 0.501915 C 1.003776 D 0.691249 Name: 2013-01-04 00:00:00, dtype: float64 1print(df.iloc[3:5, 0:2]) A B 2013-01-04 -0.450437 0.501915 2013-01-05 1.633764 0.324234 1print(df.iloc[[1,2,5],[1,3]]) B D 2013-01-02 -1.390976 0.644468 2013-01-03 0.033224 0.634459 2013-01-06 -0.922663 0.505433 1print(df.iloc[1:3,:]) A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 1print(df.iloc[:,1:3]) B C 2013-01-01 -0.409184 0.087447 2013-01-02 -1.390976 2.013105 2013-01-03 0.033224 -0.103912 2013-01-04 0.501915 1.003776 2013-01-05 0.324234 -1.707570 2013-01-06 -0.922663 -1.641314 12print(df.iloc[1,1])print(df.iat[1,1]) # equivalent to the prior method -1.3909764417520816 -1.3909764417520816 Boolean Indexing1print(df[df[&quot;A&quot;]&gt;0]) A B C D 2013-01-02 0.681543 -1.390976 2.013105 0.644468 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2013-01-05 1.633764 0.324234 -1.707570 1.163615 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 1print(df[df&gt;0]) A B C D 2013-01-01 NaN NaN 0.087447 NaN 2013-01-02 0.681543 NaN 2.013105 0.644468 2013-01-03 1.017911 0.033224 NaN 0.634459 2013-01-04 NaN 0.501915 1.003776 0.691249 2013-01-05 1.633764 0.324234 NaN 1.163615 2013-01-06 0.282402 NaN NaN 0.505433 12345df2 = df.copy()df2[&quot;E&quot;] = [&#x27;one&#x27;, &#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;, &#x27;four&#x27;, &#x27;three&#x27;]print(df2)print()print(df2[df2[&quot;E&quot;].isin([&quot;two&quot;, &quot;four&quot;])]) A B C D E 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 one 2013-01-02 0.681543 -1.390976 2.013105 0.644468 one 2013-01-03 1.017911 0.033224 -0.103912 0.634459 two 2013-01-04 -0.450437 0.501915 1.003776 0.691249 three 2013-01-05 1.633764 0.324234 -1.707570 1.163615 four 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 three A B C D E 2013-01-03 1.017911 0.033224 -0.103912 0.634459 two 2013-01-05 1.633764 0.324234 -1.707570 1.163615 four Setting12345s1 = pd.Series([1,2,3,4,5,6], index = pd.date_range(&quot;20130102&quot;,periods=6))print(s1)print()df[&quot;F&quot;] = s1print(df) 2013-01-02 1 2013-01-03 2 2013-01-04 3 2013-01-05 4 2013-01-06 5 2013-01-07 6 Freq: D, dtype: int64 A B C D F 2013-01-01 -0.818896 -0.409184 0.087447 -0.935887 NaN 2013-01-02 0.681543 -1.390976 2.013105 0.644468 1.0 2013-01-03 1.017911 0.033224 -0.103912 0.634459 2.0 2013-01-04 -0.450437 0.501915 1.003776 0.691249 3.0 2013-01-05 1.633764 0.324234 -1.707570 1.163615 4.0 2013-01-06 0.282402 -0.922663 -1.641314 0.505433 5.0 1234df.at[dates[0], &quot;A&quot;] = 0 # setting values by labeldf.iat[0,1] = 0 # setting values by positiondf.loc[:, &quot;D&quot;] = np.array([5] * len(df)) # setting by assigning with a NumPy arrayprint(df) A B C D F 2013-01-01 0.000000 0.000000 0.087447 5 NaN 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 2013-01-05 1.633764 0.324234 -1.707570 5 4.0 2013-01-06 0.282402 -0.922663 -1.641314 5 5.0 123df2 = df.copy()df2[df2&gt;0] = -df2 # setting values by booleanprint(df2) A B C D F 2013-01-01 0.000000 0.000000 -0.087447 -5 NaN 2013-01-02 -0.681543 -1.390976 -2.013105 -5 -1.0 2013-01-03 -1.017911 -0.033224 -0.103912 -5 -2.0 2013-01-04 -0.450437 -0.501915 -1.003776 -5 -3.0 2013-01-05 -1.633764 -0.324234 -1.707570 -5 -4.0 2013-01-06 -0.282402 -0.922663 -1.641314 -5 -5.0 Missing Data123df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [&quot;E&quot;])df1.loc[dates[0] : dates[1], &quot;E&quot;] = 1print(df1) A B C D F E 2013-01-01 0.000000 0.000000 0.087447 5 NaN 1.0 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 NaN 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 NaN 123print(df1.dropna(how=&quot;any&quot;)) # how=&quot;any&quot; (default) : where any NA values are presentprint()print(df1.dropna(how=&quot;all&quot;)) # how=&quot;all&quot; : where all values are NA A B C D F E 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 A B C D F E 2013-01-01 0.000000 0.000000 0.087447 5 NaN 1.0 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 NaN 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 NaN 123print(pd.isna(df1))print()print(df1.fillna(value=5)) A B C D F E 2013-01-01 False False False False True False 2013-01-02 False False False False False False 2013-01-03 False False False False False True 2013-01-04 False False False False False True A B C D F E 2013-01-01 0.000000 0.000000 0.087447 5 5.0 1.0 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 5.0 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 5.0 OperationsStats12345print(&quot;axis 0 :&quot;)print(df.mean())print()print(&quot;axis 1 :&quot;)print(df.mean(1)) axis 0 : A 0.527531 B -0.242378 C -0.058078 D 5.000000 F 3.000000 dtype: float64 axis 1 : 2013-01-01 1.271862 2013-01-02 1.460734 2013-01-03 1.589444 2013-01-04 1.811051 2013-01-05 1.850086 2013-01-06 1.543685 Freq: D, dtype: float64 123456print(df)print()s = pd.Series([1, 3, 4, np.nan, 6, 8], index=dates).shift(2)print(s)print()print(df.sub(s, axis=0)) # equivalent to (dataframe - other) A B C D F 2013-01-01 0.000000 0.000000 0.087447 5 NaN 2013-01-02 0.681543 -1.390976 2.013105 5 1.0 2013-01-03 1.017911 0.033224 -0.103912 5 2.0 2013-01-04 -0.450437 0.501915 1.003776 5 3.0 2013-01-05 1.633764 0.324234 -1.707570 5 4.0 2013-01-06 0.282402 -0.922663 -1.641314 5 5.0 2013-01-01 NaN 2013-01-02 NaN 2013-01-03 1.0 2013-01-04 3.0 2013-01-05 4.0 2013-01-06 NaN Freq: D, dtype: float64 A B C D F 2013-01-01 NaN NaN NaN NaN NaN 2013-01-02 NaN NaN NaN NaN NaN 2013-01-03 0.017911 -0.966776 -1.103912 4.0 1.0 2013-01-04 -3.450437 -2.498085 -1.996224 2.0 0.0 2013-01-05 -2.366236 -3.675766 -5.707570 1.0 0.0 2013-01-06 NaN NaN NaN NaN NaN Apply1print(df.apply(np.cumsum)) A B C D F 2013-01-01 0.000000 0.000000 0.087447 5 NaN 2013-01-02 0.681543 -1.390976 2.100552 10 1.0 2013-01-03 1.699454 -1.357753 1.996640 15 3.0 2013-01-04 1.249017 -0.855837 3.000416 20 6.0 2013-01-05 2.882781 -0.531603 1.292846 25 10.0 2013-01-06 3.165183 -1.454266 -0.348468 30 15.0 1print(df.apply(lambda x: x.max() - x.min())) A 2.084201 B 1.892892 C 3.720675 D 0.000000 F 4.000000 dtype: float64 Histogramming1234s = pd.Series(np.random.randint(0, 7, size=10))print(s)print()print(s.value_counts()) 0 1 1 6 2 5 3 1 4 1 5 3 6 2 7 1 8 1 9 6 dtype: int64 1 5 6 2 5 1 3 1 2 1 dtype: int64 String Methods1234s = pd.Series([&quot;A&quot;,&quot;Aaba&quot;, np.nan, &quot;CABA&quot;, &quot;cat&quot;])print(s.str.lower())print()print(s.str.upper()) 0 a 1 aaba 2 NaN 3 caba 4 cat dtype: object 0 A 1 AABA 2 NaN 3 CABA 4 CAT dtype: object MergeConcat combining together Series and DataFrame objects 12df = pd.DataFrame(np.random.randn(10,4))print(df) 0 1 2 3 0 -0.146700 1.278642 1.837775 0.644873 1 -0.332040 1.597295 -0.681229 -1.238212 2 0.751823 1.117058 -0.453366 0.953989 3 0.074173 -1.043050 0.276312 0.926186 4 -0.090674 1.679349 2.130480 0.950658 5 0.483778 -0.330530 0.370747 0.569736 6 -0.603331 2.363939 -0.052191 0.186119 7 -2.002784 -1.237193 -1.876920 -0.876104 8 1.121755 -0.104830 -1.675228 1.250540 9 0.008456 -1.287063 0.070528 -0.642563 1234pieces = [df[:3], df[3:7], df[7:]]print(pieces)print()print(pd.concat(pieces)) # Concatenating objects together [ 0 1 2 3 0 -0.146700 1.278642 1.837775 0.644873 1 -0.332040 1.597295 -0.681229 -1.238212 2 0.751823 1.117058 -0.453366 0.953989, 0 1 2 3 3 0.074173 -1.043050 0.276312 0.926186 4 -0.090674 1.679349 2.130480 0.950658 5 0.483778 -0.330530 0.370747 0.569736 6 -0.603331 2.363939 -0.052191 0.186119, 0 1 2 3 7 -2.002784 -1.237193 -1.876920 -0.876104 8 1.121755 -0.104830 -1.675228 1.250540 9 0.008456 -1.287063 0.070528 -0.642563] 0 1 2 3 0 -0.146700 1.278642 1.837775 0.644873 1 -0.332040 1.597295 -0.681229 -1.238212 2 0.751823 1.117058 -0.453366 0.953989 3 0.074173 -1.043050 0.276312 0.926186 4 -0.090674 1.679349 2.130480 0.950658 5 0.483778 -0.330530 0.370747 0.569736 6 -0.603331 2.363939 -0.052191 0.186119 7 -2.002784 -1.237193 -1.876920 -0.876104 8 1.121755 -0.104830 -1.675228 1.250540 9 0.008456 -1.287063 0.070528 -0.642563 Join SQL style merging 1234567left = pd.DataFrame(&#123;&#x27;key&#x27;:[&#x27;foo&#x27;, &#x27;foo&#x27;], &#x27;lval&#x27;:[1, 2]&#125;)right = pd.DataFrame(&#123;&#x27;key&#x27;:[&#x27;foo&#x27;, &#x27;foo&#x27;], &#x27;rval&#x27;:[4, 5]&#125;)print(left)print()print(right)print()print(pd.merge(left, right, on=&#x27;key&#x27;)) key lval 0 foo 1 1 foo 2 key rval 0 foo 4 1 foo 5 key lval rval 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 1234567left = pd.DataFrame(&#123;&#x27;key&#x27;:[&#x27;foo&#x27;, &#x27;bar&#x27;], &#x27;lval&#x27;:[1, 2]&#125;)right = pd.DataFrame(&#123;&#x27;key&#x27;:[&#x27;foo&#x27;, &#x27;bar&#x27;], &#x27;rval&#x27;:[4, 5]&#125;)print(left)print()print(right)print()print(pd.merge(left, right, on=&#x27;key&#x27;)) key lval 0 foo 1 1 bar 2 key rval 0 foo 4 1 bar 5 key lval rval 0 foo 1 4 1 bar 2 5 Append12df = pd.DataFrame(np.random.randn(8, 4), columns=[&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;])print(df) A B C D 0 0.120201 -0.286267 -0.205740 1.107503 1 -0.818479 -0.059564 0.192151 1.920259 2 0.687467 -0.833330 -0.426913 0.852926 3 -1.103335 0.213012 1.003000 -0.648385 4 -0.576006 1.414470 0.851227 -0.277232 5 0.189853 0.311170 -0.448372 -0.201059 6 1.707547 0.099129 -1.332999 0.478148 7 1.153861 1.831233 0.125928 -1.330353 12s = df.iloc[3]print(df.append(s, ignore_index=True)) A B C D 0 0.120201 -0.286267 -0.205740 1.107503 1 -0.818479 -0.059564 0.192151 1.920259 2 0.687467 -0.833330 -0.426913 0.852926 3 -1.103335 0.213012 1.003000 -0.648385 4 -0.576006 1.414470 0.851227 -0.277232 5 0.189853 0.311170 -0.448372 -0.201059 6 1.707547 0.099129 -1.332999 0.478148 7 1.153861 1.831233 0.125928 -1.330353 8 -1.103335 0.213012 1.003000 -0.648385 Grouping Splitting the data into groups based on some criteria Applying a function to each group independently Combining the results into a data structure 123456789df = pd.DataFrame( &#123; &quot;A&quot;: [&quot;foo&quot;, &quot;bar&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;foo&quot;, &quot;foo&quot;], &quot;B&quot;: [&quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;two&quot;, &quot;two&quot;, &quot;one&quot;, &quot;three&quot;], &quot;C&quot;: np.random.randn(8), &quot;D&quot;: np.random.randn(8), &#125;)print(df) A B C D 0 foo one 0.031321 -0.238988 1 bar one -0.626332 -0.445851 2 foo two -1.448981 1.262838 3 bar three -0.424664 -0.639157 4 foo two 1.547849 2.378992 5 bar two -0.351304 -0.521492 6 foo one -1.903777 1.998802 7 foo three -0.613947 -0.422391 1print(df.groupby(&quot;A&quot;).sum()) C D A bar -1.402300 -1.606500 foo -2.387536 4.979252 1print(df.groupby([&quot;A&quot;,&quot;B&quot;]).sum()) C D A B bar one -0.626332 -0.445851 three -0.424664 -0.639157 two -0.351304 -0.521492 foo one -1.872457 1.759814 three -0.613947 -0.422391 two 0.098868 3.641830 1234df_min = df.groupby([&quot;A&quot;,&quot;B&quot;])[&quot;C&quot;].min()df_max = df.groupby([&quot;A&quot;,&quot;B&quot;])[&quot;C&quot;].max()print(pd.merge(df_min, df_max, on=[&#x27;A&#x27;,&quot;B&quot;], suffixes=(&#x27;_min&#x27;, &#x27;_max&#x27;))) C_min C_max A B bar one -0.626332 -0.626332 three -0.424664 -0.424664 two -0.351304 -0.351304 foo one -1.903777 0.031321 three -0.613947 -0.613947 two -1.448981 1.547849 ReshapingStack stack() : Compress a level in the DataFrame’s columns: 1234567891011121314tuples = list( zip( *[ [&quot;bar&quot;, &quot;bar&quot;, &quot;baz&quot;, &quot;baz&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;qux&quot;, &quot;qux&quot;], [&quot;one&quot;, &quot;two&quot;, &quot;one&quot;, &quot;two&quot;, &quot;one&quot;, &quot;two&quot;, &quot;one&quot;, &quot;two&quot;], ] ))index = pd.MultiIndex.from_tuples(tuples, names=[&quot;idx_1&quot;, &quot;idx_2&quot;])df = pd.DataFrame(np.random.randn(8,2), index=index, columns=[&quot;A&quot;,&quot;B&quot;])df2 = df[:4]print(df2) A B idx_1 idx_2 bar one -0.748802 0.560048 two -0.214015 -0.658540 baz one -1.968829 -0.806776 two -1.314742 -0.174498 12stacked = df2.stack()print(stacked) # level: idx_1 &gt; idx_2 &gt; columns idx_1 idx_2 bar one A -0.748802 B 0.560048 two A -0.214015 B -0.658540 baz one A -1.968829 B -0.806776 two A -1.314742 B -0.174498 dtype: float64 unstack() : the inverse operation of stack() 12345print(stacked.unstack()) # columns(last level) / default -1print()print(stacked.unstack(0)) # idx_1 print()print(stacked.unstack(1)) # idx_2 A B idx_1 idx_2 bar one -0.748802 0.560048 two -0.214015 -0.658540 baz one -1.968829 -0.806776 two -1.314742 -0.174498 idx_1 bar baz idx_2 one A -0.748802 -1.968829 B 0.560048 -0.806776 two A -0.214015 -1.314742 B -0.658540 -0.174498 idx_2 one two idx_1 bar A -0.748802 -0.214015 B 0.560048 -0.658540 baz A -1.968829 -1.314742 B -0.806776 -0.174498 Pivot Table12345678910df = pd.DataFrame( &#123; &quot;A&quot;: [&quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;] * 3, &quot;B&quot;: [&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;] * 4, &quot;C&quot;: [&quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;] * 2, &quot;D&quot;: np.random.randn(12), &quot;E&quot;: np.random.randn(12), &#125;)print(df) A B C D E 0 one aa foo 1.055556 -0.342298 1 one bb foo -0.463657 -0.004332 2 two cc foo 0.953746 0.690613 3 three aa bar -0.980697 0.498251 4 one bb bar -0.352120 -0.503475 5 one cc bar 0.298470 -1.316212 6 two aa foo 0.580929 0.483970 7 three bb foo 0.391527 0.200354 8 one cc foo -1.314898 -1.183403 9 one aa bar -0.058855 -0.004713 10 two bb bar -0.253133 1.255313 11 three cc bar 0.882602 0.561369 1print(pd.pivot_table(df, values=&quot;D&quot;, index=[&quot;A&quot;,&quot;B&quot;], columns=&quot;C&quot;)) C bar foo A B one aa -0.058855 1.055556 bb -0.352120 -0.463657 cc 0.298470 -1.314898 three aa -0.980697 NaN bb NaN 0.391527 cc 0.882602 NaN two aa NaN 0.580929 bb -0.253133 NaN cc NaN 0.953746 Time Series12345rng = pd.date_range(&quot;1/1/2022&quot;, periods=100, freq=&quot;S&quot;) # secondly frequencyts = pd.Series(np.random.randint(0,500,len(rng)), index=rng)print(ts.resample(&quot;5Min&quot;).first())print(ts.resample(&quot;5Min&quot;).last())print(ts.resample(&quot;5Min&quot;).sum()) 2022-01-01 192 Freq: 5T, dtype: int64 2022-01-01 304 Freq: 5T, dtype: int64 2022-01-01 26586 Freq: 5T, dtype: int64 123rng = pd.date_range(&quot;3/6/2021 00:00&quot;, periods=5, freq=&quot;D&quot;) # daily frequencyts = pd.Series(np.random.randn(len(rng)), rng)print(ts) 2021-03-06 -1.465806 2021-03-07 0.938092 2021-03-08 2.811226 2021-03-09 -0.186533 2021-03-10 -1.048929 Freq: D, dtype: float64 12345ts_utc = ts.tz_localize(&quot;UTC&quot;) # UTC timezoneprint(&quot;UTC :&quot;)print(ts_utc)print(&quot;\\nUS/Eastern :&quot;)print(ts_utc.tz_convert(&quot;US/Eastern&quot;)) # US/Eastern timezone UTC : 2021-03-06 00:00:00+00:00 -1.465806 2021-03-07 00:00:00+00:00 0.938092 2021-03-08 00:00:00+00:00 2.811226 2021-03-09 00:00:00+00:00 -0.186533 2021-03-10 00:00:00+00:00 -1.048929 Freq: D, dtype: float64 US/Eastern : 2021-03-05 19:00:00-05:00 -1.465806 2021-03-06 19:00:00-05:00 0.938092 2021-03-07 19:00:00-05:00 2.811226 2021-03-08 19:00:00-05:00 -0.186533 2021-03-09 19:00:00-05:00 -1.048929 Freq: D, dtype: float64 12345678rng = pd.date_range(&quot;1/1/2022&quot;, periods=5, freq=&quot;M&quot;)ts = pd.Series(np.random.randn(len(rng)), index=rng)print(ts) # DatetimeIndexprint()ps = ts.to_period()print(ps) # PeriodIndexprint()print(ps.to_timestamp()) 2022-01-31 0.438135 2022-02-28 -0.052516 2022-03-31 -1.491175 2022-04-30 0.708521 2022-05-31 -1.794057 Freq: M, dtype: float64 DatetimeIndex([&#39;2022-01-31&#39;, &#39;2022-02-28&#39;, &#39;2022-03-31&#39;, &#39;2022-04-30&#39;, &#39;2022-05-31&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) 2022-01 0.438135 2022-02 -0.052516 2022-03 -1.491175 2022-04 0.708521 2022-05 -1.794057 Freq: M, dtype: float64 PeriodIndex([&#39;2022-01&#39;, &#39;2022-02&#39;, &#39;2022-03&#39;, &#39;2022-04&#39;, &#39;2022-05&#39;], dtype=&#39;period[M]&#39;) 2022-01-01 0.438135 2022-02-01 -0.052516 2022-03-01 -1.491175 2022-04-01 0.708521 2022-05-01 -1.794057 Freq: MS, dtype: float64 12345678prng = pd.period_range(&quot;1990Q1&quot;, &quot;2000Q4&quot;, freq=&quot;Q-NOV&quot;) # quarterly frequencyts = pd.Series(np.random.randn(len(prng)), prng)print(ts.head())print()# month end &amp; hour startts.index = (prng.asfreq(&quot;M&quot;, &quot;e&quot;) + 1).asfreq(&quot;H&quot;, &quot;s&quot;) + 9print(ts.head()) 1990Q1 1.479314 1990Q2 0.799696 1990Q3 0.148978 1990Q4 -0.571086 1991Q1 1.908509 Freq: Q-NOV, dtype: float64 1990-03-01 09:00 1.479314 1990-06-01 09:00 0.799696 1990-09-01 09:00 0.148978 1990-12-01 09:00 -0.571086 1991-03-01 09:00 1.908509 Freq: H, dtype: float64 Categoricals1234df = pd.DataFrame(&#123;&quot;id&quot;: [1, 2, 3, 4, 5, 6], &quot;raw_grade&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;e&quot;]&#125;)print(df) id raw_grade 0 1 a 1 2 b 2 3 b 3 4 a 4 5 a 5 6 e 123456df[&quot;grade&quot;] = df[&quot;raw_grade&quot;].astype(&quot;category&quot;)df[&quot;grade&quot;].cat.categories = [&quot;very good&quot;, &quot;good&quot;, &quot;very bad&quot;] # rename categoriesdf[&quot;grade&quot;] = df[&quot;grade&quot;].cat.set_categories( [&quot;very bad&quot;, &quot;bad&quot;,&quot;medium&quot;,&quot;good&quot;,&quot;very good&quot;])df[&quot;grade&quot;] 0 very good 1 good 2 good 3 very good 4 very good 5 very bad Name: grade, dtype: category Categories (5, object): [&#39;very bad&#39;, &#39;bad&#39;, &#39;medium&#39;, &#39;good&#39;, &#39;very good&#39;] 1print(df.sort_values(by=&quot;grade&quot;).reset_index(drop=True)) id raw_grade grade 0 6 e very bad 1 2 b good 2 3 b good 3 1 a very good 4 4 a very good 5 5 a very good 1print(df.groupby(&quot;grade&quot;).size()) grade very bad 1 bad 0 medium 0 good 2 very good 3 dtype: int64 Plotting123456import matplotlib.pyplot as pltts = pd.Series(np.random.randn(1000), index=pd.date_range(&quot;1/1/2010&quot;, periods=1000))ts = ts.cumsum()ts.plot()plt.show() 12345df = pd.DataFrame(np.random.randn(1000,4), index=ts.index, columns=[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;])df = df.cumsum()df.plot()plt.legend(loc=&#x27;best&#x27;)plt.show() Getting data in&#x2F;outCSV1df.to_csv(&quot;foo.csv&quot;) 1print(pd.read_csv(&quot;foo.csv&quot;)) Unnamed: 0 A B C D 0 2010-01-01 0.015846 0.025822 0.443290 -0.724835 1 2010-01-02 -0.084776 0.330946 -0.669265 -0.949676 2 2010-01-03 1.325583 -0.697926 -0.204375 -1.145391 3 2010-01-04 2.293831 -2.097458 -1.260249 -0.555542 4 2010-01-05 1.334001 -2.392123 -2.659330 -0.325232 .. ... ... ... ... ... 995 2012-09-22 -7.868011 -59.520981 -49.220218 -54.072522 996 2012-09-23 -7.288884 -59.584693 -49.307716 -54.783148 997 2012-09-24 -6.835061 -59.508180 -49.608991 -55.780242 998 2012-09-25 -6.863802 -58.711503 -50.458917 -55.218363 999 2012-09-26 -7.075728 -59.524064 -49.924592 -55.653215 [1000 rows x 5 columns] HDF51df.to_hdf(&quot;foo.h5&quot;, &quot;df&quot;) 1print(pd.read_hdf(&quot;foo.h5&quot;, &quot;df&quot;)) A B C D 2010-01-01 0.015846 0.025822 0.443290 -0.724835 2010-01-02 -0.084776 0.330946 -0.669265 -0.949676 2010-01-03 1.325583 -0.697926 -0.204375 -1.145391 2010-01-04 2.293831 -2.097458 -1.260249 -0.555542 2010-01-05 1.334001 -2.392123 -2.659330 -0.325232 ... ... ... ... ... 2012-09-22 -7.868011 -59.520981 -49.220218 -54.072522 2012-09-23 -7.288884 -59.584693 -49.307716 -54.783148 2012-09-24 -6.835061 -59.508180 -49.608991 -55.780242 2012-09-25 -6.863802 -58.711503 -50.458917 -55.218363 2012-09-26 -7.075728 -59.524064 -49.924592 -55.653215 [1000 rows x 4 columns] Excel1df.to_excel(&quot;foo.xlsx&quot;, sheet_name=&quot;Sheet1&quot;) 1print(pd.read_excel(&quot;foo.xlsx&quot;, &quot;Sheet1&quot;, index_col=None, na_values=[&quot;NA&quot;])) Unnamed: 0 A B C D 0 2010-01-01 0.015846 0.025822 0.443290 -0.724835 1 2010-01-02 -0.084776 0.330946 -0.669265 -0.949676 2 2010-01-03 1.325583 -0.697926 -0.204375 -1.145391 3 2010-01-04 2.293831 -2.097458 -1.260249 -0.555542 4 2010-01-05 1.334001 -2.392123 -2.659330 -0.325232 .. ... ... ... ... ... 995 2012-09-22 -7.868011 -59.520981 -49.220218 -54.072522 996 2012-09-23 -7.288884 -59.584693 -49.307716 -54.783148 997 2012-09-24 -6.835061 -59.508180 -49.608991 -55.780242 998 2012-09-25 -6.863802 -58.711503 -50.458917 -55.218363 999 2012-09-26 -7.075728 -59.524064 -49.924592 -55.653215 [1000 rows x 5 columns]","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"pandas","slug":"pandas","permalink":"http://gonekng.github.io/tags/pandas/"}],"author":"Jiwon Kang"},{"title":"Pandas tutorial 2","slug":"Python/Tutorial/pd_tutorial_02","date":"2022-03-24T02:53:00.000Z","updated":"2022-10-05T05:39:54.741Z","comments":true,"path":"2022/03/24/Python/Tutorial/pd_tutorial_02/","link":"","permalink":"http://gonekng.github.io/2022/03/24/Python/Tutorial/pd_tutorial_02/","excerpt":"","text":"라이브러리 불러오기1234import numpy as npprint(&quot;numpy: ver.&quot;, np.__version__)import pandas as pdprint(&quot;pandas: ver.&quot;, pd.__version__) numpy: ver. 1.21.5 pandas: ver. 1.3.5 데이터 불러오기12from google.colab import drivedrive.mount(&#x27;/content/drive&#x27;) Mounted at /content/drive 123DATA_PATH = &#x27;/content/drive/MyDrive/Colab Notebooks/Data/supermarket_sales.csv&#x27;sales = pd.read_csv(DATA_PATH)print(sales.head(3)) Invoice ID Branch City Customer type Gender \\ 0 750-67-8428 A Yangon Member Female 1 226-31-3081 C Naypyitaw Normal Female 2 631-41-3108 A Yangon Normal Male Product line Unit price Quantity Date Time Payment 0 Health and beauty 74.69 7 1/5/2019 13:08 Ewallet 1 Electronic accessories 15.28 5 3/8/2019 10:29 Cash 2 Home and lifestyle 46.33 7 3/3/2019 13:23 Credit card 1sales.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1000 entries, 0 to 999 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Invoice ID 1000 non-null object 1 Branch 1000 non-null object 2 City 1000 non-null object 3 Customer type 1000 non-null object 4 Gender 1000 non-null object 5 Product line 1000 non-null object 6 Unit price 1000 non-null float64 7 Quantity 1000 non-null int64 8 Date 1000 non-null object 9 Time 1000 non-null object 10 Payment 1000 non-null object dtypes: float64(1), int64(1), object(9) memory usage: 86.1+ KB 데이터 그룹화 groupby() 및 다양한 집계함수 활용 1sales.groupby(by=&quot;Product line&quot;)[&#x27;Quantity&#x27;].count() Product line Electronic accessories 170 Fashion accessories 178 Food and beverages 174 Health and beauty 152 Home and lifestyle 160 Sports and travel 166 Name: Quantity, dtype: int64 1sales.groupby(by=&quot;Product line&quot;)[&#x27;Quantity&#x27;].sum() Product line Electronic accessories 971 Fashion accessories 902 Food and beverages 952 Health and beauty 854 Home and lifestyle 911 Sports and travel 920 Name: Quantity, dtype: int64 12print(sales.groupby(by=[&quot;Branch&quot;,&quot;Customer type&quot;])[&#x27;Quantity&#x27;].sum())print(type(sales.groupby(by=[&quot;Branch&quot;,&quot;Customer type&quot;])[&#x27;Quantity&#x27;].sum())) # Series 객체 Branch Customer type A Member 964 Normal 895 B Member 924 Normal 896 C Member 897 Normal 934 Name: Quantity, dtype: int64 &lt;class &#39;pandas.core.series.Series&#39;&gt; 12print(sales.groupby(by=[&quot;Branch&quot;,&quot;Payment&quot;], as_index=False)[&#x27;Quantity&#x27;].sum())print(type(sales.groupby(by=[&quot;Branch&quot;,&quot;Payment&quot;], as_index=False)[&#x27;Quantity&#x27;].sum())) # DataFrmae 객체 Branch Payment Quantity 0 A Cash 572 1 A Credit card 580 2 A Ewallet 707 3 B Cash 628 4 B Credit card 599 5 B Ewallet 593 6 C Cash 696 7 C Credit card 543 8 C Ewallet 592 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 12print(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])[&#x27;Unit price&#x27;].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]))print(type(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])[&#x27;Unit price&#x27;].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]))) # DataFrame 객체 max min mean Branch Payment A Cash 99.78 10.08 56.374636 Credit card 99.56 11.94 53.011635 Ewallet 99.83 10.13 54.849762 B Cash 99.69 11.85 56.758818 Credit card 99.96 10.59 56.838991 Ewallet 99.92 10.75 53.450973 C Cash 99.96 10.17 57.100081 Credit card 99.82 10.18 53.143061 Ewallet 99.79 10.16 59.238962 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 12print(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])[&#x27;Unit price&#x27;].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]).reset_index())print(type(sales.groupby(by=[&quot;Branch&quot;, &quot;Payment&quot;])[&#x27;Unit price&#x27;].agg([&quot;max&quot;, &quot;min&quot;, &quot;mean&quot;]).reset_index())) Branch Payment max min mean 0 A Cash 99.78 10.08 56.374636 1 A Credit card 99.56 11.94 53.011635 2 A Ewallet 99.83 10.13 54.849762 3 B Cash 99.69 11.85 56.758818 4 B Credit card 99.96 10.59 56.838991 5 B Ewallet 99.92 10.75 53.450973 6 C Cash 99.96 10.17 57.100081 7 C Credit card 99.82 10.18 53.143061 8 C Ewallet 99.79 10.16 59.238962 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 결측치 처리결측치 데이터 생성1234567dict_01 = &#123; &#x27;Score A&#x27; : [80, 90, np.nan, 80], &#x27;Score B&#x27; : [30, 45, np.nan, np.nan], &#x27;Score C&#x27; : [np.nan, 50, 80, 90]&#125;df = pd.DataFrame(dict_01)print(df) Score A Score B Score C 0 80.0 30.0 NaN 1 90.0 45.0 50.0 2 NaN NaN 80.0 3 80.0 NaN 90.0 123print(df.isnull())print(&quot;\\n&quot;)print(df.isnull().sum()) Score A Score B Score C 0 False False True 1 False False False 2 True True False 3 False True False Score A 1 Score B 2 Score C 1 dtype: int64 1234567dict_02 = &#123; &quot;Gender&quot; : [&quot;Male&quot;, &quot;Female&quot;, np.nan, &quot;Male&quot;], &quot;Salary&quot; : [30, 45, 90, 70]&#125;df2 = pd.DataFrame(dict_02)print(df2) Gender Salary 0 Male 30 1 Female 45 2 NaN 90 3 Male 70 123print(df.isnull())print(&quot;\\n&quot;)print(df.isnull().sum()) Score A Score B Score C 0 False False True 1 False False False 2 True True False 3 False True False Score A 1 Score B 2 Score C 1 dtype: int64 결측치 값 대체 문자열 타입과 숫자 타입의 접근 방법 상이 문자열 : 최빈값 등 숫자 : 평균, 최대값, 최소값, 중간값 등 1print(df.fillna(0)) # 0으로 대체 Score A Score B Score C 0 80.0 30.0 0.0 1 90.0 45.0 50.0 2 0.0 0.0 80.0 3 80.0 0.0 90.0 Score A float64 Score B float64 Score C float64 dtype: object 12print(df.fillna(method=&quot;pad&quot;)) # 앞 데이터로 대체# print(df.fillna(method=&quot;ffill&quot;)) : 동일한 결과 Score A Score B Score C 0 80.0 30.0 NaN 1 90.0 45.0 50.0 2 90.0 45.0 80.0 3 80.0 45.0 90.0 Score A Score B Score C 0 80.0 30.0 NaN 1 90.0 45.0 50.0 2 90.0 45.0 80.0 3 80.0 45.0 90.0 12print(df.fillna(method=&quot;backfill&quot;)) # 뒤 데이터로 대체# print(df.fillna(method=&quot;bfill&quot;)) : 동일한 결과 Score A Score B Score C 0 80.0 30.0 50.0 1 90.0 45.0 50.0 2 80.0 NaN 80.0 3 80.0 NaN 90.0 1print(df2[&#x27;Gender&#x27;].fillna(&quot;Genderless&quot;)) # 특정 문자열로 대체 0 Male 1 Female 2 Genderless 3 Male Name: Gender, dtype: object 결측치가 있는 행, 열 제거12345678dict_03 = &#123; &#x27;Score A&#x27; : [80, 90, np.nan, 80], &#x27;Score B&#x27; : [30, 45, np.nan, np.nan], &#x27;Score C&#x27; : [np.nan, 50, 80, 90], &#x27;Score D&#x27; : [50, 30, 80, 60]&#125;df3 = pd.DataFrame(dict_03)print(df3) Score A Score B Score C Score D 0 80.0 30.0 NaN 50 1 90.0 45.0 50.0 30 2 NaN NaN 80.0 80 3 80.0 NaN 90.0 60 123print(df3.dropna()) # axis: default 0print(&quot;\\n&quot;)print(df3.dropna(axis=1)) Score A Score B Score C Score D 1 90.0 45.0 50.0 30 Score D 0 50 1 30 2 80 3 60 이상치 탐지 일반적으로 IQR(&#x3D; Q3 - Q1; 사분위수범위)를 활용하여 탐지 하한 경계값 : Q1 - IQR * 1.5 상한 경계값 : Q3 + IQR * 1.5 Box Plot으로도 확인 가능 실무에서는 각 도메인(비즈니스 영역)별로 기준 상이 1print(sales[[&#x27;Unit price&#x27;]].describe()) Unit price count 1000.000000 mean 55.672130 std 26.494628 min 10.080000 25% 32.875000 50% 55.230000 75% 77.935000 max 99.960000 1234567891011q1 = sales[&#x27;Unit price&#x27;].quantile(0.25)q3 = sales[&#x27;Unit price&#x27;].quantile(0.75)iqr = q3 - q1lim_q1 = q1 - 1.5 * IQRlim_q3 = q3 + 1.5 * IQRprint(tuple([lim_q1, lim_q3]))print(&quot;\\n&quot;)out_q1 = (sales[&#x27;Unit price&#x27;] &lt; lim_q1)out_q3 = (sales[&#x27;Unit price&#x27;] &gt; lim_q3)outliers = (sales[&#x27;Unit price&#x27;][out_q1 | out_q3])print(outliers) (-34.715, 145.525) Series([], Name: Unit price, dtype: float64) 12import matplotlib.pyplot as pltplt.boxplot(sales[&#x27;Unit price&#x27;]) &#123;&#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7fefce5f93d0&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7fefce5fe3d0&gt;, &lt;matplotlib.lines.Line2D at 0x7fefce5fe910&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7fefce605410&gt;], &#39;means&#39;: [], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7fefce5fee90&gt;], &#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7fefce5f9910&gt;, &lt;matplotlib.lines.Line2D at 0x7fefce5f9e50&gt;]&#125;","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"pandas","slug":"pandas","permalink":"http://gonekng.github.io/tags/pandas/"}],"author":"Jiwon Kang"},{"title":"Pandas tutorial 1","slug":"Python/Tutorial/pd_tutorial_01","date":"2022-03-24T02:52:00.000Z","updated":"2022-10-05T05:39:54.625Z","comments":true,"path":"2022/03/24/Python/Tutorial/pd_tutorial_01/","link":"","permalink":"http://gonekng.github.io/2022/03/24/Python/Tutorial/pd_tutorial_01/","excerpt":"","text":"데이터 전처리 프로세스 중복값 제거 및 결측치 처리 완전 무작위(MCAR) &#x2F; 무작위(MAR) &#x2F; 비무작위(NMAR) 제거, 치환, 모델 기반 처리 등 이상치 탐지 및 처리 삭제, 대체, 변환(스케일링) 등 Feature Engineering 정규화, 표준화, 로그변환, 벡터화 등 PCA, EFA 등을 통한 차원 축소 Pandas 라이브러리Pandas의 기본 자료형 Series 객체, DataFrame 객체 Index: 숫자 또는 문자, 중복X Series: Index &amp; Column 1개 DataFrame: Index &amp; Column 2개 이상 각 객체에 따라 사용 가능한 method가 상이함 Pandas 라이브러리 불러오기12import pandas as pdprint(pd.__version__) 1.3.5 테스트12345# DataFrame 객체temp_dic = &#123;&#x27;col1&#x27; : [1, 2, 3], &#x27;col2&#x27; : [4, 5, 6]&#125;df = pd.DataFrame(temp_dic)print(df)print(type(df)) col1 col2 0 1 4 1 2 5 2 3 6 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 12345# Series 객체temp_dic = &#123;&#x27;a&#x27;:1, &#x27;b&#x27;:2, &#x27;c&#x27;:3&#125;ser = pd.Series(temp_dic)print(ser)print(type(ser)) a 1 b 2 c 3 dtype: int64 &lt;class &#39;pandas.core.series.Series&#39;&gt; 구글 드라이브 연동12from google.colab import drivedrive.mount(&#x27;/content/drive&#x27;) Mounted at /content/drive 데이터 불러오기123DATA_PATH = &#x27;/content/drive/MyDrive/Colab Notebooks/Data/Lemonade2016.csv&#x27;juice = pd.read_csv(DATA_PATH)print(juice) Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 5 7/6/2016 Beach 103 69 82 90.0 0.25 6 7/6/2016 Beach 103 69 82 90.0 0.25 7 7/7/2016 Beach 143 101 81 135.0 0.25 8 NaN Beach 123 86 82 113.0 0.25 9 7/9/2016 Beach 134 95 80 126.0 0.25 10 7/10/2016 Beach 140 98 82 131.0 0.25 11 7/11/2016 Beach 162 120 83 135.0 0.25 12 7/12/2016 Beach 130 95 84 99.0 0.25 13 7/13/2016 Beach 109 75 77 99.0 0.25 14 7/14/2016 Beach 122 85 78 113.0 0.25 15 7/15/2016 Beach 98 62 75 108.0 0.50 16 7/16/2016 Beach 81 50 74 90.0 0.50 17 7/17/2016 Beach 115 76 77 126.0 0.50 18 7/18/2016 Park 131 92 81 122.0 0.50 19 7/19/2016 Park 122 85 78 113.0 0.50 20 7/20/2016 Park 71 42 70 NaN 0.50 21 7/21/2016 Park 83 50 77 90.0 0.50 22 7/22/2016 Park 112 75 80 108.0 0.50 23 7/23/2016 Park 120 82 81 117.0 0.50 24 7/24/2016 Park 121 82 82 117.0 0.50 25 7/25/2016 Park 156 113 84 135.0 0.50 26 7/26/2016 Park 176 129 83 158.0 0.35 27 7/27/2016 Park 104 68 80 99.0 0.35 28 7/28/2016 Park 96 63 82 90.0 0.35 29 7/29/2016 Park 100 66 81 95.0 0.35 30 7/30/2016 Beach 88 57 82 81.0 0.35 31 7/31/2016 Beach 76 47 82 68.0 0.35 12# 전체적인 구조, 결측치 개수, 데이터 타입 파악juice.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 123print(juice.head())print(&quot;-------------------------------------------------------------------&quot;)print(juice.tail()) Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 ------------------------------------------------------------------- Date Location Lemon Orange Temperature Leaflets Price 27 7/27/2016 Park 104 68 80 99.0 0.35 28 7/28/2016 Park 96 63 82 90.0 0.35 29 7/29/2016 Park 100 66 81 95.0 0.35 30 7/30/2016 Beach 88 57 82 81.0 0.35 31 7/31/2016 Beach 76 47 82 68.0 0.35 describe() : 기술통계량 확인 (int형, float형 변수) value_counts() : 범주형 변수 빈도 수 확인 12print(juice.describe())print(&quot;** type(juice.describe()) :&quot;, type(juice.describe())) # DataFrame 객체로 반환 Lemon Orange Temperature Leaflets Price count 32.000000 32.000000 32.000000 31.000000 32.000000 mean 116.156250 80.000000 78.968750 108.548387 0.354687 std 25.823357 21.863211 4.067847 20.117718 0.113137 min 71.000000 42.000000 70.000000 68.000000 0.250000 25% 98.000000 66.750000 77.000000 90.000000 0.250000 50% 113.500000 76.500000 80.500000 108.000000 0.350000 75% 131.750000 95.000000 82.000000 124.000000 0.500000 max 176.000000 129.000000 84.000000 158.000000 0.500000 ** type(juice.describe()) : &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 12print(juice[&#x27;Location&#x27;].value_counts())print(&quot;** type(juice.describe()) :&quot;, type(juice[&#x27;Location&#x27;].value_counts())) # Series 객체로 반환 Beach 17 Park 15 Name: Location, dtype: int64 ** type(juice.describe()) : &lt;class &#39;pandas.core.series.Series&#39;&gt; 데이터 다루기12juice[&#x27;Sold&#x27;] = 0juice.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 0 1 7/2/2016 Park 98 67 72 90.0 0.25 0 2 7/3/2016 Park 110 77 71 104.0 0.25 0 3 7/4/2016 Beach 134 99 76 98.0 0.25 0 4 7/5/2016 Beach 159 118 78 135.0 0.25 0 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-d7644e91-b826-4359-8387-1580b5266658 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-d7644e91-b826-4359-8387-1580b5266658&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 12juice[&#x27;Sold&#x27;] = juice[&#x27;Lemon&#x27;] + juice[&#x27;Orange&#x27;]juice.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 2 7/3/2016 Park 110 77 71 104.0 0.25 187 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-2d21cec2-5539-4100-b9f5-ac71828a5d25 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-2d21cec2-5539-4100-b9f5-ac71828a5d25&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 매출액(Revenue) &#x3D; 가격(Price) * 판매량(Sold) 12juice[&#x27;Revenue&#x27;] = juice[&#x27;Price&#x27;] * juice[&#x27;Sold&#x27;]juice.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-506a14b7-9d00-4793-9a3f-5c54252c47b5 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-506a14b7-9d00-4793-9a3f-5c54252c47b5&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 행 또는 열 제거 : drop(axis&#x3D;0|1) axis&#x3D;0 : 행 방향(index) 실행 axis&#x3D;1 : 열 방향(column) 실행 12juice_col_drop = juice.drop(&#x27;Sold&#x27;, axis=1)juice_col_drop.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 46.75 3 7/4/2016 Beach 134 99 76 98.0 0.25 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 69.25 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-4ee85edb-efbb-47d3-84c6-8b0cab09eb0f button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-4ee85edb-efbb-47d3-84c6-8b0cab09eb0f&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 12juice_ind_drop = juice.drop(2, axis=0)juice_ind_drop.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-8329c7e5-dc41-41e9-aa8f-d31cdc115b69 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-8329c7e5-dc41-41e9-aa8f-d31cdc115b69&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 데이터 인덱싱1juice[5:10] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 8 NaN Beach 123 86 82 113.0 0.25 209 52.25 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-e5a46c70-33e1-4ae6-868e-0fbcf1b0c121 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-e5a46c70-33e1-4ae6-868e-0fbcf1b0c121&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; boolean 값 활용 (조건식)12# Location 값이 Park인 경우juice[juice[&#x27;Location&#x27;] == &#x27;Park&#x27;] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 19 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 20 7/20/2016 Park 71 42 70 NaN 0.50 113 56.50 21 7/21/2016 Park 83 50 77 90.0 0.50 133 66.50 22 7/22/2016 Park 112 75 80 108.0 0.50 187 93.50 23 7/23/2016 Park 120 82 81 117.0 0.50 202 101.00 24 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 27 7/27/2016 Park 104 68 80 99.0 0.35 172 60.20 28 7/28/2016 Park 96 63 82 90.0 0.35 159 55.65 29 7/29/2016 Park 100 66 81 95.0 0.35 166 58.10 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-6440b49e-9b42-4a3b-9edd-37f9b7d5b0a6 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-6440b49e-9b42-4a3b-9edd-37f9b7d5b0a6&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 12# Leaflets 값이 120 이상인 경우juice[juice[&#x27;Leaflets&#x27;] &gt;= 120] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 59.50 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 70.50 17 7/17/2016 Beach 115 76 77 126.0 0.50 191 95.50 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-92086f96-d263-4c70-85ec-af04ffb445c1 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-92086f96-d263-4c70-85ec-af04ffb445c1&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; iloc vs loc iloc : index 기반, 속도↑, 대용량 데이터에 적합 syntax : df.iloc[row_index, column_index] loc : label or boolean(조건식) 기반, 가독성↑ syntax : df.loc[row_label, column_label] 12%%timejuice.iloc[0:3, 0:2] # 해당 인덱스 미포함 CPU times: user 514 µs, sys: 40 µs, total: 554 µs Wall time: 523 µs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-219659e3-19a4-418f-84d6-7d01a8076e00 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-219659e3-19a4-418f-84d6-7d01a8076e00&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 12%%timejuice.loc[0:2, [&quot;Date&quot;,&#x27;Location&#x27;]] # 해당 라벨명 포함 CPU times: user 1.67 ms, sys: 0 ns, total: 1.67 ms Wall time: 5.19 ms .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-5e3530fd-7c31-4a6f-8a4f-dea8d6d42e78 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-5e3530fd-7c31-4a6f-8a4f-dea8d6d42e78&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 1234# juice.iloc[juice[&#x27;Leaflets&#x27;] &gt;= 130, [&#x27;Date&#x27;, &#x27;Location&#x27;, &#x27;Leaflets&#x27;]]# Error: iLocation based boolean indexing on an integer type is not availablejuice.loc[juice[&#x27;Leaflets&#x27;] &gt;= 130, [&#x27;Date&#x27;, &#x27;Location&#x27;, &#x27;Leaflets&#x27;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Leaflets 4 7/5/2016 Beach 135.0 7 7/7/2016 Beach 135.0 10 7/10/2016 Beach 131.0 11 7/11/2016 Beach 135.0 25 7/25/2016 Park 135.0 26 7/26/2016 Park 158.0 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-80998d16-2836-479c-9ea3-75efcac52299 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-80998d16-2836-479c-9ea3-75efcac52299&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 데이터 정렬 sort_values() 함수 12# Revenue 기준 오름차순juice.sort_values(by=[&#x27;Revenue&#x27;]).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 43.05 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-4d6ed323-d5df-47ed-893d-db57c2b39110 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-4d6ed323-d5df-47ed-893d-db57c2b39110&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 12# Revenue 기준 내림차순juice.sort_values(by=[&#x27;Revenue&#x27;], ascending = False).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 19 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 24 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-ecf0c228-c2c7-4d62-aa08-e194d9fd1e4e button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-ecf0c228-c2c7-4d62-aa08-e194d9fd1e4e&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 12# Price 기준 내림차순, Temperature 기준 오름차순juice.sort_values(by=[&#x27;Price&#x27;, &#x27;Temperature&#x27;], ascending = [False, True]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 20 7/20/2016 Park 71 42 70 NaN 0.50 113 56.50 16 7/16/2016 Beach 81 50 74 90.0 0.50 131 65.50 15 7/15/2016 Beach 98 62 75 108.0 0.50 160 80.00 17 7/17/2016 Beach 115 76 77 126.0 0.50 191 95.50 21 7/21/2016 Park 83 50 77 90.0 0.50 133 66.50 19 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 22 7/22/2016 Park 112 75 80 108.0 0.50 187 93.50 18 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 23 7/23/2016 Park 120 82 81 117.0 0.50 202 101.00 24 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 25 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 27 7/27/2016 Park 104 68 80 99.0 0.35 172 60.20 29 7/29/2016 Park 100 66 81 95.0 0.35 166 58.10 28 7/28/2016 Park 96 63 82 90.0 0.35 159 55.65 30 7/30/2016 Beach 88 57 82 81.0 0.35 145 50.75 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 43.05 26 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 0 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 2 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 1 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 13 7/13/2016 Beach 109 75 77 99.0 0.25 184 46.00 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 14 7/14/2016 Beach 122 85 78 113.0 0.25 207 51.75 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 8 NaN Beach 123 86 82 113.0 0.25 209 52.25 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 59.50 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 70.50 12 7/12/2016 Beach 130 95 84 99.0 0.25 225 56.25 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-56042f49-547e-4908-99b7-0e79a9445a3d button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-56042f49-547e-4908-99b7-0e79a9445a3d&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 123# index를 새로 지정해서 새로운 객체로 저장juice2 = juice.sort_values(by=[&#x27;Price&#x27;, &#x27;Temperature&#x27;], ascending = [False, True]).reset_index(drop=True)juice2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 0 7/20/2016 Park 71 42 70 NaN 0.50 113 56.50 1 7/16/2016 Beach 81 50 74 90.0 0.50 131 65.50 2 7/15/2016 Beach 98 62 75 108.0 0.50 160 80.00 3 7/17/2016 Beach 115 76 77 126.0 0.50 191 95.50 4 7/21/2016 Park 83 50 77 90.0 0.50 133 66.50 5 7/19/2016 Park 122 85 78 113.0 0.50 207 103.50 6 7/22/2016 Park 112 75 80 108.0 0.50 187 93.50 7 7/18/2016 Park 131 92 81 122.0 0.50 223 111.50 8 7/23/2016 Park 120 82 81 117.0 0.50 202 101.00 9 7/24/2016 Park 121 82 82 117.0 0.50 203 101.50 10 7/25/2016 Park 156 113 84 135.0 0.50 269 134.50 11 7/27/2016 Park 104 68 80 99.0 0.35 172 60.20 12 7/29/2016 Park 100 66 81 95.0 0.35 166 58.10 13 7/28/2016 Park 96 63 82 90.0 0.35 159 55.65 14 7/30/2016 Beach 88 57 82 81.0 0.35 145 50.75 15 7/31/2016 Beach 76 47 82 68.0 0.35 123 43.05 16 7/26/2016 Park 176 129 83 158.0 0.35 305 106.75 17 7/1/2016 Park 97 67 70 90.0 0.25 164 41.00 18 7/3/2016 Park 110 77 71 104.0 0.25 187 46.75 19 7/2/2016 Park 98 67 72 90.0 0.25 165 41.25 20 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 21 7/13/2016 Beach 109 75 77 99.0 0.25 184 46.00 22 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 23 7/14/2016 Beach 122 85 78 113.0 0.25 207 51.75 24 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 25 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 26 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 27 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 28 NaN Beach 123 86 82 113.0 0.25 209 52.25 29 7/10/2016 Beach 140 98 82 131.0 0.25 238 59.50 30 7/11/2016 Beach 162 120 83 135.0 0.25 282 70.50 31 7/12/2016 Beach 130 95 84 99.0 0.25 225 56.25 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-95b0ec19-306b-4312-a2fb-6c6bf238bb16 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-95b0ec19-306b-4312-a2fb-6c6bf238bb16&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 데이터 그룹화 groupby() 함수 그룹별 집계함수를 통해 피벗테이블 생성 1juice.groupby(by=&#x27;Location&#x27;).count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Date Lemon Orange Temperature Leaflets Price Sold Revenue Location Beach 16 17 17 17 17 17 17 17 Park 15 15 15 15 14 15 15 15 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-464f987e-371e-4805-8b50-ee52bdc321e8 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-464f987e-371e-4805-8b50-ee52bdc321e8&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt; 1234import numpy as np# Location 그룹별 Lemon, Orange 변수에 대한 집계함수juice.groupby(by=&#x27;Location&#x27;)[[&#x27;Lemon&#x27;,&#x27;Orange&#x27;]].agg([max, min, sum, np.mean]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead tr th &#123; text-align: left; &#125; .dataframe thead tr:last-of-type th &#123; text-align: right; &#125; Lemon Orange max min sum mean max min sum mean Location Beach 162 76 2020 118.823529 120 47 1402 82.470588 Park 176 71 1697 113.133333 129 42 1158 77.200000 &lt;svg xmlns&#x3D;”http://www.w3.org/2000/svg&quot; height&#x3D;”24px”viewBox&#x3D;”0 0 24 24” width&#x3D;”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector(&#39;#df-1d771570-530b-4471-945e-e1db8f8b3457 button.colab-df-convert&#39;); buttonEl.style.display = google.colab.kernel.accessAllowed ? &#39;block&#39; : &#39;none&#39;; async function convertToInteractive(key) &#123; const element = document.querySelector(&#39;#df-1d771570-530b-4471-945e-e1db8f8b3457&#39;); const dataTable = await google.colab.kernel.invokeFunction(&#39;convertToInteractive&#39;, [key], &#123;&#125;); if (!dataTable) return; const docLinkHtml = &#39;Like what you see? Visit the &#39; + &#39;&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;&#39; + &#39; to learn more about interactive tables.&#39;; element.innerHTML = &#39;&#39;; dataTable[&#39;output_type&#39;] = &#39;display_data&#39;; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement(&#39;div&#39;); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); &#125; &lt;/script&gt; &lt;/div&gt;","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"pandas","slug":"pandas","permalink":"http://gonekng.github.io/tags/pandas/"}],"author":"Jiwon Kang"},{"title":"Numpy tutorial","slug":"Python/Tutorial/np_tutorial","date":"2022-03-24T02:51:00.000Z","updated":"2022-10-05T05:39:54.385Z","comments":true,"path":"2022/03/24/Python/Tutorial/np_tutorial/","link":"","permalink":"http://gonekng.github.io/2022/03/24/Python/Tutorial/np_tutorial/","excerpt":"","text":"파이썬 라이브러리 설치in R (코드에서 실행) install.package(“패키지명”) library(패키지명) in Python (터미널에서 실행) 방법1. conda 설치 (주 사용목적: 데이터 과학) 아나콘다 설치 후에 conda 설치 가능 방법2. pip 설치 (개발, 데이터 과학, 그 외) 아나콘다 없이도 python만 설치하면 됨 NumPy 라이브러리 파이썬의 대표적인 배열 라이브러리 파이썬 수치 연산과 관련된 모든 라이브러리의 기본 scikit learn, TensorFlow, PyTorch 등등 .array, .reshape, .matlib 등등 다양한 메서드 활용 Numpy 라이브러리 불러오기12import numpy as npprint(np.__version__) 1.21.5 NumPy 배열 생성리스트를 배열로 변환 NumPy 배열로 변환하여 저장 1234567temp = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]print(temp)print(type(temp))arr = np.array(temp)print(arr)print(type(arr)) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] &lt;class &#39;list&#39;&gt; [ 1 2 3 4 5 6 7 8 9 10] &lt;class &#39;numpy.ndarray&#39;&gt; NumPy 배열에도 인덱싱, 슬라이싱 동일하게 적용 123print(arr[4])print(arr[3:7])print(arr[::2]) 5 [4 5 6 7] [1 3 5 7 9] NumPy를 통한 기초 통계 함수 사용 123456print(&quot;sum:&quot;, np.sum(arr))print(&quot;mean:&quot;, np.mean(arr))print(&quot;median:&quot;, np.median(arr))print(&quot;min:&quot;, np.min(arr))print(&quot;max:&quot;, np.max(arr))print(&quot;std:&quot;, np.std(arr)) sum: 55 mean: 5.5 median: 5.5 min: 1 max: 10 std: 2.8722813232690143 사칙연산123456789math_scores = [90, 80, 88]english_scores = [80, 70, 90]total_scores = math_scores + english_scores # 단순한 리스트 연결 수행print(total_scores)math_arr = np.array(math_scores)english_arr = np.array(english_scores)total_arr = math_arr + english_arr # 각 요소의 덧셈 수행print(total_arr) [90, 80, 88, 80, 70, 90] [170 150 178] 123456789arr1 = np.array([2, 5, 4])arr2 = np.array([4, 2, 3])# 사칙연산print(&quot;덧셈:&quot;, np.add(arr1, arr2))print(&quot;뺄셈:&quot;, np.subtract(arr1, arr2))print(&quot;곱셈:&quot;, np.multiply(arr1, arr2))print(&quot;나눗셈:&quot;, np.divide(arr1, arr2))print(&quot;거듭제곱:&quot;, np.power(arr1, arr2)) 덧셈: [6 7 7] 뺄셈: [-2 3 1] 곱셈: [ 8 10 12] 나눗셈: [0.5 2.5 1.33333333] 거듭제곱: [16 25 64] 소수점 처리123456# 소수점 절삭temp_arr = np.trunc([-1.23, 1.23])print(temp_arr)temp_arr = np.fix([-1.23, 1.23])print(temp_arr) [-1. 1.] [-1. 1.] 1234567891011121314# 올림temp_arr = np.ceil([-1.23789, 1.23789])print(temp_arr)# 내림temp_arr = np.floor([-1.23789, 1.23789])print(temp_arr)# 반올림temp_arr = np.around([-1.23789, 1.23789], 2)print(temp_arr)temp_arr = np.around([-1.23789, 1.23789], 4)print(temp_arr) [-1. 2.] [-2. 1.] [-1.24 1.24] [-1.2379 1.2379] 배열의 형태 및 차원 0차원부터 3차원까지 생성하는 방법 .shape : axis 축 기준으로 배열의 형태 반환 123456# 0차원 배열temp_arr = np.array(20)print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) 20 &lt;class &#39;numpy.ndarray&#39;&gt; 배열의 형태: () 배열의 차원: 0 123456# 1차원 배열temp_arr = np.array([1,2,3,5])print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [1 2 3 5] &lt;class &#39;numpy.ndarray&#39;&gt; 배열의 형태: (4,) 배열의 차원: 1 123456# 2차원 배열temp_arr = np.array([[1,2,3,5],[4,5,1,6],[9,0,2,3]])print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [[1 2 3 5] [4 5 1 6] [9 0 2 3]] &lt;class &#39;numpy.ndarray&#39;&gt; 배열의 형태: (3, 4) 배열의 차원: 2 1234567# 3차원 배열temp_arr = np.array([[[1,2,3,5],[4,5,1,6],[9,0,2,3]], [[1,1,2,3],[2,3,1,7],[4,9,5,6]]])print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [[[1 2 3 5] [4 5 1 6] [9 0 2 3]] [[1 1 2 3] [2 3 1 7] [4 9 5 6]]] &lt;class &#39;numpy.ndarray&#39;&gt; 배열의 형태: (2, 3, 4) 배열의 차원: 3 123456# parameter를 활용한 배열의 최소 차원 명시temp_arr = np.array([1,2,3,4], ndmin=2)print(temp_arr)print(type(temp_arr))print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim) [[1 2 3 4]] &lt;class &#39;numpy.ndarray&#39;&gt; 배열의 형태: (1, 4) 배열의 차원: 2 배열을 생성하는 다양한 방법12345temp_arr = np.arange(5) # [0:4]print(temp_arr)temp_arr = np.arange(1,11,3) # [1:11:3]print(temp_arr) [0 1 2 3 4] [ 1 4 7 10] 1234zero_arr = np.zeros((2,3)) # 0으로만 이루어진 배열print(zero_arr)print(type(zero_arr))print(&quot;데이터 타입:&quot;, zero_arr.dtype) [[0. 0. 0.] [0. 0. 0.]] &lt;class &#39;numpy.ndarray&#39;&gt; 데이터 타입: float64 123456789temp_arr = np.ones((2,3)) # 1로만 이루어진 배열print(temp_arr)print(type(temp_arr))print(&quot;데이터 타입:&quot;, temp_arr.dtype)temp_arr = np.ones((2,3), dtype=&quot;int32&quot;) # 자체적인 데이터 형변환print(temp_arr)print(type(temp_arr))print(&quot;데이터 타입:&quot;, temp_arr.dtype) [[1. 1. 1.] [1. 1. 1.]] &lt;class &#39;numpy.ndarray&#39;&gt; 데이터 타입: float64 [[1 1 1] [1 1 1]] &lt;class &#39;numpy.ndarray&#39;&gt; 데이터 타입: int32 12345678910111213141516temp_arr = np.ones((2,6))print(temp_arr)print(&quot;배열의 형태:&quot;, temp_arr.shape)print(&quot;배열의 차원:&quot;, temp_arr.ndim)print(&quot;\\n&quot;)temp_res = temp_arr.reshape(3,4)print(temp_res)print(&quot;배열의 형태:&quot;, temp_res.shape)print(&quot;배열의 차원:&quot;, temp_res.ndim)print(&quot;\\n&quot;)temp_res2 = temp_arr.reshape(2,2,3)print(temp_res2)print(&quot;배열의 형태:&quot;, temp_res2.shape)print(&quot;배열의 차원:&quot;, temp_res2.ndim) [[1 1 1 1 1 1] [1 1 1 1 1 1]] 배열의 형태: (2, 6) 배열의 차원: 2 [[1 1 1 1] [1 1 1 1] [1 1 1 1]] 배열의 형태: (3, 4) 배열의 차원: 2 [[[1 1 1] [1 1 1]] [[1 1 1] [1 1 1]]] 배열의 형태: (2, 2, 3) 배열의 차원: 3 12345678temp_arr = np.ones((12,6))temp_res = temp_arr.reshape(3,-1)print(temp_res)print(&quot;배열의 형태:&quot;, temp_res.shape)print(&quot;배열의 차원:&quot;, temp_res.ndim)# temp_res2 = temp_arr.reshape(5,-1)# ValueError: cannot reshape array of size 72 into shape (5,newaxis) [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]] 배열의 형태: (3, 24) 배열의 차원: 2 NumPy 조건식 조건식이 하나일 때: np.where 12345temp_arr = np.arange(10)print(temp_arr)# 5보다 큰 값은 기존 값 * 10print(np.where(temp_arr &gt; 5, temp_arr * 10, temp_arr)) [0 1 2 3 4 5 6 7 8 9] [ 0 1 2 3 4 5 60 70 80 90] 12345temp_arr = np.arange(21)print(temp_arr)# 10보다 작은 값은 기존 값 * 10print(np.where(temp_arr &lt; 10, temp_arr * 10, temp_arr)) [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [ 0 10 20 30 40 50 60 70 80 90 10 11 12 13 14 15 16 17 18 19 20] 조건식이 2개 이상일 때: np.select 1234567temp_arr = np.arange(10)print(temp_arr)# 5보다 큰 값은 기존 값 * 2, 2보다 작은 값은 기존 값 + 100condlist = [temp_arr &gt; 5, temp_arr &lt; 2] # The list of conditionschoicelist = [temp_arr * 2, temp_arr + 100] # The list of outputsnp.select(condlist, choicelist, default = temp_arr) [0 1 2 3 4 5 6 7 8 9] array([100, 101, 2, 3, 4, 5, 12, 14, 16, 18]) NumPy Broadcasting 서로 다른 크기의 배열을 계산할 때의 기본적인 규칙 url : https://numpy.org/doc/stable/user/basics.broadcasting.html?highlight=broadcasting 1","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"numpy","slug":"numpy","permalink":"http://gonekng.github.io/tags/numpy/"}],"author":"Jiwon Kang"},{"title":"Python Basic 4","slug":"Python/Basic/python_basic_4","date":"2022-03-22T08:31:40.000Z","updated":"2022-10-05T05:39:51.382Z","comments":true,"path":"2022/03/22/Python/Basic/python_basic_4/","link":"","permalink":"http://gonekng.github.io/2022/03/22/Python/Basic/python_basic_4/","excerpt":"","text":"클래스(Class) 목적 : 코드의 간결화, 코드의 재사용, 유지보수 용이 여러 클래스가 모여서 하나의 라이브러리가 됨 장고 &#x2F; 웹개발 &#x2F; 머신러닝 &#x2F; 시각화 &#x2F; 전처리 클래스명은 대문자로 시작해야 함 1234567891011121314151617class Person: # class attribute (선택) country = &quot;korean&quot; # instance attribute (필수) def __init__(self, name, age): self.name = name self.age = ageif __name__ == &quot;__main__&quot;: kim = Person(&quot;Kim&quot;, 30) lee = Person(&quot;Lee&quot;, 28) # access class attribute print(&quot;Kim은 &#123;&#125;&quot;.format(kim.__class__.country)) print(&quot;Lee는 &#123;&#125;&quot;.format(lee.__class__.country)) Kim은 korean Lee는 korean 인스턴스 메서드 생성 list.append(), list.extend() 123456789101112131415161718192021class Person: # class attribute (선택) country = &quot;korean&quot; # instance attribute (필수) def __init__(self, name, age): self.name = name self.age = age # instance method 정의 def singing(self, songtitle): return &quot;&#123;&#125;: &#x27;&#123;&#125;&#x27; 노래를 부릅니다.&quot;.format(self.name, songtitle)if __name__ == &quot;__main__&quot;: kim = Person(&quot;Kim&quot;, 30) lee = Person(&quot;Lee&quot;, 28) # call instance method print(kim.singing(&quot;creep&quot;)) print(lee.singing(&quot;peaches&quot;)) Kim: &#39;creep&#39; 노래를 부릅니다. Lee: &#39;peaches&#39; 노래를 부릅니다. 클래스 상속12345678910111213141516171819202122232425262728293031323334353637383940class Parent: # init constructor def __init__(self, name, age): self.name = name self.age = age # instance method def whoAmI(self): print(&quot;I am Parent!&quot;) def singing(self, songtitle): return &quot;&#123;&#125;: &#x27;&#123;&#125;&#x27; 노래를 부릅니다.&quot;.format(self.name, songtitle) def dancing(self): return &quot;&#123;&#125;: 춤을 춥니다.&quot;.format(self.name)class Child(Parent): # instance attribute def __init__(self, name, age): super().__init__(name, age) # 부모 클래스의 생성자 그대로 가져오기 print(&quot;Child Class On.&quot;) # instance method def whoAmI(self): print(&quot;I am Child!&quot;) def studying(self, subject): return &quot;&#123;&#125; : &#123;&#125; 공부를 합니다.&quot;.format(self.name, subject)if __name__ == &quot;__main__&quot;: child_kim = Child(&quot;kim&quot;, 13) parent_kim = Parent(&quot;kim&quot;, 49) child_kim.whoAmI() parent_kim.whoAmI() print(parent_kim.dancing()) # print(parent_kim.studying()) -&gt; AttributeError 발생 print(child_kim.singing(&quot;fake love&quot;)) print(child_kim.studying(&quot;math&quot;)) Child Class On. I am Child! I am Parent! kim: 춤을 춥니다. kim: &#39;fake love&#39; 노래를 부릅니다. kim : math 공부를 합니다. 123456789101112131415161718192021222324252627class TV: def __init__(self): # private variable (외부 접근 불가능) self.__maxprice = 500 def sell(self): print(&quot;Selling Price: &#123;&#125;&quot;.format(self.__maxprice)) # set method, get method def setMaxPrice(self, price): self.__maxprice = price print(&quot;Price Updated&quot;) def getMaxPrice(self): return self.__maxprice if __name__==&quot;__main__&quot;: tv = TV() tv.sell() # 강제로 값을 변경할 수 없음 tv.__maxprice = 100 tv.sell() # 별도의 method를 통해 변경 가능 tv.setMaxPrice(400) tv.sell() Selling Price: 500 Selling Price: 500 Price Updated Selling Price: 400 클래스 내부 조건문 init constructor 1234567891011121314151617181920212223242526272829303132class Employee: # init constructor def __init__(self, name, salary = 0): self.name = name # public variable (외부 접근 가능) if salary &gt; 0: self.salary = salary else: self.salary = 0 print(&quot;급여는 0원이 될 수 없습니다. 다시 입력하세요.&quot;) def update_salary(self, amount): self.salary += amount def weekly_salary(self): return int(self.salary / 7)if __name__==&quot;__main__&quot;: emp1 = Employee(&quot;David&quot;, -50000) print(&quot;&#123;&#125;의 급여는 &#123;&#125;원입니다.&quot;.format(emp1.name, emp1.salary)) emp1.salary = emp1.salary + 1500 print(&quot;&#123;&#125;의 급여는 &#123;&#125;원입니다.&quot;.format(emp1.name, emp1.salary)) emp1.update_salary(3000) print(&quot;&#123;&#125;의 급여는 &#123;&#125;원입니다.&quot;.format(emp1.name, emp1.salary)) week_salary = emp1.weekly_salary() print(&quot;&#123;&#125;의 주 급여는 &#123;&#125;원입니다.&quot;.format(emp1.name, week_salary)) 급여는 0원이 될 수 없습니다. 다시 입력하세요. David의 급여는 0원입니다. David의 급여는 1500원입니다. David의 급여는 4500원입니다. David의 주 급여는 642원입니다. 클래스 Docstring12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Person: &quot;&quot;&quot; 사람을 표현하는 클래스 *** Attributes ---------- name: str Name of the person age: int Age of the person Methods ------- info(additional=&quot;&quot;): Prints the person&#x27;s name and age &quot;&quot;&quot; def __init__(self, name, age): &quot;&quot;&quot; Constructs all the neccessary attributes for the person object Parameters ---------- name: str Name of the person age: int Age of the person &quot;&quot;&quot; self.name = name self.age = age def info(self, additional=None): &quot;&quot;&quot; Prints the person&#x27;s information Parameters ---------- additional: str, optional more info to be diplayed (Default is None) / A, B, C Returns ------- None &quot;&quot;&quot; print(f&#x27;My name is &#123;self.name&#125;. I am &#123;self.age&#125; years old. &#x27; + additional)if __name__==&quot;__main__&quot;: print(Person.__doc__) person = Person(&quot;Jiwon&quot;, age = 27) person.info(&quot;I wanna be a data analyst.&quot;) 사람을 표현하는 클래스 *** Attributes ---------- name: str Name of the person age: int Age of the person Methods ------- info(additional=&quot;&quot;): Prints the person&#39;s name and age My name is Jiwon. I am 27 years old. I wanna be a data analyst.","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"}],"author":"Jiwon Kang"},{"title":"Python Basic 3","slug":"Python/Basic/python_basic_3","date":"2022-03-22T08:31:10.000Z","updated":"2022-10-05T05:39:51.283Z","comments":true,"path":"2022/03/22/Python/Basic/python_basic_3/","link":"","permalink":"http://gonekng.github.io/2022/03/22/Python/Basic/python_basic_3/","excerpt":"","text":"기초 문법 리뷰리스트, 튜플, 딕셔너리1234567891011121314# 리스트book_list = [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;]print(book_list)# append, extend, insert, remove, pop, etc# 튜플book_tuple = (&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;)print(book_tuple)# 수정, 삭제 불가능# 딕셔너리book_dictionary = &#123;&quot;title&quot; : [&quot;A&quot;, &quot;B&quot;], &quot;year&quot; : [2011, 2002]&#125;print(book_dictionary)# keys(), values(), items(), get() [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) &#123;&#39;title&#39;: [&#39;A&#39;, &#39;B&#39;], &#39;year&#39;: [2011, 2002]&#125; 조건문 &amp; 반복문123456if True: print(&quot;코드 실행&quot;) # 들여쓰기 주의elif True: print(&quot;코드 실행&quot;)else: print(&quot;코드 실행&quot;) 12for i in range(3): print(i+1, &quot;안녕하세요&quot;) 1 안녕하세요 2 안녕하세요 3 안녕하세요 123456789101112131415161718book_list = [&quot;R&quot;, &quot;Python&quot;]for book in book_list: print(book, end=&quot; &quot;)print(&quot;\\n&quot;)strings01 = &quot;Hello&quot;for char in strings01: print(char, end=&quot; &quot;)num_tuple = (1, 2, 3, 4)for num in num_tuple: print(num, end=&quot; &quot;)print(&quot;\\n&quot;)num_dict = &#123;&quot;A&quot;:1, &quot;B&quot;:2&#125;for num in num_dict: print(num, end=&quot; &quot;) # key 값 print(num_dict[num], end=&quot; &quot;) # value 값 R Python H e l l o 1 2 3 4 A 1 B 2 반복문의 필요성123456789name_list = [&quot;요구르트&quot;, &quot;우유&quot;, &quot;콜라&quot;, &quot;사이다&quot;, &quot;과자&quot;]price_list = [1000, 1500, 1200, 1200, 1000]quantity_list = [5, 3, 1, 2, 4]for i in range(len(name_list)): name = name_list[i] sales = price_list[i] * quantity_list[i] print(name + &quot;의 매출액 : &quot; + str(sales) + &quot;원&quot;) 요구르트의 매출액 : 5000원 우유의 매출액 : 4500원 콜라의 매출액 : 1200원 사이다의 매출액 : 2400원 과자의 매출액 : 4000원 while 조건식이 들어간 반복문 1234count = 5while count &gt; 0: print(count, &quot;안녕하세요.&quot;) count = count - 1 5 안녕하세요. 4 안녕하세요. 3 안녕하세요. 2 안녕하세요. 1 안녕하세요. 리스트 컴프리핸션 for-loop 반복문을 한 줄로 처리 123456789letters = []for char in &quot;helloworld&quot;: letters.append(char)print(&quot;for-loop 반복문 사용 :&quot;)print(&quot;\\t&quot;, letters)letters2 = [char for char in &quot;helloworld&quot;]print(&quot;리스트 컴프리핸션 사용 :&quot;)print(&quot;\\t&quot;, letters2) for-loop 반복문 사용 : [&#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;o&#39;, &#39;r&#39;, &#39;l&#39;, &#39;d&#39;] 리스트 컴프리핸션 사용 : [&#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;o&#39;, &#39;r&#39;, &#39;l&#39;, &#39;d&#39;] 1234567891011121314# 중첩 리스트를 단일 리스트로my_list = [[10],[20,30]]print(my_list)# for-loop 중첩 반복문 사용flattened_list1 = []for value_list in my_list: for value in value_list: flattened_list1.append(value)print(&quot;중첩 반복문 사용 :&quot;, flattened_list1)# 리스트 컴프리핸션 사용flattened_list2 = [value for value_list in my_list for value in value_list]print(&quot;리스트 컴프리핸션 사용 :&quot;, flattened_list2) [[10], [20, 30]] 중첩 반복문 사용 : [10, 20, 30] 리스트 컴프리핸션 사용 : [10, 20, 30] 사용자 정의 함수1234567891011121314151617181920def plus(a,b): c = a + b return cdef minus(a,b): c = a - b return cdef multiply(a,b): c = a * b return cdef divide(a,b): c = a / b return cprint(plus(1,5))print(minus(10,3))print(multiply(2,4))print(divide(8,2)) 6 7 8 4.0 basic.py로 저장할 때 예시 1!which python /usr/local/bin/python 12345678910111213# /usr/local/bin/python# -*- coding: utf-8 -*-def add(a, b): c = a + b return cif __name__ == &quot;__main__&quot;: a = 1 b = 2 c= add(a, b) print(c) 3 파이썬 함수 주석 처리 Docstring(문서화) 1234567891011121314151617181920212223# /usr/local/bin/python# -*- coding: utf-8 -*-def temp(content, letter): &quot;&quot;&quot; content 안에 있는 문자를 세는 함수입니다. Args: content(str) : 탐색 문자열 letter(str) : 찾을 문자열 Returns: int &quot;&quot;&quot; print(&quot;함수 테스트&quot;) cnt = len([char for char in content if char == letter]) return cntif __name__ == &quot;__main__&quot;: # help(temp) print(temp.__doc__) content 안에 있는 문자를 세는 함수입니다. Args: content(str) : 탐색 문자열 letter(str) : 찾을 문자열 Returns: int 12345678910111213141516171819202122232425262728def mean_and_median(value_list): &quot;&quot;&quot; 숫자 리스트 요소들의 평균과 중간값을 구하는 함수 Args: value_list (iterable of int / float) : A list of int numbers Returns: tuple(float, float) &quot;&quot;&quot; # 평균 mean = sum(value_list) / len(value_list) # 중간값 midpoint = int(len(value_list) / 2) if len(value_list) % 2 == 0: median = (value_list[midpoint - 1] + value_list[midpoint]) / 2 else: median = value_list[midpoint] return mean, medianif __name__ == &quot;__main__&quot;: value_list = [1, 1, 2, 2, 3, 4, 5] avg, median = mean_and_median(value_list) print(&quot;avg:&quot;, avg) print(&quot;median:&quot;, median) avg: 2.5714285714285716 median: 2 12345678910111213141516171819202122232425262728293031def calculation(num1,num2): &quot;&quot;&quot; 두 수에 대한 사칙연산을 수행하는 함수 Args: num1 : float number num2 : float number Returns: tuple(float, float, float, float) &quot;&quot;&quot; # 덧셈 plus_num = num1 + num2 # 뺄셈 minus_num = num1 - num2 # 곱셈 multiply_num = num1 * num2 # 나눗셈(소수점 둘째 자리까지) divide_num = round(num1 / num2, 2) return plus_num, minus_num, multiply_num, divide_numif __name__ == &quot;__main__&quot;: num1 = 13 num2 = 7 plus, minus, multiply, divide = calculation(num1, num2) print(&quot;+ :&quot;, plus) print(&quot;- :&quot;, minus) print(&quot;* :&quot;, multiply) print(&quot;/ :&quot;, divide) + : 20 - : 6 * : 91 / : 1.86 이터레이터, 제너레이터, 데코레이터 변수명 immutable or mutable, context manager","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"}],"author":"Jiwon Kang"},{"title":"Python Basic 2","slug":"Python/Basic/python_basic_2","date":"2022-03-22T08:30:50.000Z","updated":"2022-10-05T05:39:51.198Z","comments":true,"path":"2022/03/22/Python/Basic/python_basic_2/","link":"","permalink":"http://gonekng.github.io/2022/03/22/Python/Basic/python_basic_2/","excerpt":"","text":"리스트 시퀀스 데이터 타입 데이터에 순서가 존재하며, 인덱싱 및 슬라이싱 가능 대괄호(‘[값1, 값2, 값3]’)를 사용하여 표현 12345678910111213a = [] # 빈 리스트a_func = list() # 함수를 통해 생성b = [1]c = [&#x27;apple&#x27;]d = [1,2,[&#x27;apple&#x27;]] # 리스트 안에 리스트print(a)print(a_func)print(b)print(c)print(d)print(type(d)) [] [] [1] [&#39;apple&#39;] [1, 2, [&#39;apple&#39;]] &lt;class &#39;list&#39;&gt; 리스트 Indexing, Slicing12345678910a = [1,2,3,4,5,6,7,8,9,10]print(a)print(a[0])print(a[5])print(a[:5])print(a[8:])print(a[3:9:2])print(a[:-3:3])print(a[::-1]) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 1 6 [1, 2, 3, 4, 5] [9, 10] [4, 6, 8] [1, 4, 7] [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] 1234a = [[&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;], 10]print(a[0])print(a[0][1])print(a[0][2][2]) [&#39;apple&#39;, &#39;banana&#39;, &#39;cherry&#39;] banana e 리스트 연산자 사용12345a = [&quot;john&quot;, &quot;evan&quot;]b = [&quot;alice&quot;, &quot;eva&quot;]c = a + b # 리스트가 하나로 합쳐짐print(c) [&#39;john&#39;, &#39;evan&#39;, &#39;alice&#39;, &#39;eva&#39;] 1234c = a * 3d = b * 0print(&quot;a * 3 =&quot;, c) # 숫자만큼 반복print(&quot;b * 0 =&quot;, d) # 빈 리스트 출력 a * 3 = [&#39;john&#39;, &#39;evan&#39;, &#39;john&#39;, &#39;evan&#39;, &#39;john&#39;, &#39;evan&#39;] b * 0 = [] 리스트 수정 및 삭제123a = [0, 1, 2]a[1] = &#x27;b&#x27;print(a) [0, &#39;b&#39;, 2] 리스트 값 추가123456a = [100,200,300]a.append(400)print(a)a.append([500,600]) # 리스트 자체를 요소로 추가print(a) [100, 200, 300, 400] [100, 200, 300, 400, [500, 600]] 123456a = [100,200,300]a.append(400)print(a)a.extend([500,600]) # 리스트의 값들을 요소로 추가print(a) [100, 200, 300, 400] [100, 200, 300, 400, 500, 600] 123a = [0,1,2]a.insert(1, 100) # 원하는 위치에 원하는 값 추가print(a) [0, 100, 1, 2] 리스트 값 삭제1234a = [4,3,2,1,&quot;A&quot;]a.remove(1) # 해당되는 값 제거 a.remove(&quot;A&quot;)print(a) [4, 3, 2] 123456a = [1,2,3,4,5,6,7,8,9,10]del a[1] # 인덱스 번호를 이용하여 제거print(a)del a[1:5]print(a) [1, 3, 4, 5, 6, 7, 8, 9, 10] [1, 7, 8, 9, 10] 1234567b = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;]x = b.pop(2)print(x)print(b)y = b.pop() # 인덱스를 지정하지 않으면 마지막 요소 추출 및 제거print(y)print(b) c [&#39;a&#39;, &#39;b&#39;, &#39;d&#39;, &#39;e&#39;] e [&#39;a&#39;, &#39;b&#39;, &#39;d&#39;] 그 외 메서드12345a = [0,1,2,3]print(a)a.clear()print(a) [0, 1, 2, 3] [] 12a = [&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;]print(a.index(&quot;b&quot;)) # 해당 요소가 처음으로 등장하는 위치 2 12345678a = [1,4,5,2,3]b = [1,4,5,2,3]a.sort() # 오름차순print(a)b.sort(reverse=True) # 내림차순print(b) [1, 2, 3, 4, 5] [5, 4, 3, 2, 1] 12345678c = [&#x27;d&#x27;,&#x27;bye&#x27;,&#x27;five&#x27;,&#x27;a&#x27;]d = [&#x27;d&#x27;,&#x27;bye&#x27;,&#x27;five&#x27;,&#x27;a&#x27;]c.sort()print(c)d.sort(reverse=True)print(d) [&#39;a&#39;, &#39;bye&#39;, &#39;d&#39;, &#39;five&#39;] [&#39;five&#39;, &#39;d&#39;, &#39;bye&#39;, &#39;a&#39;] 튜플 리스트와 비슷한 형태로 Indexing, Slicing 가능 리스트와 달리 수정 및 삭제가 안 됨 소괄호(‘(값1, 값2, 값3)’)를 사용하여 표현 123456789tuple1 = (0) # 끝에 comma(,)를 붙이지 않으면 int 자료형tuple2 = (0,) # 끝에 comma(,)를 붙여야 tuple 자료형tuple3 = 0, 1, 2print(tuple1)print(type(tuple1))print(tuple2)print(type(tuple2))print(tuple3)print(type(tuple3)) 0 &lt;class &#39;int&#39;&gt; (0,) &lt;class &#39;tuple&#39;&gt; (0, 1, 2) &lt;class &#39;tuple&#39;&gt; 123456789a = (0,1,2,3,&#x27;a&#x27;)print(type(a))# del a[4] : 튜플에서는 수정, 삭제 안 됨b = list(a)print(b)b[1] = &#x27;b&#x27;a = tuple(b)print(a) &lt;class &#39;tuple&#39;&gt; [0, 1, 2, 3, &#39;a&#39;] (0, &#39;b&#39;, 2, 3, &#39;a&#39;) 튜플 Indexing, Slicing1234567a = (0,1,2,3,&#x27;a&#x27;)print(type(a))print(a[1])print(a[-2])print(a[1:3])print(a[::2]) &lt;class &#39;tuple&#39;&gt; 1 3 (1, 2) (0, 2, &#39;a&#39;) 튜플 연산자 사용123456t1 = (0,1,2)t2 = (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)print(t1 + t2)print(t1 * 3)print(t1 * 0) (0, 1, 2, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;) (0, 1, 2, 0, 1, 2, 0, 1, 2) () 딕셔너리 Key와 Value로 구분됨 중괄호({‘키1’:’값1’, ‘키2’:’값2’})를 사용하여 표현 12345678dict_01 = &#123;&#x27;teacher&#x27; : &#x27;evan&#x27;, &#x27;class&#x27; : &#x27;601호&#x27;, &#x27;open&#x27; : &#x27;2022-03-10&#x27;, &#x27;students&#x27; : 24, &#x27;names&#x27; : [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;R&#x27;, &#x27;Z&#x27;]&#125;print(dict_01[&#x27;teacher&#x27;])print(dict_01[&#x27;open&#x27;])print(dict_01[&#x27;names&#x27;]) evan 2022-03-10 [&#39;A&#39;, &#39;B&#39;, &#39;R&#39;, &#39;Z&#39;] 123print(dict_01.keys())print(type(dict_01.keys()))print(list(dict_01.keys())) # 다양한 연산과 메서드를 적용할 수 있는 리스트형으로 변환 dict_keys([&#39;teacher&#39;, &#39;class&#39;, &#39;open&#39;, &#39;students&#39;, &#39;names&#39;]) &lt;class &#39;dict_keys&#39;&gt; [&#39;teacher&#39;, &#39;class&#39;, &#39;open&#39;, &#39;students&#39;, &#39;names&#39;] 123print(dict_01.values())print(type(dict_01.values()))print(list(dict_01.values())) # 다양한 연산과 메서드를 적용할 수 있는 리스트형으로 변환 dict_values([&#39;evan&#39;, &#39;601호&#39;, &#39;2022-03-10&#39;, 24, [&#39;A&#39;, &#39;B&#39;, &#39;R&#39;, &#39;Z&#39;]]) &lt;class &#39;dict_values&#39;&gt; [&#39;evan&#39;, &#39;601호&#39;, &#39;2022-03-10&#39;, 24, [&#39;A&#39;, &#39;B&#39;, &#39;R&#39;, &#39;Z&#39;]] 1dict_01.items() # 각 key와 value가 튜플 형태로 출력됨 dict_items([(&#39;teacher&#39;, &#39;evan&#39;), (&#39;class&#39;, &#39;601호&#39;), (&#39;open&#39;, &#39;2022-03-10&#39;), (&#39;students&#39;, 24), (&#39;names&#39;, [&#39;A&#39;, &#39;B&#39;, &#39;R&#39;, &#39;Z&#39;])]) 1234567print(dict_01.get(&quot;teacher&quot;))# print(dict_01[&#x27;선생님&#x27;])print(dict_01.get(&quot;선생님&quot;)) # key가 없으면 None을 반환print(dict_01.get(&quot;선생님&quot;, &quot;없음&quot;)) # key가 없을 때 대체값 지정 가능print(dict_01.get(&quot;class&quot;))# 그냥 값을 출력해도 되지만, get 메서드를 사용하면 key가 없더라도 에러 없이 출력 가능 evan None 없음 601호 조건문 &amp; 반복문조건문12345weather = &#x27;맑음&#x27;if weather == &quot;비&quot;: print(&quot;우산을 가져간다.&quot;)else: print(&quot;우산을 가져가지 않는다.&quot;) 우산을 가져가지 않는다. 1234567# 60점 이상 합격score = int(input(&quot;점수를 입력하시오. : &quot;))if score &gt;= 60: print(&quot;합격입니다.&quot;)else: print(&quot;불합격입니다.&quot;) 점수를 입력하시오. : 50 불합격입니다. 12345678910111213141516# 90점 이상은 A, 80점 이상은 B, 70점 이상은 C, 나머지는 Fscore = int(input(&quot;점수를 입력하시오. : &quot;))grade = &quot;&quot;if score &gt;= 90: grade = &quot;A&quot;elif score &gt;= 80: grade = &quot;B&quot;elif score &gt;= 70: grade = &quot;C&quot;elif score &gt;= 60: grade = &quot;D&quot;else: grade = &quot;F&quot; print(grade) 점수를 입력하시오. : 68 D 반복문12for i in range(4): print(i+1, &quot;안녕하세요!&quot;) 1 안녕하세요! 2 안녕하세요! 3 안녕하세요! 4 안녕하세요! 123456789count = range(5)print(count)for n in count: print(str(n+1) + &quot;번째&quot;) if (n+1) == 3: print(&quot;stop!&quot;) break print(&quot;shoot!&quot;) range(0, 5) 1번째 shoot! 2번째 shoot! 3번째 stop! 1234567a = &quot;hello&quot;for x in a: if x==&#x27;l&#x27;: break print(x) h e 반복문 작성 방식 : zip, range, enumerate, len, etc 12345alphabets = [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;]# enumerate는 인덱스와 값을 튜플 형태로 묶어주는 객체for i, value in enumerate(alphabets): print(i, value) 0 A 1 B 2 C","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"}],"author":"Jiwon Kang"},{"title":"Python Basic 1","slug":"Python/Basic/python_basic_1","date":"2022-03-22T08:30:00.000Z","updated":"2022-10-05T05:39:51.077Z","comments":true,"path":"2022/03/22/Python/Basic/python_basic_1/","link":"","permalink":"http://gonekng.github.io/2022/03/22/Python/Basic/python_basic_1/","excerpt":"","text":"Hello World1print(&quot;Hello, World!&quot;) Hello, World! 주석 처리 코드 작업 시, 특정 코드에 대해 설명 사용자 정의 함수 작성 시, 클래스 작성 시 중요 (도움말 작성) 123456# 한 줄 주석 처리&quot;&quot;&quot;여러 줄 주석 처리&quot;&quot;&quot;print(&quot;Hello, World!&quot;) Hello, World! 변수 (Scalar) 객체(OBject)로 구현이 됨 하나의 자료형(Type)을 가진다. 클래스(Class)로 정의된다. 다양한 함수들 존재 int int 정수를 표현하는 데 사용 12345num_int = 1num_int2 = 3print(num_int)print(num_int2)print(type(num_int)) 1 3 &lt;class &#39;int&#39;&gt; float 실수를 표현하는 데 사용 123num_float = 0.2print(num_float)print(type(num_float)) 0.2 &lt;class &#39;float&#39;&gt; bool True와 False로 나타나는 Boolean 값을 표현하는 데 사용 123bool_true = Trueprint(bool_true)print(type(bool_true)) True &lt;class &#39;bool&#39;&gt; None Null을 나타내는 자료형으로 None이라는 한 가지 값만 가진다. 123none_x = Noneprint(none_x)print(type(none_x)) None &lt;class &#39;NoneType&#39;&gt; 사칙연산정수형 사칙연산123456789a = 15 # intb = 2 # intprint(&#x27;a + b = &#x27;, a+b) # intprint(&#x27;a - b = &#x27;, a-b) # intprint(&#x27;a * b = &#x27;, a*b) # intprint(&#x27;a / b = &#x27;, a/b) # floatprint(&#x27;a // b = &#x27;, a//b) # intprint(&#x27;a % b = &#x27;, a%b) # intprint(&#x27;a ** b = &#x27;, a**b) # int a + b = 17 a - b = 13 a * b = 30 a / b = 7.5 a // b = 7 a % b = 1 a ** b = 225 실수형 사칙연산123456789a = 15.0 # floatb = 2.0 # floatprint(&#x27;a + b =&#x27;, a+b) # floatprint(&#x27;a - b =&#x27;, a-b) # floatprint(&#x27;a * b =&#x27;, a*b) # floatprint(&#x27;a / b =&#x27;, a/b) # floatprint(&#x27;a // b =&#x27;, a//b) # floatprint(&#x27;a % b =&#x27;, a%b) # floatprint(&#x27;a ** b =&#x27;, a**b) # float a + b = 17.0 a - b = 13.0 a * b = 30.0 a / b = 7.5 a // b = 7.0 a % b = 1.0 a ** b = 225.0 논리형 연산자 Bool형은 True와 False 값으로 정의 AND, OR, NOT 123456789101112x = 5 &gt; 4print(&#x27;x =&#x27;, x)y = 3 &gt; 9print(&#x27;y =&#x27;, y)print(&#x27;x and x =&#x27;, x and x)print(&#x27;x and y =&#x27;, x and y)print(&#x27;y and x =&#x27;, y and x)print(&#x27;y and y =&#x27;, y and y)print(&#x27;x or x =&#x27;, x or x)print(&#x27;x or y =&#x27;, x or y)print(&#x27;y or x =&#x27;, y or x)print(&#x27;y or y =&#x27;, y or y) x = True y = False x and x = True x and y = False y and x = False y and y = False x or x = True x or y = True y or x = True y or y = False 비교 연산자 부등호를 의미 비교 연산자를 True와 False 값을 도출 논리 &amp; 비교 연산자 응용123var = input(&quot;숫자를 입력하시오. :&quot;)print(var)print(type(var)) 숫자를 입력하시오. :24 24 &lt;class &#39;str&#39;&gt; 123var = int(input(&quot;숫자를 입력하시오. :&quot;))print(var)print(type(var)) 숫자를 입력하시오. :92 92 &lt;class &#39;int&#39;&gt; 123456789num1 = int(input(&quot;숫자를 입력하시오. :&quot;))num2 = int(input(&quot;숫자를 입력하시오. :&quot;))num3 = int(input(&quot;숫자를 입력하시오. :&quot;))num4 = int(input(&quot;숫자를 입력하시오. :&quot;))var1 = num1 &gt;= num2 var2 = num3 &lt; num4print(var1 and var2)print(var1 or var2) 숫자를 입력하시오. :29 숫자를 입력하시오. :15 숫자를 입력하시오. :8 숫자를 입력하시오. :10 True True 문자열문자열 입력 방법 문자열을 입력하는 4가지 방법 1234print(&quot;Hello, World&quot;)print(&#x27;Hello, World&#x27;)print(&quot;&#x27;Hello, World&#x27;&quot;)print(&#x27;&quot;Hello, World&quot;&#x27;) Hello, World Hello, World &#39;Hello, World &quot;Hello, World&quot; 문자열에 작은따옴표, 큰따옴표 포함하는 방법 123456789food = &quot;Python&#x27;s favorite food is perl&quot;print(food)say = &#x27;&quot;Python is very easy.&quot; he says.&#x27;print(say)food2 = &#x27;Python\\&#x27;s favorite food is perl&#x27;print(food2)say2 = &quot;\\&quot;Python is very easy.\\&quot; he says.&quot;print(say2) Python&#39;s favorite food is perl &quot;Python is very easy.&quot; he says. Python&#39;s favorite food is perl &quot;Python is very easy.&quot; he says. 변수에 여러 줄의 문자열 대입 12multiline = &quot;Life is too short.\\nYou need python.&quot;print(multiline) Life is too short. You need python. 12345multiline =&#x27;&#x27;&#x27;Life is too short.You need python&#x27;&#x27;&#x27;print(multiline) Life is too short. You need python String 연산자 덧셈 연산자 1234str1 = &quot;Hello &quot;str2 = &quot;World! &quot;print(str1 + str2) Hello World! 곱셈 연산자 12greeting = str1 + str2print(greeting * 3) Hello World! Hello World! Hello World! Indexing 문자열 인덱싱은 문자열 안에서 범위를 지정하여 특정 단일문자 추출 12345greeting = &quot;Hello Kaggle!&quot;print(greeting[0])print(greeting[6])print(greeting[len(greeting)-1])print(greeting[-1]) H K ! ! Slicing 문자열 슬라이싱은 문자열 안에서 범위를 지정하고 특정 문자열 추출 1234567print(greeting[:])print(greeting[:5])print(greeting[6:])print(greeting[3:9])print(greeting[0:9:2])print(greeting[6:-1])print(greeting[::-1]) Hello Kaggle! Hello Kaggle! lo Kag HloKg Kaggle !elggaK olleH Formattingformat 코드1234567print(&quot;I eat %d apples.&quot; % 3) # 숫자 대입print(&quot;I eat %s apples.&quot; % &quot;five&quot;) # 문자열 대입num = 10day = &quot;three&quot;say = &quot;I ate %d apples, so I was sick for %s days.&quot; % (num, day)print(say) I eat 3 apples. I eat five apples. I ate 10 apples, so I was sick for three days. 123print(&quot;I have %s apples&quot; % 3)print(&quot;rate is %s&quot; % 3.234)print(&quot;Error is %d%%.&quot; % 98) # fomatting 연산자와 %를 함께 쓸 때는 %% I have 3 apples rate is 3.234 Error is 98%. 123456print(&quot;%10s,Jane!&quot; % &quot;hi&quot;)print(&quot;%-10s,Jane!&quot; % &quot;hi&quot;)print(&quot;&#x27;%0.4f&#x27;&quot; % 3.42134234)print(&quot;&#x27;%10.4f&#x27;&quot; % 3.42134234)print(&quot;&#x27;%-10.4f&#x27;&quot; % 3.42134234) hi,Jane! hi ,Jane! &#39;3.4213&#39; &#39; 3.4213&#39; &#39;3.4213 &#39; format 함수12345678910print(&quot;I eat &#123;0&#125; apples.&quot;.format(7))print(&quot;I eat &#123;0&#125; apples.&quot;.format(&quot;five&quot;))num = 8day = 3print(&quot;I ate &#123;0&#125; apples.&quot;.format(num))print(&quot;I ate &#123;0&#125; apples, so I was sick for &#123;1&#125; days.&quot;.format(num, day))print(&quot;I ate &#123;num&#125; apples, so I was sick for &#123;day&#125; days.&quot;.format(num=6,day=2))print(&quot;I ate &#123;0&#125; apples, so I was sick for &#123;day&#125; days.&quot;.format(4,day=1)) I eat 7 apples. I eat five apples. I ate 8 apples. I ate 8 apples, so I was sick for 3 days. I ate 6 apples, so I was sick for 2 days. I ate 4 apples, so I was sick for 1 days. 123456print(&quot;&#x27;&#123;0:&lt;10&#125;&#x27;&quot;.format(&quot;hi&quot;))print(&quot;&#x27;&#123;0:^10&#125;&#x27;&quot;.format(&quot;hi&quot;))print(&quot;&#x27;&#123;0:&gt;10&#125;&#x27;&quot;.format(&quot;hi&quot;))print(&quot;&#x27;&#123;0:=^10&#125;&#x27;&quot;.format(&quot;hi&quot;))print(&quot;&#x27;&#123;0:!&lt;10&#125;&#x27;&quot;.format(&quot;hi&quot;)) &#39;hi &#39; &#39; hi &#39; &#39; hi&#39; &#39;====hi====&#39; &#39;hi!!!!!!!!&#39; 12345y = 3.42134234print(&quot;&#x27;&#123;0:0.4f&#125;&#x27;&quot;.format(y))print(&quot;&#x27;&#123;0:10.4f&#125;&#x27;&quot;.format(y))print(&quot;&#x27;&#123;0:^10.4f&#125;&#x27;&quot;.format(y))print(&quot;&#x27;&#123;0:&lt;10.4f&#125;&#x27;&quot;.format(y)) &#39;3.4213&#39; &#39; 3.4213&#39; &#39; 3.4213 &#39; &#39;3.4213 &#39; 123name1 = &quot;John&quot;name2 = &quot;Marry&quot;print(&quot;&#123;0&#125; &#123;&#123;and&#125;&#125; &#123;1&#125;&quot;.format(name1, name2)) John &#123;and&#125; Marry f 문자열1234567name = &#x27;Sally&#x27;age = 29print(f&quot;My name is &#123;name&#125;, and I&#x27;m &#123;age&#125; years old.&quot;)print(f&quot;Next year, I&#x27;m going to be &#123;age+1&#125; years old.&quot;)d = &#123;&#x27;name&#x27;:&#x27;Sally&#x27;, &#x27;age&#x27;:29&#125;print(f&quot;My name is &#123;d[&#x27;name&#x27;]&#125;, and I&#x27;m &#123;d[&#x27;age&#x27;]&#125; years old.&quot;) # 딕셔너리 자료형 활용 My name is Sally, and I&#39;m 29 years old. Next year, I&#39;m going to be 30 years old. My name is Sally, and I&#39;m 29 years old. 123456print(f&#x27;&#123;&quot;hi&quot;:&lt;10&#125;&#x27;)print(f&#x27;&#123;&quot;hi&quot;:^10&#125;&#x27;)print(f&#x27;&#123;&quot;hi&quot;:&gt;10&#125;&#x27;)print(f&#x27;&#123;&quot;hi&quot;:=^10&#125;&#x27;)print(f&#x27;&#123;&quot;hi&quot;:!&lt;10&#125;&#x27;) hi hi hi ====hi==== hi!!!!!!!! 12345y = 3.42134234print(f&#x27;&#123;y:0.4f&#125;&#x27;)print(f&#x27;&#123;y:10.4f&#125;&#x27;)print(f&#x27;&#123;y:^10.4f&#125;&#x27;)print(f&#x27;&#123;y:&lt;10.4f&#125;&#x27;) 3.4213 3.4213 3.4213 3.4213 123name1 = &quot;John&quot;name2 = &quot;Marry&quot;print(f&quot;&#123;name1&#125; &#123;&#123;and&#125;&#125; &#123;name2&#125;&quot;) John &#123;and&#125; Marry 문자열 함수12345678910# counta = &#x27;hobby&#x27;print(a.count(&#x27;b&#x27;))# find, indexa = &quot;Python is the best choice&quot;print(a.find(&quot;b&quot;))print(a.find(&quot;k&quot;)) # 없으면 -1 반환print(a.index(&quot;t&quot;))# print(a.index(&quot;k&quot;)) # 없으면 에러 2 14 -1 2 12345678910111213# joinprint(&quot;,&quot;.join(&#x27;abcdefg&#x27;))# upper, lowera = &quot;Hello&quot;print(a.upper())print(a.lower())# lstrip, rstrip, stripa = &quot; OK &quot;print(a.lstrip())print(a.rstrip())print(a.strip()) a,b,c,d,e,f,g HELLO hello OK OK OK 123456789# replacea = &quot;That&#x27;s right!&quot;print(a.replace(&#x27;right&#x27;, &#x27;wrong&#x27;))# splita = &quot;I Love You&quot;print(a.split()) # 공백 기준b = &quot;a:b:c:d&quot;print(b.split(&#x27;:&#x27;)) # 특정 구분자 기준 That&#39;s wrong! [&#39;I&#39;, &#39;Love&#39;, &#39;You&#39;] [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]","categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"}],"author":"Jiwon Kang"},{"title":"R_markdown Sample","slug":"R/R_sample","date":"2022-03-18T01:02:35.000Z","updated":"2022-10-05T05:39:55.091Z","comments":true,"path":"2022/03/18/R/R_sample/","link":"","permalink":"http://gonekng.github.io/2022/03/18/R/R_sample/","excerpt":"","text":"개요 R에서 만든 sample 파일 github에 업로드 R MarkdownThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 1summary(cars) 1234567## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Including PlotsYou can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.","categories":[{"name":"r","slug":"r","permalink":"http://gonekng.github.io/categories/r/"}],"tags":[{"name":"r","slug":"r","permalink":"http://gonekng.github.io/tags/r/"}],"author":"Jiwon Kang"},{"title":"Hexo 블로그 생성","slug":"hexo/hexo_blog","date":"2022-03-17T02:14:21.000Z","updated":"2022-11-16T09:54:09.424Z","comments":true,"path":"2022/03/17/hexo/hexo_blog/","link":"","permalink":"http://gonekng.github.io/2022/03/17/hexo/hexo_blog/","excerpt":"","text":"Hexo 설치 node.js 설치 옵션 - Chocolatey도 함께 설치 바탕화면 git bash에 입력 node -v : 버전 확인 npm install -g hexo-cli : hexo command line 설치 hexo init myblog : 바탕화면에 myblog 폴더 생성 myblog 폴더 위치에서 Git Bash 열고 hexo server 입력 출력되는 링크로 이동하여 hexo 서버가 잘 열리는지 확인 깃허브 레포지토리 생성 깃허브 로그인 후 profile - new repositories - new 클릭 Repository 이름은 myblog로 지정 (로컬에 생성한 폴더 이름으로 지정해야함) 별도의 옵션 없이 Creating repository 클릭 myblog 폴더 위치에서 Git Bash 열고 아래 코드 한 줄씩 입력 12345671 echo &quot;# myblog&quot; &gt;&gt; README.md2 git init3 git add README.md4 git commit -m &quot;first commit&quot;5 git branch -M main6 git remote add origin https://github.com/[깃허브아이디]/myblog.git7 git push -u origin main 해당 레포지토리에 README.md 파일 생성 새로운 git 저장소를 해당 로컬 폴더에 초기화 README.md 파일을 git에 추가 git에 추가된 모든 내용 커밋 (커밋 메시지는 자유롭게 지정 가능) 사용자 에러 발생 시 아래 코드 입력 12git config --global user.email “[이메일주소]”git config --global user.name “[깃허브아이디]” git 브랜치를 main으로 변경 git 초기화 시 기본값은 master이나, 대부분의 프로젝트에서 main을 사용하기 때문에 초기에 변경해주는 것이 좋음 해당 로컬 폴더에 깃허브 레포지토리를 연결 커밋한 내용 푸시 초기 세팅 이후에는 add, commit, push 명령어만 입력 깃허브에서 새로고침 후 업로드된 README.md 파일 확인 Hexo 블로그 생성 깃허브에서 새로운 Repository 생성 (이름 : [깃허브아이디].github.io) myblog 폴더 위치에서 Git Bash 열고 아래 코드 입력 123$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save 블로그 폴더 안에 잇는 _config.yml 파일 내용 수정 title, subtitle, author 등 세부사항 입력 깃허브 블로그 연결 : url에 깃허브 블로그 주소 입력 (ex. https://[깃허브아이디].github.io) 배포 관련 설정 : 맨 아래 deploy에 다음과 같이 입력 1234deploy: type: git repo: https://github.com/[깃허브아이디]/[깃허브아이디].github.io.git branch: main myblog 폴더 위치에서 Git Bash 열고 hexo generate --deploy 입력 Deploy done이라는 메시지가 출력되면 배포 완료된 것 게시글 추가 또는 수정할 때마다 위의 코드 입력 블로그 테마 변경 https://hexo.io/themes/ 에서 테마 정해서 해당 테마의 Github으로 이동 TIP : 최근에도 지속적으로 업데이트 되고 있는지 확인 npm install hexo-theme-[테마명] 입력 hexo config theme [테마명] 입력 hexo server 입력 이때 에러가 발생하는 경우 npm install --save bulma-stylus@0.8.0 hexo-renderer-inferno@^0.1.3 입력 hexo clean 을 통해 정리한 후 hexo generate --deploy 로 블로그에 배포 R 마크다운 업로드 R 마크다운 소스에서 개요 부분 수정 123output: html_document: keep_md: true R에서 Knit 버튼 클릭하면 해당 디렉토리에 md 파일 생성됨 myblog&#x2F;source&#x2F;_posts 경로에 해당 파일 복사 후 내용 수정 R 디렉토리에 있는 blog_files 폴더를 myblog&#x2F;source&#x2F;images 경로에 복사 후 md 파일에 있는 이미지 링크 수정 및 배포 Google Colab 업로드 google colab &gt; 파일 &gt; 다운로드 &gt; .ipynb 다운로드 Jupiter Lab에서 다운로드한 파일 열고 File &gt; Save and Export Notebook As &gt; Markdown 생성된 md 파일 및 이미지를 블로그 폴더로 복사 후 배포","categories":[{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/tags/hexo/"},{"name":"github","slug":"github","permalink":"http://gonekng.github.io/tags/github/"}],"author":"Jiwon Kang"},{"title":"R_basic_statistics","slug":"R/R_basic_stat","date":"2022-03-15T07:42:09.000Z","updated":"2022-10-05T05:39:54.984Z","comments":true,"path":"2022/03/15/R/R_basic_stat/","link":"","permalink":"http://gonekng.github.io/2022/03/15/R/R_basic_stat/","excerpt":"","text":"통계 분석 개요 기술통계(discriptive Statistics) : 평균, 최솟값, 최댓값, 중앙값 등 데이터의 특징을 서술하는 것 추론통계(inferential Statistics) : 변수 간의 관계를 파악하여 변수 간 인과관계나 새로운 사실을 밝혀내는 것 평균 차이 검정 : 집단별 평균의 차이가 실제로 있는가를 검정하는 것 교차분석 : 범주형 변수로 구성된 집단들의 관련성을 검정하는 것 상관관계분석 : 변수 간의 상관관계(correlation)를 알아보는 것 상관관계는 한 변수가 변화하면 다른 변수도 변화하는 관계를 의미 상관계수(r) : 변화의 강도와 방향을 나타내는 계수 (-1 &lt;&#x3D; r &lt;&#x3D; 1) 수치가 클수록 영향을 주는 강도가 크며, ‘+’는 정의 관계, ‘-‘는 역의 관계 회귀분석 : 독립변수와 종속변수 간의 인과관계를 분석하는 것 독립변수 : 영향을 주는 변수 &#x2F; 종속변수 : 영향을 받는 변수 단순회귀분석 : 종속변수 1개, 독립변수 1개 (y &#x3D; a + b*x) 다중회귀분석 : 종속변수 1개, 독립변수 2개 이상 (y &#x3D; a + b1x1 + b2x2 +…) 통계 검정 가설(hypothesis) 어떤 현상을 설명하기 위해서 가정하는 명제 귀무가설(H0) : 처음부터 기각될 것으로 예상되는 가설 (영가설) 대립가설(H1) : 귀무가설이 기각될 경우 받아들여지는 가설 유의수준(significance level, p값) 귀무가설이 맞는데도 대립가설을 채택할 확률 (제1종 오류의 최대 허용 범위) 가설 검정에서 인정하는 유의수준 : 5%, 1%, 0.1% 신뢰수준(confidence level) : 신뢰할 수 있는 범위 (1-유의수준) 척도(scale) 명목척도 : 측정대상의 특성이나 범주를 구분하는 척도 등번호, 성별, 인종, 지역 등 산술 연산을 할 수 없음 서열척도 : 측정대상의 등급순위를 나타내는 척도 계급, 사회계층, 자격등급 등 산술 연산을 할 수 없음 척도 간의 거리나 간격을 나타내지는 않음 등간척도 : 측정대상을 일정한 간격으로 구분한 척도 온도, 학력, 시험점수 등 서열 뿐만 아니라 거리와 간격도 표현 가능 덧셈, 뺄셈을 할 수 있음 비율척도 : 측정대상을 비율로 나타낼 수 있는 척도 연령, 키, 무게 등 사칙연산을 모두 할 수 있음 통계 분석 사례1. 두 집단의 평균 차이 검정 - 독립표본 t검정(t.test()) 독립변수는 명목척도, 종속변수는 등간척도 또는 비율척도이어야 함 귀무가설 : auto와 manual의 cty평균은 차이가 없다. 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## &#x27;data.frame&#x27;: 234 obs. of 5 variables:## $ manufacturer : chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1t.test(data=mpg1, cty~trans) 1234567891011## ## Welch Two Sample t-test## ## data : cty by trans## t = -4.5375, df = 132.32, p-value = 1.263e-05## alternative hypothesis : true difference in means between group auto and group manual is not equal to 0## 95 percent confidence interval:## -3.887311 -1.527033## sample estimates:## mean in group auto mean in group manual ## 15.96815 18.67532 &gt;&gt; p-value &#x3D; 1.263e-05, 귀무가설 기각(유의수준 .05에서 유의미한 차이가 있음)2. 교차분석 - 카이제곱 검정(chisq.test()) 귀무가설 : trans에 따라 drv의 차이가 없다. 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## &#x27;data.frame&#x27;: 234 obs. of 5 variables:## $ manufacturer : chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1table(mpg1$trans, mpg1$drv) 1234## ## 4 f r## auto 75 65 17## manual 28 41 8 1prop.table(table(mpg1$trans, mpg1$drv),1) 1234## ## 4 f r## auto 0.4777070 0.4140127 0.1082803## manual 0.3636364 0.5324675 0.1038961 1chisq.test(mpg1$trans, mpg1$drv) 12345## ## Pearson&#x27;s Chi-squared test## ## data : mpg1$trans and mpg1$drv## X-squared = 3.1368, df = 2, p-value = 0.2084 &gt;&gt; p-value &#x3D; 0.2084, 귀무가설 채택(유의수준 .05에서 유의미한 차이가 없음)3) 상관관계분석 - cor.test()- 귀무가설 : cty와 hwy는 상관관계가 없다. 12mpg1 &lt;- read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## &#x27;data.frame&#x27;: 234 obs. of 5 variables:## $ manufacturer : chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... 1cor.test(mpg1$cty, mpg1$hwy) 1234567891011## ## Pearson&#x27;s product-moment correlation## ## data : mpg1$cty and mpg1$hwy## t = 49.585, df = 232, p-value &lt; 2.2e-16## alternative hypothesis : true correlation is not equal to 0## 95 percent confidence interval:## 0.9433129 0.9657663## sample estimates:## cor ## 0.9559159 &gt;&gt; p-value &lt; 2.2e-16, 귀무가설 기각(유의수준 .05에서 상관관계가 있음)&gt;&gt; 상관계수 r &#x3D; 0.9559159 (매우 높은 상관관계)4. 단순회귀분석 - lm() 독립변수와 종속변수가 모두 등간척도 또는 비율척도이어야 함 귀무가설 : disp는 mpg에 영향을 주지 않는다. 12RA &lt;- lm(data=mtcars, mpg~disp)summary(RA) 123456789101112131415161718## ## Call:## lm(formula = mpg ~ disp, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 ***## disp -0.041215 0.004712 -8.747 9.38e-10 ***## ---## Signif. codes : 0 &#x27;***&#x27; 0.001 &#x27;**&#x27; 0.01 &#x27;*&#x27; 0.05 &#x27;.&#x27; 0.1 &#x27; &#x27; 1## ## Residual standard error : 3.251 on 30 degrees of freedom## Multiple R-squared : 0.7183, Adjusted R-squared : 0.709 ## F-statistic : 76.51 on 1 and 30 DF, p-value : 9.38e-10 12plot(data=mtcars, mpg~disp)abline(RA, col=&quot;red&quot;) &gt;&gt; p-value &#x3D; 9.38e-10, 귀무가설 기각(유의수준 .05에서 회귀모형이 적합함)&gt;&gt; 절편(Intercept) &#x3D; 29.599855 (유의수준 .05에서 유의함)&gt;&gt; 회귀계수(Estimate) &#x3D; -0.041215 (유의수준 .05에서 유의함)&gt;&gt; 회귀식 : mpg &#x3D; 29.599855 - 0.041215 * disp&gt;&gt; 수정된 결정계수(Adjusted R-Squared) &#x3D; .7095. 다중회귀분석 - lm()12RA &lt;- lm(data=mtcars, mpg~disp+hp+wt)summary(RA) 1234567891011121314151617181920## ## Call:## lm(formula = mpg ~ disp + hp + wt, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -3.891 -1.640 -0.172 1.061 5.861 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.105505 2.110815 17.579 &lt; 2e-16 ***## disp -0.000937 0.010350 -0.091 0.92851 ## hp -0.031157 0.011436 -2.724 0.01097 * ## wt -3.800891 1.066191 -3.565 0.00133 ** ## ---## Signif. codes : 0 &#x27;***&#x27; 0.001 &#x27;**&#x27; 0.01 &#x27;*&#x27; 0.05 &#x27;.&#x27; 0.1 &#x27; &#x27; 1## ## Residual standard error : 2.639 on 28 degrees of freedom## Multiple R-squared : 0.8268, Adjusted R-squared : 0.8083 ## F-statistic : 44.57 on 3 and 28 DF, p-value : 8.65e-11 &gt;&gt; p-value &#x3D; 8.65e-11, 귀무가설 기각(유의수준 .05에서 회귀모형이 적합함)&gt;&gt; 절편(Intercept) &#x3D; 29.599855 (유의수준 .05에서 유의함)&gt;&gt; dist의 계수 &#x3D; -0.000937 (유의수준 .05에서 통계적으로 유의하지 않음)&gt;&gt; hp의 계수 &#x3D; -0.031157 (유의수준 .05에서 유의함)&gt;&gt; wt의 계수 &#x3D; -3.800891 (유의수준 .05에서 유의함)&gt;&gt; 회귀식 : mpg &#x3D; 29.599855 - 0.000937 * disp - 0.031157 * hp - 3.800891 * wt&gt;&gt; 수정된 결정계수(Adjusted R-Squared) &#x3D; .8083","categories":[{"name":"r","slug":"r","permalink":"http://gonekng.github.io/categories/r/"}],"tags":[{"name":"r","slug":"r","permalink":"http://gonekng.github.io/tags/r/"},{"name":"statistic","slug":"statistic","permalink":"http://gonekng.github.io/tags/statistic/"}],"author":"Jiwon Kang"}],"categories":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/categories/python/"},{"name":"ML","slug":"python/ML","permalink":"http://gonekng.github.io/categories/python/ML/"},{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/categories/%EC%B7%A8%EC%A4%80/"},{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/categories/sql/"},{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/categories/hexo/"},{"name":"coding test","slug":"python/coding-test","permalink":"http://gonekng.github.io/categories/python/coding-test/"},{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/categories/setting/"},{"name":"development","slug":"development","permalink":"http://gonekng.github.io/categories/development/"},{"name":"crawling","slug":"python/crawling","permalink":"http://gonekng.github.io/categories/python/crawling/"},{"name":"tutorial","slug":"python/tutorial","permalink":"http://gonekng.github.io/categories/python/tutorial/"},{"name":"r","slug":"r","permalink":"http://gonekng.github.io/categories/r/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gonekng.github.io/tags/python/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://gonekng.github.io/tags/machine-learning/"},{"name":"scikit-learn","slug":"scikit-learn","permalink":"http://gonekng.github.io/tags/scikit-learn/"},{"name":"취준","slug":"취준","permalink":"http://gonekng.github.io/tags/%EC%B7%A8%EC%A4%80/"},{"name":"면접","slug":"면접","permalink":"http://gonekng.github.io/tags/%EB%A9%B4%EC%A0%91/"},{"name":"자소서","slug":"자소서","permalink":"http://gonekng.github.io/tags/%EC%9E%90%EC%86%8C%EC%84%9C/"},{"name":"sql","slug":"sql","permalink":"http://gonekng.github.io/tags/sql/"},{"name":"oracle","slug":"oracle","permalink":"http://gonekng.github.io/tags/oracle/"},{"name":"hexo","slug":"hexo","permalink":"http://gonekng.github.io/tags/hexo/"},{"name":"hueman","slug":"hueman","permalink":"http://gonekng.github.io/tags/hueman/"},{"name":"disqus","slug":"disqus","permalink":"http://gonekng.github.io/tags/disqus/"},{"name":"programmers","slug":"programmers","permalink":"http://gonekng.github.io/tags/programmers/"},{"name":"setting","slug":"setting","permalink":"http://gonekng.github.io/tags/setting/"},{"name":"git","slug":"git","permalink":"http://gonekng.github.io/tags/git/"},{"name":"windows11","slug":"windows11","permalink":"http://gonekng.github.io/tags/windows11/"},{"name":"vscode","slug":"vscode","permalink":"http://gonekng.github.io/tags/vscode/"},{"name":"development","slug":"development","permalink":"http://gonekng.github.io/tags/development/"},{"name":"github","slug":"github","permalink":"http://gonekng.github.io/tags/github/"},{"name":"crawling","slug":"crawling","permalink":"http://gonekng.github.io/tags/crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"http://gonekng.github.io/tags/BeautifulSoup/"},{"name":"data engineering","slug":"data-engineering","permalink":"http://gonekng.github.io/tags/data-engineering/"},{"name":"wsl2","slug":"wsl2","permalink":"http://gonekng.github.io/tags/wsl2/"},{"name":"spark","slug":"spark","permalink":"http://gonekng.github.io/tags/spark/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://gonekng.github.io/tags/elasticsearch/"},{"name":"kibana","slug":"kibana","permalink":"http://gonekng.github.io/tags/kibana/"},{"name":"apache","slug":"apache","permalink":"http://gonekng.github.io/tags/apache/"},{"name":"airflow","slug":"airflow","permalink":"http://gonekng.github.io/tags/airflow/"},{"name":"google colab","slug":"google-colab","permalink":"http://gonekng.github.io/tags/google-colab/"},{"name":"pipeline","slug":"pipeline","permalink":"http://gonekng.github.io/tags/pipeline/"},{"name":"visualization","slug":"visualization","permalink":"http://gonekng.github.io/tags/visualization/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://gonekng.github.io/tags/matplotlib/"},{"name":"seaborn","slug":"seaborn","permalink":"http://gonekng.github.io/tags/seaborn/"},{"name":"pandas","slug":"pandas","permalink":"http://gonekng.github.io/tags/pandas/"},{"name":"numpy","slug":"numpy","permalink":"http://gonekng.github.io/tags/numpy/"},{"name":"r","slug":"r","permalink":"http://gonekng.github.io/tags/r/"},{"name":"statistic","slug":"statistic","permalink":"http://gonekng.github.io/tags/statistic/"}]}